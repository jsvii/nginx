<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Russ Cox" />
  <meta name="author" content="Frans Kaashoek" />
  <meta name="author" content="Robert Morris" />
  <title>xv6: a simple, Unix-like teaching operating system</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title"><strong>xv6: a simple, Unix-like teaching operating system</strong></h1>
<p class="author">Russ Cox</p>
<p class="author">Frans Kaashoek</p>
<p class="author">Robert Morris</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#CH:UNIX">Operating system interfaces</a>
<ul>
<li><a href="#processes-and-memory">Processes and memory</a></li>
<li><a href="#io-and-file-descriptors">I/O and File descriptors</a></li>
<li><a href="#pipes">Pipes</a></li>
<li><a href="#file-system">File system</a></li>
<li><a href="#real-world">Real world</a></li>
<li><a href="#exercises">Exercises</a></li>
</ul></li>
<li><a href="#CH:FIRST">Operating system organization</a>
<ul>
<li><a href="#abstracting-physical-resources">Abstracting physical resources</a></li>
<li><a href="#user-mode-supervisor-mode-and-system-calls">User mode, supervisor mode, and system calls</a></li>
<li><a href="#kernel-organization">Kernel organization</a></li>
<li><a href="#code-xv6-organization">Code: xv6 organization</a></li>
<li><a href="#process-overview">Process overview</a></li>
<li><a href="#code-starting-xv6-and-the-first-process">Code: starting xv6 and the first process</a></li>
<li><a href="#real-world-1">Real world</a></li>
<li><a href="#exercises-1">Exercises</a></li>
</ul></li>
<li><a href="#CH:MEM">Page tables</a>
<ul>
<li><a href="#paging-hardware">Paging hardware</a></li>
<li><a href="#kernel-address-space">Kernel address space</a></li>
<li><a href="#code-creating-an-address-space">Code: creating an address space</a></li>
<li><a href="#physical-memory-allocation">Physical memory allocation</a></li>
<li><a href="#code-physical-memory-allocator">Code: Physical memory allocator</a></li>
<li><a href="#process-address-space">Process address space</a></li>
<li><a href="#code-sbrk">Code: sbrk</a></li>
<li><a href="#code-exec">Code: exec</a></li>
<li><a href="#real-world-2">Real world</a></li>
<li><a href="#exercises-2">Exercises</a></li>
</ul></li>
<li><a href="#CH:TRAP">Traps and device drivers</a>
<ul>
<li><a href="#risc-v-trap-machinery">RISC-V trap machinery</a></li>
<li><a href="#traps-from-kernel-space">Traps from kernel space</a></li>
<li><a href="#traps-from-user-space">Traps from user space</a></li>
<li><a href="#timer-interrupts">Timer interrupts</a></li>
<li><a href="#code-calling-system-calls">Code: Calling system calls</a></li>
<li><a href="#code-system-call-arguments">Code: System call arguments</a></li>
<li><a href="#device-drivers">Device drivers</a></li>
<li><a href="#code-the-console-driver">Code: The console driver</a></li>
<li><a href="#real-world-3">Real world</a></li>
<li><a href="#exercises-3">Exercises</a></li>
</ul></li>
<li><a href="#CH:LOCK">Locking</a>
<ul>
<li><a href="#race-conditions">Race conditions</a></li>
<li><a href="#code-locks">Code: Locks</a></li>
<li><a href="#code-using-locks">Code: Using locks</a></li>
<li><a href="#deadlock-and-lock-ordering">Deadlock and lock ordering</a></li>
<li><a href="#locks-and-interrupt-handlers">Locks and interrupt handlers</a></li>
<li><a href="#instruction-and-memory-ordering">Instruction and memory ordering</a></li>
<li><a href="#sleep-locks">Sleep locks</a></li>
<li><a href="#real-world-4">Real world</a></li>
<li><a href="#exercises-4">Exercises</a></li>
</ul></li>
<li><a href="#CH:SCHED">Scheduling</a>
<ul>
<li><a href="#multiplexing">Multiplexing</a></li>
<li><a href="#code-context-switching">Code: Context switching</a></li>
<li><a href="#code-scheduling">Code: Scheduling</a></li>
<li><a href="#code-mycpu-and-myproc">Code: mycpu and myproc</a></li>
<li><a href="#sec:sleep">Sleep and wakeup</a></li>
<li><a href="#code-sleep-and-wakeup">Code: Sleep and wakeup</a></li>
<li><a href="#code-pipes">Code: Pipes</a></li>
<li><a href="#code-wait-exit-and-kill">Code: Wait, exit, and kill</a></li>
<li><a href="#real-world-5">Real world</a></li>
<li><a href="#exercises-5">Exercises</a></li>
</ul></li>
<li><a href="#CH:FS">File system</a>
<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#buffer-cache-layer">Buffer cache layer</a></li>
<li><a href="#code-buffer-cache">Code: Buffer cache</a></li>
<li><a href="#logging-layer">Logging layer</a></li>
<li><a href="#log-design">Log design</a></li>
<li><a href="#code-logging">Code: logging</a></li>
<li><a href="#code-block-allocator">Code: Block allocator</a></li>
<li><a href="#inode-layer">Inode layer</a></li>
<li><a href="#code-inodes">Code: Inodes</a></li>
<li><a href="#code-inode-content">Code: Inode content</a></li>
<li><a href="#code-directory-layer">Code: directory layer</a></li>
<li><a href="#code-path-names">Code: Path names</a></li>
<li><a href="#file-descriptor-layer">File descriptor layer</a></li>
<li><a href="#code-system-calls">Code: System calls</a></li>
<li><a href="#real-world-6">Real world</a></li>
<li><a href="#exercises-6">Exercises</a></li>
</ul></li>
<li><a href="#CH:LOCK2">Concurrency revisited</a>
<ul>
<li><a href="#locking-patterns">Locking patterns</a></li>
<li><a href="#lock-like-patterns">Lock-like patterns</a></li>
<li><a href="#no-locks-at-all">No locks at all</a></li>
<li><a href="#parallelism">Parallelism</a></li>
<li><a href="#exercises-7">Exercises</a></li>
</ul></li>
<li><a href="#CH:SUM">Summary</a></li>
</ul>
</nav>
<h1 id="CH:UNIX">Operating system interfaces</h1>
<p>The job of an operating system is to share a computer among multiple programs and to provide a more useful set of services than the hardware alone supports. The operating system manages and abstracts the low-level hardware, so that, for example, a word processor need not concern itself with which type of disk hardware is being used. It also shares the hardware among multiple programs so that they run (or appear to run) at the same time. Finally, operating systems provide controlled ways for programs to interact, so that they can share data or work together.</p>
<p>An operating system provides services to user programs through an interface. Designing a good interface turns out to be difficult. On the one hand, we would like the interface to be simple and narrow because that makes it easier to get the implementation right. On the other hand, we may be tempted to offer many sophisticated features to applications. The trick in resolving this tension is to design interfaces that rely on a few mechanisms that can be combined to provide much generality.</p>
<p>This book uses a single operating system as a concrete example to illustrate operating system concepts. That operating system, xv6, provides the basic interfaces introduced by Ken Thompson and Dennis Ritchie’s Unix operating system <span class="citation" data-cites="unix"></span>, as well as mimicking Unix’s internal design. Unix provides a narrow interface whose mechanisms combine well, offering a surprising degree of generality. This interface has been so successful that modern operating systems—BSD, Linux, Mac OS X, Solaris, and even, to a lesser extent, Microsoft Windows—have Unix-like interfaces. Understanding xv6 is a good start toward understanding any of these systems and many others.</p>
<p>As shown in Figure <a href="#fig:os" data-reference-type="ref" data-reference="fig:os">1.1</a>, xv6 takes the traditional form of a <em>kernel</em>, a special program that provides services to running programs. Each running program, called a <em>process</em>, has memory containing instructions, data, and a stack. The instructions implement the program’s computation. The data are the variables on which the computation acts. The stack organizes the program’s procedure calls.</p>
<p>When a process needs to invoke a kernel service, it invokes a procedure call in the operating system interface. Such a procedure is called a <em>system call</em>. The system call enters the kernel; the kernel performs the service and returns. Thus a process alternates between executing in <em>user space</em> and <em>kernel space</em>.</p>
<p>The kernel uses the CPU’s hardware protection mechanisms to ensure that each process executing in user space can access only its own memory. The kernel executes with the hardware privileges required to implement these protections; user programs execute without those privileges. When a user program invokes a system call, the hardware raises the privilege level and starts executing a pre-arranged function in the kernel.</p>
<figure>
<embed src="fig/os.svg" id="fig:os" /><figcaption>A kernel and two user processes.</figcaption>
</figure>
<p>The collection of system calls that a kernel provides is the interface that user programs see. The xv6 kernel provides a subset of the services and system calls that Unix kernels traditionally offer. Figure <a href="#fig:api" data-reference-type="ref" data-reference="fig:api">[fig:api]</a> lists all of xv6’s system calls.</p>
<p>The rest of this chapter outlines xv6’s services—processes, memory, file descriptors, pipes, and a file system—and illustrates them with code snippets and discussions of how the <em>shell</em>, which is the primary user interface to traditional Unix-like systems, uses them. The shell’s use of system calls illustrates how carefully they have been designed.</p>
<p>The shell is an ordinary program that reads commands from the user and executes them. The fact that the shell is a user program and not part of the kernel, illustrates the power of the system call interface: there is nothing special about the shell. It also means that the shell is easy to replace; as a result, modern Unix systems have a variety of shells to choose from, each with its own user interface and scripting features. The xv6 shell is a simple implementation of the essence of the Unix Bourne shell. Its implementation can be found at <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//user/sh.c#L1"><span>(user/sh.c:1)</span></a>.</p>
<h2 id="processes-and-memory">Processes and memory</h2>
<p>An xv6 process consists of user-space memory (instructions, data, and stack) and per-process state private to the kernel. Xv6 can <em>time-share</em> processes: it transparently switches the available CPUs among the set of processes waiting to execute. When a process is not executing, xv6 saves its CPU registers, restoring them when it next runs the process. The kernel associates a process identifier, or <code>pid</code>, with each process.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;"><span><strong>System call</strong></span></th>
<th style="text-align: left;"><span><strong>Description</strong></span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">fork()</td>
<td style="text-align: left;">Create a process</td>
</tr>
<tr class="even">
<td style="text-align: left;">exit(xstatus)</td>
<td style="text-align: left;">Terminate the current process with xstatus indicating success of failure</td>
</tr>
<tr class="odd">
<td style="text-align: left;">wait(*xstatus)</td>
<td style="text-align: left;">Wait for a child process to exit and copy the child’s exit status to xstatus</td>
</tr>
<tr class="even">
<td style="text-align: left;">kill(pid)</td>
<td style="text-align: left;">Terminate process pid</td>
</tr>
<tr class="odd">
<td style="text-align: left;">getpid()</td>
<td style="text-align: left;">Return the current process’s pid</td>
</tr>
<tr class="even">
<td style="text-align: left;">sleep(n)</td>
<td style="text-align: left;">Sleep for n clock ticks</td>
</tr>
<tr class="odd">
<td style="text-align: left;">exec(filename, *argv)</td>
<td style="text-align: left;">Load a file and execute it</td>
</tr>
<tr class="even">
<td style="text-align: left;">sbrk(n)</td>
<td style="text-align: left;">Grow process’s memory by n bytes</td>
</tr>
<tr class="odd">
<td style="text-align: left;">open(filename, flags)</td>
<td style="text-align: left;">Open a file; the flags indicate read/write</td>
</tr>
<tr class="even">
<td style="text-align: left;">read(fd, buf, n)</td>
<td style="text-align: left;">Read n bytes from an open file into buf</td>
</tr>
<tr class="odd">
<td style="text-align: left;">write(fd, buf, n)</td>
<td style="text-align: left;">Write n bytes to an open file</td>
</tr>
<tr class="even">
<td style="text-align: left;">close(fd)</td>
<td style="text-align: left;">Release open file fd</td>
</tr>
<tr class="odd">
<td style="text-align: left;">dup(fd)</td>
<td style="text-align: left;">Duplicate fd</td>
</tr>
<tr class="even">
<td style="text-align: left;">pipe(p)</td>
<td style="text-align: left;">Create a pipe and return fd’s in p</td>
</tr>
<tr class="odd">
<td style="text-align: left;">chdir(dirname)</td>
<td style="text-align: left;">Change the current directory</td>
</tr>
<tr class="even">
<td style="text-align: left;">mkdir(dirname)</td>
<td style="text-align: left;">Create a new directory</td>
</tr>
<tr class="odd">
<td style="text-align: left;">mknod(name, major, minor)</td>
<td style="text-align: left;">Create a device file</td>
</tr>
<tr class="even">
<td style="text-align: left;">fstat(fd)</td>
<td style="text-align: left;">Return info about an open file</td>
</tr>
<tr class="odd">
<td style="text-align: left;">link(f1, f2)</td>
<td style="text-align: left;">Create another name (f2) for the file f1</td>
</tr>
<tr class="even">
<td style="text-align: left;">unlink(filename)</td>
<td style="text-align: left;">Remove a file</td>
</tr>
</tbody>
</table>
<p>A process may create a new process using the <code>fork</code> system call. <code>Fork</code> creates a new process, called the <em>child process</em>, with exactly the same memory contents as the calling process, called the <em>parent process</em>. <code>Fork</code> returns in both the parent and the child. In the parent, <code>fork</code> returns the child’s pid; in the child, it returns zero. For example, consider the following program fragment written in the C programming language <span class="citation" data-cites="kernighan"></span>:</p>
<pre><code>int pid = fork();
if(pid &gt; 0){
  printf(&quot;parent: child=%d\en&quot;, pid);
  pid = wait(0);
  printf(&quot;child %d is done\en&quot;, pid);
} else if(pid == 0){
  printf(&quot;child: exiting\en&quot;);
  exit(0);
} else {
  printf(&quot;fork error\en&quot;);
}</code></pre>
<p>The <code>exit</code> system call causes the calling process to stop executing and to release resources such as memory and open files. Exit takes an integer status argument, conventionally 0 to indicate success and 1 to indicate failure. The <code>wait</code> system call returns the pid of an exited child of the current process and copies the exit status of the child to the address passed to wait; if none of the caller’s children has exited, <code>wait</code> waits for one to do so. If the parent doesn’t care about the exit status of a child, it can pass a 0 address to <code>wait</code>.</p>
<p>In the example, the output lines</p>
<pre><code>parent: child=1234
child: exiting</code></pre>
<p>might come out in either order, depending on whether the parent or child gets to its <code>printf</code> call first. After the child exits the parent’s <code>wait</code> returns, causing the parent to print</p>
<pre><code>parent: child 1234 is done</code></pre>
<p>Although the child has the same memory contents as the parent initially, the parent and child are executing with different memory and different registers: changing a variable in one does not affect the other. For example, when the return value of <code>wait</code> is stored into <code>pid</code> in the parent process, it doesn’t change the variable <code>pid</code> in the child. The value of <code>pid</code> in the child will still be zero.</p>
<p>The <code>exec</code> system call replaces the calling process’s memory with a new memory image loaded from a file stored in the file system. The file must have a particular format, which specifies which part of the file holds instructions, which part is data, at which instruction to start, etc. xv6 uses the ELF format, which Chapter <a href="#CH:MEM" data-reference-type="ref" data-reference="CH:MEM">3</a> discusses in more detail. When <code>exec</code> succeeds, it does not return to the calling program; instead, the instructions loaded from the file start executing at the entry point declared in the ELF header. <code>Exec</code> takes two arguments: the name of the file containing the executable and an array of string arguments. For example:</p>
<pre><code>char *argv[3];

argv[0] = &quot;echo&quot;;
argv[1] = &quot;hello&quot;;
argv[2] = 0;
exec(&quot;/bin/echo&quot;, argv);
printf(&quot;exec error\en&quot;);</code></pre>
<p>This fragment replaces the calling program with an instance of the program <code>/bin/echo</code> running with the argument list <code>echo</code> <code>hello</code>. Most programs ignore the first argument, which is conventionally the name of the program.</p>
<p>The xv6 shell uses the above calls to run programs on behalf of users. The main structure of the shell is simple; see <code>main</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//user/sh.c#L145"><span>(user/sh.c:145)</span></a>. The main loop reads a line of input from the user with <code>getcmd</code>. Then it calls <code>fork</code>, which creates a copy of the shell process. The parent calls <code>wait</code>, while the child runs the command. For example, if the user had typed “<code>echo hello</code>” to the shell, <code>runcmd</code> would have been called with “<code>echo hello</code>” as the argument. <code>runcmd</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//user/sh.c#L58"><span>(user/sh.c:58)</span></a> runs the actual command. For “<code>echo hello</code>”, it would call <code>exec</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//user/sh.c#L78"><span>(user/sh.c:78)</span></a>. If <code>exec</code> succeeds then the child will execute instructions from <code>echo</code> instead of <code>runcmd</code>. At some point <code>echo</code> will call <code>exit</code>, which will cause the parent to return from <code>wait</code> in <code>main</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//user/sh.c#L145"><span>(user/sh.c:145)</span></a>.</p>
<p>You might wonder why <code>fork</code> and <code>exec</code> are not combined in a single call; we will see later that separate calls for creating a process and loading a program has some clever usages in the shell for I/O redirection. To avoid the wastefulness of creating a duplicate process and then immediately replacing it, operating kernels optimize the implementation of <code>fork</code> for this use case by using virtual memory techniques such as copy-on-write.</p>
<p>Xv6 allocates most user-space memory implicitly: <code>fork</code> allocates the memory required for the child’s copy of the parent’s memory, and <code>exec</code> allocates enough memory to hold the executable file. A process that needs more memory at run-time (perhaps for <code>malloc</code>) can call <code>sbrk(n)</code> to grow its data memory by <code>n</code> bytes; <code>sbrk</code> returns the location of the new memory.</p>
<p>Xv6 does not provide a notion of users or of protecting one user from another; in Unix terms, all xv6 processes run as root.</p>
<h2 id="io-and-file-descriptors">I/O and File descriptors</h2>
<p>A <em>file descriptor</em> is a small integer representing a kernel-managed object that a process may read from or write to. A process may obtain a file descriptor by opening a file, directory, or device, or by creating a pipe, or by duplicating an existing descriptor. For simplicity we’ll often refer to the object a file descriptor refers to as a “file”; the file descriptor interface abstracts away the differences between files, pipes, and devices, making them all look like streams of bytes.</p>
<p>Internally, the xv6 kernel uses the file descriptor as an index into a per-process table, so that every process has a private space of file descriptors starting at zero. By convention, a process reads from file descriptor 0 (standard input), writes output to file descriptor 1 (standard output), and writes error messages to file descriptor 2 (standard error). As we will see, the shell exploits the convention to implement I/O redirection and pipelines. The shell ensures that it always has three file descriptors open <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//user/sh.c#L151"><span>(user/sh.c:151)</span></a>, which are by default file descriptors for the console.</p>
<p>The <code>read</code> and <code>write</code> system calls read bytes from and write bytes to open files named by file descriptors. The call <code>read(fd</code>, <code>buf</code>, <code>n)</code> reads at most <code>n</code> bytes from the file descriptor <code>fd</code>, copies them into <code>buf</code>, and returns the number of bytes read. Each file descriptor that refers to a file has an offset associated with it. <code>Read</code> reads data from the current file offset and then advances that offset by the number of bytes read: a subsequent <code>read</code> will return the bytes following the ones returned by the first <code>read</code>. When there are no more bytes to read, <code>read</code> returns zero to indicate the end of the file.</p>
<p>The call <code>write(fd</code>, <code>buf</code>, <code>n)</code> writes <code>n</code> bytes from <code>buf</code> to the file descriptor <code>fd</code> and returns the number of bytes written. Fewer than <code>n</code> bytes are written only when an error occurs. Like <code>read</code>, <code>write</code> writes data at the current file offset and then advances that offset by the number of bytes written: each <code>write</code> picks up where the previous one left off.</p>
<p>The following program fragment (which forms the essence of the program <code>cat</code>) copies data from its standard input to its standard output. If an error occurs, it writes a message to the standard error.</p>
<pre><code>char buf[512];
int n;

for(;;){
  n = read(0, buf, sizeof buf);
  if(n == 0)
    break;
  if(n &lt; 0){
    fprintf(2, &quot;read error\en&quot;);
    exit();
  }
  if(write(1, buf, n) != n){
    fprintf(2, &quot;write error\en&quot;);
    exit();
  }
}</code></pre>
<p>The important thing to note in the code fragment is that <code>cat</code> doesn’t know whether it is reading from a file, console, or a pipe. Similarly <code>cat</code> doesn’t know whether it is printing to a console, a file, or whatever. The use of file descriptors and the convention that file descriptor 0 is input and file descriptor 1 is output allows a simple implementation of <code>cat</code>.</p>
<p>The <code>close</code> system call releases a file descriptor, making it free for reuse by a future <code>open</code>, <code>pipe</code>, or <code>dup</code> system call (see below). A newly allocated file descriptor is always the lowest-numbered unused descriptor of the current process.</p>
<p>File descriptors and <code>fork</code> interact to make I/O redirection easy to implement. <code>Fork</code> copies the parent’s file descriptor table along with its memory, so that the child starts with exactly the same open files as the parent. The system call <code>exec</code> replaces the calling process’s memory but preserves its file table. This behavior allows the shell to implement I/O redirection by forking, reopening chosen file descriptors, and then execing the new program. Here is a simplified version of the code a shell runs for the command <code>cat</code> <code>&lt;</code> <code>input.txt</code>:</p>
<pre><code>char *argv[2];

argv[0] = &quot;cat&quot;;
argv[1] = 0;
if(fork() == 0) {
  close(0);
  open(&quot;input.txt&quot;, O_RDONLY);
  exec(&quot;cat&quot;, argv);
}</code></pre>
<p>After the child closes file descriptor 0, <code>open</code> is guaranteed to use that file descriptor for the newly opened <code>input.txt</code>: 0 will be the smallest available file descriptor. <code>Cat</code> then executes with file descriptor 0 (standard input) referring to <code>input.txt</code>.</p>
<p>The code for I/O redirection in the xv6 shell works in exactly this way <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//user/sh.c#L82"><span>(user/sh.c:82)</span></a>. Recall that at this point in the code the shell has already forked the child shell and that <code>runcmd</code> will call <code>exec</code> to load the new program. Now it should be clear why it is a good idea that <code>fork</code> and <code>exec</code> are separate calls. Because if they are separate, the shell can fork a child, use <code>open</code>, <code>close</code>, <code>dup</code> in the child to change the standard input and output file descriptors, and then <code>exec</code>. No changes to the program being exec-ed (<code>cat</code> in our example) are required. If <code>fork</code> and <code>exec</code> were combined into a single system call, some other (probably more complex) scheme would be required for the shell to redirect standard input and output, or the program itself would have to understand how to redirect I/O.</p>
<p>Although <code>fork</code> copies the file descriptor table, each underlying file offset is shared between parent and child. Consider this example:</p>
<pre><code>if(fork() == 0) {
  write(1, &quot;hello &quot;, 6);
  exit(0);
} else {
  wait(0);
  write(1, &quot;world\en&quot;, 6);
}</code></pre>
<p>At the end of this fragment, the file attached to file descriptor 1 will contain the data <code>hello</code> <code>world</code>. The <code>write</code> in the parent (which, thanks to <code>wait</code>, runs only after the child is done) picks up where the child’s <code>write</code> left off. This behavior helps produce sequential output from sequences of shell commands, like <code>(echo</code> <code>hello</code>; <code>echo</code> <code>world)</code> <code>&gt;output.txt</code>.</p>
<p>The <code>dup</code> system call duplicates an existing file descriptor, returning a new one that refers to the same underlying I/O object. Both file descriptors share an offset, just as the file descriptors duplicated by <code>fork</code> do. This is another way to write <code>hello</code> <code>world</code> into a file:</p>
<pre><code>fd = dup(1);
write(1, &quot;hello &quot;, 6);
write(fd, &quot;world\en&quot;, 6);</code></pre>
<p>Two file descriptors share an offset if they were derived from the same original file descriptor by a sequence of <code>fork</code> and <code>dup</code> calls. Otherwise file descriptors do not share offsets, even if they resulted from <code>open</code> calls for the same file. <code>Dup</code> allows shells to implement commands like this: <code>ls</code> <code>existing-file</code> <code>non-existing-file</code> <code>&gt;</code> <code>tmp1</code> <code>2&gt;&amp;1</code>. The <code>2&gt;&amp;1</code> tells the shell to give the command a file descriptor 2 that is a duplicate of descriptor 1. Both the name of the existing file and the error message for the non-existing file will show up in the file <code>tmp1</code>. The xv6 shell doesn’t support I/O redirection for the error file descriptor, but now you know how to implement it.</p>
<p>File descriptors are a powerful abstraction, because they hide the details of what they are connected to: a process writing to file descriptor 1 may be writing to a file, to a device like the console, or to a pipe.</p>
<h2 id="pipes">Pipes</h2>
<p>A <em>pipe</em> is a small kernel buffer exposed to processes as a pair of file descriptors, one for reading and one for writing. Writing data to one end of the pipe makes that data available for reading from the other end of the pipe. Pipes provide a way for processes to communicate.</p>
<p>The following example code runs the program <code>wc</code> with standard input connected to the read end of a pipe.</p>
<pre><code>int p[2];
char *argv[2];

argv[0] = &quot;wc&quot;;
argv[1] = 0;

pipe(p);
if(fork() == 0) {
  close(0);
  dup(p[0]);
  close(p[0]);
  close(p[1]);
  exec(&quot;/bin/wc&quot;, argv);
} else {
  close(p[0]);
  write(p[1], &quot;hello world\en&quot;, 12);
  close(p[1]);
}</code></pre>
<p>The program calls <code>pipe</code>, which creates a new pipe and records the read and write file descriptors in the array <code>p</code>. After <code>fork</code>, both parent and child have file descriptors referring to the pipe. The child dups the read end onto file descriptor 0, closes the file descriptors in <code>p</code>, and execs <code>wc</code>. When <code>wc</code> reads from its standard input, it reads from the pipe. The parent closes the read side of the pipe, writes to the pipe, and then closes the write side.</p>
<p>If no data is available, a <code>read</code> on a pipe waits for either data to be written or all file descriptors referring to the write end to be closed; in the latter case, <code>read</code> will return 0, just as if the end of a data file had been reached. The fact that <code>read</code> blocks until it is impossible for new data to arrive is one reason that it’s important for the child to close the write end of the pipe before executing <code>wc</code> above: if one of <code>wc</code> ’s file descriptors referred to the write end of the pipe, <code>wc</code> would never see end-of-file.</p>
<p>The xv6 shell implements pipelines such as <code>grep fork sh.c | wc -l</code> in a manner similar to the above code <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//user/sh.c#L100"><span>(user/sh.c:100)</span></a>. The child process creates a pipe to connect the left end of the pipeline with the right end. Then it calls <code>fork</code> and <code>runcmd</code> for the left end of the pipeline and <code>fork</code> and <code>runcmd</code> for the right end, and waits for both to finish. The right end of the pipeline may be a command that itself includes a pipe (e.g., <code>a</code> <code>|</code> <code>b</code> <code>|</code> <code>c)</code>, which itself forks two new child processes (one for <code>b</code> and one for <code>c</code>). Thus, the shell may create a tree of processes. The leaves of this tree are commands and the interior nodes are processes that wait until the left and right children complete. In principle, you could have the interior nodes run the left end of a pipeline, but doing so correctly would complicate the implementation.</p>
<p>Pipes may seem no more powerful than temporary files: the pipeline</p>
<pre><code>echo hello world | wc</code></pre>
<p>could be implemented without pipes as</p>
<pre><code>echo hello world &gt;/tmp/xyz; wc &lt;/tmp/xyz</code></pre>
<p>Pipes have at least four advantages over temporary files in this situation. First, pipes automatically clean themselves up; with the file redirection, a shell would have to be careful to remove <code>/tmp/xyz</code> when done. Second, pipes can pass arbitrarily long streams of data, while file redirection requires enough free space on disk to store all the data. Third, pipes allow for parallel execution of pipeline stages, while the file approach requires the first program to finish before the second starts. Fourth, if you are implementing inter-process communication, pipes’ blocking reads and writes are more efficient than the non-blocking semantics of files.</p>
<h2 id="file-system">File system</h2>
<p>The xv6 file system provides data files, which are uninterpreted byte arrays, and directories, which contain named references to data files and other directories. The directories form a tree, starting at a special directory called the <em>root</em>. A <em>path</em> like <code>/a/b/c</code> refers to the file or directory named <code>c</code> inside the directory named <code>b</code> inside the directory named <code>a</code> in the root directory <code>/</code>. Paths that don’t begin with <code>/</code> are evaluated relative to the calling process’s <em>current directory</em>, which can be changed with the <code>chdir</code> system call. Both these code fragments open the same file (assuming all the directories involved exist):</p>
<pre><code>chdir(&quot;/a&quot;);
chdir(&quot;b&quot;);
open(&quot;c&quot;, O_RDONLY);

open(&quot;/a/b/c&quot;, O_RDONLY);</code></pre>
<p>The first fragment changes the process’s current directory to <code>/a/b</code>; the second neither refers to nor changes the process’s current directory.</p>
<p>There are multiple system calls to create a new file or directory: <code>mkdir</code> creates a new directory, <code>open</code> with the <code>O_CREATE</code> flag creates a new data file, and <code>mknod</code> creates a new device file. This example illustrates all three:</p>
<pre><code>mkdir(&quot;/dir&quot;);
fd = open(&quot;/dir/file&quot;, O_CREATE|O_WRONLY);
close(fd);
mknod(&quot;/console&quot;, 1, 1);</code></pre>
<p><code>Mknod</code> creates a file in the file system, but the file has no contents. Instead, the file’s metadata marks it as a device file and records the major and minor device numbers (the two arguments to <code>mknod</code>), which uniquely identify a kernel device. When a process later opens the file, the kernel diverts <code>read</code> and <code>write</code> system calls to the kernel device implementation instead of passing them to the file system.</p>
<p><code>Fstat</code> retrieves information about the object a file descriptor refers to. It fills in a <code>struct</code> <code>stat</code>, defined in <code>stat.h</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/stat.h"><span>(kernel/stat.h)</span></a> as:</p>
<pre><code>#define T_DIR     1   // Directory
#define T_FILE    2   // File
#define T_DEVICE  3   // Device

struct stat {
  int dev;     // File system&#39;s disk device
  uint ino;    // Inode number
  short type;  // Type of file
  short nlink; // Number of links to file
  uint64 size; // Size of file in bytes
};</code></pre>
<p>A file’s name is distinct from the file itself; the same underlying file, called an <em>inode</em>, can have multiple names, called <em>links</em>. The <code>link</code> system call creates another file system name referring to the same inode as an existing file. This fragment creates a new file named both <code>a</code> and <code>b</code>.</p>
<pre><code>open(&quot;a&quot;, O_CREATE|O_WRONLY);
link(&quot;a&quot;, &quot;b&quot;);</code></pre>
<p>Reading from or writing to <code>a</code> is the same as reading from or writing to <code>b</code>. Each inode is identified by a unique <em>inode</em> <em>number</em>. After the code sequence above, it is possible to determine that <code>a</code> and <code>b</code> refer to the same underlying contents by inspecting the result of <code>fstat</code>: both will return the same inode number (<code>ino</code>), and the <code>nlink</code> count will be set to 2.</p>
<p>The <code>unlink</code> system call removes a name from the file system. The file’s inode and the disk space holding its content are only freed when the file’s link count is zero and no file descriptors refer to it. Thus adding</p>
<pre><code>unlink(&quot;a&quot;);</code></pre>
<p>to the last code sequence leaves the inode and file content accessible as <code>b</code>. Furthermore,</p>
<pre><code>fd = open(&quot;/tmp/xyz&quot;, O_CREATE|O_RDWR);
unlink(&quot;/tmp/xyz&quot;);</code></pre>
<p>is an idiomatic way to create a temporary inode that will be cleaned up when the process closes <code>fd</code> or exits.</p>
<p>Shell commands for file system operations are implemented as user-level programs such as <code>mkdir</code>, <code>ln</code>, <code>rm</code>, etc. This design allows anyone to extend the shell with new user commands by just adding a new user-level program. In hindsight this plan seems obvious, but other systems designed at the time of Unix often built such commands into the shell (and built the shell into the kernel).</p>
<p>One exception is <code>cd</code>, which is built into the shell <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//user/sh.c#L160"><span>(user/sh.c:160)</span></a>. <code>cd</code> must change the current working directory of the shell itself. If <code>cd</code> were run as a regular command, then the shell would fork a child process, the child process would run <code>cd</code>, and <code>cd</code> would change the <em>child</em> ’s working directory. The parent’s (i.e., the shell’s) working directory would not change.</p>
<h2 id="real-world">Real world</h2>
<p>Unix’s combination of “standard” file descriptors, pipes, and convenient shell syntax for operations on them was a major advance in writing general-purpose reusable programs. The idea sparked a whole culture of “software tools” that was responsible for much of Unix’s power and popularity, and the shell was the first so-called “scripting language.” The Unix system call interface persists today in systems like BSD, Linux, and Mac OS X.</p>
<p>The Unix system call interface has been standardized through the Portable Operating System Interface (POSIX) standard. Xv6 is <em>not</em> POSIX compliant. It misses system calls (including basic ones such as <code>lseek</code>), it implements system calls only partially, as well as other differences. Our main goals for xv6 are simplicity and clarity while providing a simple UNIX-like system-call interface. Several people have extended xv6 with a few more system calls and a simple C library in order to run basic Unix programs. Modern kernels, however, provide many more system calls, and many more kinds of kernel services, than xv6. For example, they support networking, windowing systems, user-level threads, drivers for many devices, and so on. Modern kernels evolve continuously and rapidly, and offer many features beyond POSIX.</p>
<p>For the most part, modern Unix-derived operating systems have not followed the early Unix model of exposing devices as special files, like the <code>console</code> device file discussed above. The authors of Unix went on to build Plan 9, which applied the “resources are files” concept to modern facilities, representing networks, graphics, and other resources as files or file trees.</p>
<p>The file system and file descriptors have been powerful abstractions. Even so, there are other models for operating system interfaces. Multics, a predecessor of Unix, abstracted file storage in a way that made it look like memory, producing a very different flavor of interface. The complexity of the Multics design had a direct influence on the designers of Unix, who tried to build something simpler.</p>
<p>This book examines how xv6 implements its Unix-like interface, but the ideas and concepts apply to more than just Unix. Any operating system must multiplex processes onto the underlying hardware, isolate processes from each other, and provide mechanisms for controlled inter-process communication. After studying xv6, you should be able to look at other, more complex operating systems and see the concepts underlying xv6 in those systems as well.</p>
<h2 id="exercises">Exercises</h2>
<ol>
<li><p>Write a program that uses UNIX system calls to “ping-pong” a byte between two processes over a pair of pipes, one for each direction. Measure the program’s performance, in exchanges per second.</p></li>
</ol>
<h1 id="CH:FIRST">Operating system organization</h1>
<p>A key requirement for an operating system is to support several activities at once. For example, using the system call interface described in chapter <a href="#CH:UNIX" data-reference-type="ref" data-reference="CH:UNIX">1</a> a process can start new processes with <code>fork</code>. The operating system must <em>time-share</em> the resources of the computer among these processes. For example, even if there are more processes than there are hardware CPUs, the operating system must ensure that all of the processes make progress. The operating system must also arrange for <em>isolation</em> between the processes. That is, if one process has a bug and fails, it shouldn’t affect processes that don’t depend on the failed process. Complete isolation, however, is too strong, since it should be possible for processes to intentionally interact; pipelines are an example. Thus an operating system must fulfill three requirements: multiplexing, isolation, and interaction.</p>
<p>This chapter provides an overview of how operating systems are organized to achieve these three requirements. It turns out there are many ways to do so, but this text focuses on mainstream designs centered around a <em>monolithic kernel</em>, which is used by many Unix operating systems. This chapter also provides an overview of an xv6 process, which is the unit of isolation in xv6, and the creation of the first process when xv6 starts running.</p>
<p>Xv6 runs on multi-core RISC-V microprocessor, and much of its low-level functionality (for example, its process implementation) is specific to RISC-V. RISC-V is a 64-bit CPU, and xv6 is written in “LP64” C, which means long (L) and pointers (P) in the C programming language are 64 bits, but int is 32-bit. This book assumes the reader has done a bit of machine-level programming on some architecture, and will introduce RISC-V-specific ideas as they come up. A useful reference for RISC-V is “The RISC-V Reader: An Open Architecture Atlas” <span class="citation" data-cites="riscv"></span>. The user-level ISA <span class="citation" data-cites="riscv:user"></span> and the privileged architecture <span class="citation" data-cites="riscv:priv"></span> are the official specifications.</p>
<p>This text generally refers to the hardware element that executes a computation with the term <em>CPU</em>, an acronym for central processing unit. Other documentation (e.g., the RISC-V specification) also uses the words processor, core, and hart instead of CPU. Xv6 expects <em>multi-core</em> RISC-V hardware; that is, multiple CPUs that share memory but execute independent programs in parallel. This text sometimes uses the term <em>multiprocessor</em> as a synonym for multi-core, though the former term can also refer to a computer board with several distinct processor chips.</p>
<p>The CPU in a complete computer is surrounded by support hardware, much of it in the form of I/O interfaces. Xv6 is written for the support hardware simulated by qemu’s “-machine virt” option. This includes RAM, a ROM containing boot code, a serial connection to the user’s keyboard/screen, and a disk for storage.</p>
<h2 id="abstracting-physical-resources">Abstracting physical resources</h2>
<p>The first question one might ask when encountering an operating system is why have it at all? That is, one could implement the system calls in Figure <a href="#fig:api" data-reference-type="ref" data-reference="fig:api">[fig:api]</a> as a library, with which applications link. In this plan, each application could even have its own library tailored to its needs. Applications could directly interact with hardware resources and use those resources in the best way for the application (e.g., to achieve high or predictable performance). Some operating systems for embedded devices or real-time systems are organized in this way.</p>
<p>The downside of this library approach is that, if there is more than one application running, the applications must be well-behaved. For example, each application must periodically give up the CPU so that other applications can run. Such a <em>cooperative</em> time-sharing scheme may be OK if all applications trust each other and have no bugs. It’s more typical for applications to not trust each other, and to have bugs, so one often wants stronger isolation than a cooperative scheme provides.</p>
<p>To achieve strong isolation it’s helpful to forbid applications from directly accessing sensitive hardware resources, and instead to abstract the resources into services. For example, applications interact with a file system only through <code>open</code>, <code>read</code>, <code>write</code>, and <code>close</code> system calls, instead of reading and writing raw disk sectors. This provides the application with the convenience of pathnames, and it allows the operating system (as the implementer of the interface) to manage the disk.</p>
<p>Similarly, Unix transparently switches hardware CPUs among processes, saving and restoring register state as necessary, so that applications don’t have to be aware of time sharing. This transparency allows the operating system to share CPUs even if some applications are in infinite loops.</p>
<p>As another example, Unix processes use <code>exec</code> to build up their memory image, instead of directly interacting with physical memory. This allows the operating system to decide where to place a process in memory; if memory is tight, the operating system might even store some of a process’s data on disk. <code>Exec</code> also provides users with the convenience of a file system to store executable program images.</p>
<p>Many forms of interaction among Unix processes occur via file descriptors. Not only do file descriptors abstract away many details (e.g., where data in a pipe or file is stored), they are also defined in a way that simplifies interaction. For example, if one application in a pipeline fails, the kernel generates an end-of-file signal for the next process in the pipeline.</p>
<p>As you can see, the system-call interface in Figure <a href="#fig:api" data-reference-type="ref" data-reference="fig:api">[fig:api]</a> is carefully designed to provide both programmer convenience and the possibility of strong isolation. The Unix interface is not the only way to abstract resources, but it has proven to be a very good one.</p>
<h2 id="user-mode-supervisor-mode-and-system-calls">User mode, supervisor mode, and system calls</h2>
<p>Strong isolation requires a hard boundary between applications and the operating system. If the application makes a mistake, we don’t want the operating system to fail or other applications to fail. Instead, the operating system should be able to clean up the failed application and continue running other applications. To achieve strong isolation, the operating system must arrange that applications cannot modify (or even read) the operating system’s data structures and instructions and that applications cannot access other processes’ memory.</p>
<p>CPUs provide hardware support for strong isolation. For example, RISC-V has three modes in which the CPU can execute instructions: <em>machine mode</em>, <em>supervisor mode</em>, and <em>user mode</em>. Instructions executing in machine mode have full privilege; a CPU starts in machine mode. Machine mode is mostly intended for configuring a computer. Xv6 executes a few lines in machine mode and then changes to supervisor mode.</p>
<p>In supervisor mode the CPU is allowed to execute <em>privileged instructions</em>: for example, enabling and disabling interrupts, reading and writing the register that holds the address of a page table, etc. If an application in user mode attempts to execute a privileged instruction, then the CPU doesn’t execute the instruction, but switches to supervisor mode so that supervisor-mode code can terminate the application, because it did something it shouldn’t be doing. Figure <a href="#fig:os" data-reference-type="ref" data-reference="fig:os">1.1</a> in Chapter <a href="#CH:UNIX" data-reference-type="ref" data-reference="CH:UNIX">1</a> illustrates this organization. An application can execute only user-mode instructions (e.g., adding numbers, etc.) and is said to be running in <em>user space</em>, while the software in supervisor mode can also execute privileged instructions and is said to be running in <em>kernel space</em>. The software running in kernel space (or in supervisor mode) is called the <em>kernel</em>.</p>
<p>An application that wants to invoke a kernel function (e.g., the <code>read</code> system call in xv6) must transition to the kernel. CPUs provide a special instruction that switches the CPU from user mode to supervisor mode and enters the kernel at an entry point specified by the kernel. (RISC-V provides the <code>ecall</code> instruction for this purpose.) Once the CPU has switched to supervisor mode, the kernel can then validate the arguments of the system call, decide whether the application is allowed to perform the requested operation, and then deny it or execute it. It is important that the kernel control the entry point for transitions to supervisor mode; if the application could decide the kernel entry point, a malicious application could, for example, enter the kernel at a point where the validation of arguments is skipped.</p>
<h2 id="kernel-organization">Kernel organization</h2>
<p>A key design question is what part of the operating system should run in supervisor mode. One possibility is that the entire operating system resides in the kernel, so that the implementations of all system calls run in supervisor mode. This organization is called a <em>monolithic kernel</em>.</p>
<p>In this organization the entire operating system runs with full hardware privilege. This organization is convenient because the OS designer doesn’t have to decide which part of the operating system doesn’t need full hardware privilege. Furthermore, it easy for different parts of the operating system to cooperate. For example, an operating system might have a buffer cache that can be shared both by the file system and the virtual memory system.</p>
<p>A downside of the monolithic organization is that the interfaces between different parts of the operating system are often complex (as we will see in the rest of this text), and therefore it is easy for an operating system developer to make a mistake. In a monolithic kernel, a mistake is fatal, because an error in supervisor mode will often cause the kernel to fail. If the kernel fails, the computer stops working, and thus all applications fail too. The computer must reboot to start again.</p>
<p>To reduce the risk of mistakes in the kernel, OS designers can minimize the amount of operating system code that runs in supervisor mode, and execute the bulk of the operating system in user mode. This kernel organization is called a <em>microkernel</em>.</p>
<figure>
<embed src="fig/mkernel.svg" id="fig:mkernel" /><figcaption>A microkernel with a file-system server</figcaption>
</figure>
<p>Figure <a href="#fig:mkernel" data-reference-type="ref" data-reference="fig:mkernel">2.1</a> illustrates this microkernel design. In the figure, the file system runs as a user-level process. OS services running as processes are called servers. To allow applications to interact with the file server, the kernel provides an inter-process communication mechanism to send messages from one user-mode process to another. For example, if an application like the shell wants to read or write a file, it sends a message to the file server and waits for a response.</p>
<p>In a microkernel, the kernel interface consists of a few low-level functions for starting applications, sending messages, accessing device hardware, etc. This organization allows the kernel to be relatively simple, as most of the operating system resides in user-level servers.</p>
<p>Xv6 is implemented as a monolithic kernel, like most Unix operating systems. Thus, the xv6 kernel interface corresponds to the operating system interface, and the kernel implements the complete operating system. Since xv6 doesn’t provide many services, its kernel is smaller than some microkernels, but conceptually xv6 is monolithic.</p>
<h2 id="code-xv6-organization">Code: xv6 organization</h2>
<table>
<thead>
<tr class="header">
<th style="text-align: left;"><span><strong>File</strong></span></th>
<th style="text-align: left;"><span><strong>Description</strong></span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">bio.c</td>
<td style="text-align: left;">Disk block cache for the file system.</td>
</tr>
<tr class="even">
<td style="text-align: left;">console.c</td>
<td style="text-align: left;">Connect to the user keyboard and screen.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">entry.S</td>
<td style="text-align: left;">Very first boot instructions.</td>
</tr>
<tr class="even">
<td style="text-align: left;">exec.c</td>
<td style="text-align: left;">exec() system call.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">file.c</td>
<td style="text-align: left;">File descriptor support.</td>
</tr>
<tr class="even">
<td style="text-align: left;">fs.c</td>
<td style="text-align: left;">File system.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">kalloc.c</td>
<td style="text-align: left;">Physical page allocator.</td>
</tr>
<tr class="even">
<td style="text-align: left;">kernelvec.S</td>
<td style="text-align: left;">Handle traps from kernel, and timer interrupts.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">log.c</td>
<td style="text-align: left;">File system logging and crash recovery.</td>
</tr>
<tr class="even">
<td style="text-align: left;">main.c</td>
<td style="text-align: left;">Control initialization of other modules during boot.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">pipe.c</td>
<td style="text-align: left;">Pipes.</td>
</tr>
<tr class="even">
<td style="text-align: left;">plic.c</td>
<td style="text-align: left;">RISC-V interrupt controller.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">printf.c</td>
<td style="text-align: left;">Formatted output to the console.</td>
</tr>
<tr class="even">
<td style="text-align: left;">proc.c</td>
<td style="text-align: left;">Processes and scheduling.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">sleeplock.c</td>
<td style="text-align: left;">Locks that yield the CPU.</td>
</tr>
<tr class="even">
<td style="text-align: left;">spinlock.c</td>
<td style="text-align: left;">Locks that don’t yield the CPU.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">start.c</td>
<td style="text-align: left;">Early machine-mode boot code.</td>
</tr>
<tr class="even">
<td style="text-align: left;">string.c</td>
<td style="text-align: left;">C string and byte-array library.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">swtch.S</td>
<td style="text-align: left;">Thread switching.</td>
</tr>
<tr class="even">
<td style="text-align: left;">syscall.c</td>
<td style="text-align: left;">Dispatch system calls to handling function.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">sysfile.c</td>
<td style="text-align: left;">File-related system calls.</td>
</tr>
<tr class="even">
<td style="text-align: left;">sysproc.c</td>
<td style="text-align: left;">Process-related system calls.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">trampoline.S</td>
<td style="text-align: left;">Assembly code to switch between user and kernel.</td>
</tr>
<tr class="even">
<td style="text-align: left;">trap.c</td>
<td style="text-align: left;">C code to handle and return from traps and interrupts.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">uart.c</td>
<td style="text-align: left;">Serial-port console device driver.</td>
</tr>
<tr class="even">
<td style="text-align: left;">virtio_disk.c</td>
<td style="text-align: left;">Disk device driver.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">vm.c</td>
<td style="text-align: left;">Manage page tables and address spaces.</td>
</tr>
</tbody>
</table>
<p>The xv6 kernel source is in the <span><code>kernel/</code></span> sub-directory. The source is divided into files, following a rough notion of modularity; Figure <a href="#fig:source" data-reference-type="ref" data-reference="fig:source">[fig:source]</a> lists the files. The interface for each module is defined in <code>defs.h</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/defs.h"><span>(kernel/defs.h)</span></a>.</p>
<h2 id="process-overview">Process overview</h2>
<p>The unit of isolation in xv6 (as in other Unix operating systems) is a <em>process</em>. The process abstraction prevents one process from wrecking or spying on another process’s memory, CPU, file descriptors, etc. It also prevents a process from wrecking the kernel itself, so that a process can’t subvert the kernel’s isolation mechanisms. The kernel must implement the process abstraction with care because a buggy or malicious application may trick the kernel or hardware into doing something bad (e.g., circumventing isolation). The mechanisms used by the kernel to implement processes include the user/supervisor mode flag, address spaces, and time-slicing of threads.</p>
<p>To help enforce isolation, the process abstraction provides the illusion to a program that it has its own private machine. A process provides a program with what appears to be a private memory system, or <em>address space</em>, which other processes cannot read or write. A process also provides the program with what appears to be its own CPU to execute the program’s instructions.</p>
<p>Xv6 uses page tables (which are implemented by hardware) to give each process its own address space. The RISC-V page table translates (or “maps”) a <em>virtual address</em> (the address that an RISC-V instruction manipulates) to a <em>physical address</em> (an address that the CPU chip sends to main memory).</p>
<figure>
<embed src="fig/as.svg" id="fig:as" /><figcaption>Layout of a virtual address space of a user process</figcaption>
</figure>
<p>Xv6 maintains a separate page table for each process that defines that process’s address space. As illustrated in Figure <a href="#fig:as" data-reference-type="ref" data-reference="fig:as">2.2</a>, an address space includes the process’s <em>user memory</em> starting at virtual address zero. Instructions come first, followed by global variables, then the stack, and finally a “heap” area (for malloc) that the process can expand as needed. Xv6 runs on RISC-V with 39 bits for virtual addresses, but uses only 38 bits. Thus, the maximum address is <span class="math inline">2<sup>38</sup> − 1</span> = 0x3fffffffff, which is <code>MAXVA</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/riscv.h#L349"><span>(kernel/riscv.h:349)</span></a>. At the top of the address space xv6 reserves a page for a <em>trampoline</em> and a page mapping the process’s <em>trapframe</em> to switch to the kernel, as we will explain in Chapter <a href="#CH:TRAP" data-reference-type="ref" data-reference="CH:TRAP">4</a>.</p>
<p>The xv6 kernel maintains many pieces of state for each process, which it gathers into a <code>struct proc</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.h#L86"><span>(kernel/proc.h:86)</span></a>. A process’s most important pieces of kernel state are its page table, its kernel stack, and its run state. We’ll use the notation <code>p-&gt;xxx</code> to refer to elements of the <code>proc</code> structure. The trapframe mentioned above is <code>p-&gt;tf</code>.</p>
<p>Each process has a thread of execution (or <em>thread</em> for short) that executes the process’s instructions. A thread can be suspended and later resumed. To switch transparently between processes, the kernel suspends the currently running thread and resumes another process’s thread. Much of the state of a thread (local variables, function call return addresses) is stored on the thread’s stacks. Each process has two stacks: a user stack and a kernel stack (<code>p-&gt;kstack</code>). When the process is executing user instructions, only its user stack is in use, and its kernel stack is empty. When the process enters the kernel (for a system call or interrupt), the kernel code executes on the process’s kernel stack; while a process is in the kernel, its user stack still contains saved data, but isn’t actively used. A process’s thread alternates between actively using its user stack and its kernel stack. The kernel stack is separate (and protected from user code) so that the kernel can execute even if a process has wrecked its user stack.</p>
<p>A process can make a system call by executing the RISC-V <code>ecall</code> instruction. This instruction raises the hardware privilege level and changes the program counter to a kernel-defined entry point. The code at the entry point switches to a kernel stack and executes the kernel instructions that implement the system call. When the system call completes, the kernel switches back to the user stack and returns to user space by calling the <code>sret</code> instruction, which lowers the hardware privilege level and resumes executing user instructions just after the system call instruction. A process’s thread can “block” in the kernel to wait for I/O, and resume where it left off when the I/O has finished.</p>
<p><code>p-&gt;state</code> indicates whether the process is allocated, ready to run, running, waiting for I/O, or exiting.</p>
<p><code>p-&gt;pagetable</code> holds the process’s page table, in the format that the RISC-V hardware expects. xv6 causes the paging hardware to use a process’s <code>p-&gt;pagetable</code> when executing that process in user space. A process’s page table also serves as the record of the addresses of the physical pages allocated to store the process’s memory.</p>
<h2 id="code-starting-xv6-and-the-first-process">Code: starting xv6 and the first process</h2>
<p>To make xv6 more concrete, we’ll outline how the kernel starts and runs the first process. The subsequent chapters will describe the mechanisms that show up in this overview in more detail.</p>
<p>When the RISC-V computer powers on, it initializes itself and runs a boot loader which is stored in read-only memory. The boot loader loads the xv6 kernel into memory. Then, in machine mode, the CPU executes xv6 starting at <code>_entry</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/entry.S#L12"><span>(kernel/entry.S:12)</span></a>. Xv6 starts with the RISC-V paging hardware disabled: virtual addresses map directly to physical addresses.</p>
<p>The loader loads the xv6 kernel into memory at physical address <code>0x80000000</code>. The reason it places the kernel at <code>0x80000000</code> rather than <code>0x0</code> is because the address range <code>0x0:0x80000000</code> contains I/O devices.</p>
<p>The instructions at <code>_entry</code> set up a stack so that xv6 can run C code. Xv6 declares space for an initial stack, <code>stack0</code>, in the file <code>start.c</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/start.c#L11"><span>(kernel/start.c:11)</span></a>. The code at <code>_entry</code> loads the stack pointer register <code>sp</code> with the address <code>stack0+4096</code>, the top of the stack, because the stack on RISC-V grows down. Now that we have a stack, <code>_entry</code> calls into C code at <code>start</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/start.c#L21"><span>(kernel/start.c:21)</span></a>.</p>
<p>The function <code>start</code> performs some configuration that is only allowed in machine mode, and then switches to supervisor mode. To enter supervisor mode, RISC-V provides the instruction <code>mret</code>. This instruction is most often used to return from a previous call from supervisor mode to machine mode. <code>start</code> isn’t returning from such a call, and instead sets things up as if there had been one: it sets the previous privilege mode to supervisor in the register <code>mstatus</code>, it sets the return address to <code>main</code> by writing <code>main</code>’s address into the register <code>mepc</code>, disables virtual address translation in supervisor mode by writing <code>0</code> into the page-table register <code>satp</code>, and delegates all interrupts and exceptions to supervisor mode.</p>
<p>Before jumping into supervisor mode, <code>start</code> performs one more task: it programs the clock chip to generate timer interrupts. With this housekeeping out of the way, <code>start</code> “returns” to supervisor mode by calling <code>mret</code>. This causes the program counter to change to <code>main</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/main.c#L11"><span>(kernel/main.c:11)</span></a>.</p>
<p>After <code>main</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/main.c#L11"><span>(kernel/main.c:11)</span></a> initializes several devices and subsystems, it creates the first process by calling <code>userinit</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L197"><span>(kernel/proc.c:197)</span></a>. The first process executes a small program, <code>initcode.S</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//user/initcode.S#L1"><span>(user/initcode.S:1)</span></a>, which re-enters the kernel by invoking the <code>exec</code> system call. As we saw in Chapter <a href="#CH:UNIX" data-reference-type="ref" data-reference="CH:UNIX">1</a>, <code>exec</code> replaces the memory and registers of the current process with a new program (in this case, <code>/init</code>). Once the kernel has completed <code>exec</code>, it returns to user space and runs <code>/init</code>. <code>Init</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//user/init.c#L11"><span>(user/init.c:11)</span></a> creates a new console device file if needed and then opens it as file descriptors 0, 1, and 2. Then it loops, starting a console shell, handles orphaned zombies until the shell exits, and repeats. The system is up.</p>
<h2 id="real-world-1">Real world</h2>
<p>In the real world, one can find both monolithic kernels and microkernels. Many Unix kernels are monolithic. For example, Linux has a monolithic kernel, although some OS functions run as user-level servers (e.g., the windowing system). Kernels such as L4, Minix, and QNX are organized as a microkernel with servers, and have seen wide deployment in embedded settings.</p>
<p>Most operating systems have adopted the process concept, and most processes look similar to xv6’s. Modern operating systems, however, support several threads within a process, to allow a single process to exploit multiple CPUs. Supporting multiple threads in a process involves quite a bit of machinery that xv6 doesn’t have, including potential interface changes (e.g., Linux’s <code>clone</code>, a variant of <code>fork</code>), to control which parts of a process threads share.</p>
<h2 id="exercises-1">Exercises</h2>
<ol>
<li><p>You can use gdb to observe the very first kernel-to-user transition. Run <span><code>make qemu-gdb</code></span>. In another window, in the same directory, run <span><code>gdb</code></span>. Type the gdb command <span><code>break *0x3ffffff07e</code></span>, which sets a breakpoint at the <span><code>sret</code></span> instruction in the kernel that jumps into user space. Type the <span><code>continue</code></span> gdb command. gdb should stop at the breakpoint, about to execute <span><code>sret</code></span>. Type <span><code>stepi</code></span>. gdb should now indicate that it is executing at address 0x4, which is in user space at the start of <span><code>initcode.S</code></span>.</p></li>
</ol>
<h1 id="CH:MEM">Page tables</h1>
<p>Page tables are the mechanism through which the operating system provides each process with its own private address space and memory. Page tables determine what memory addresses mean, and what parts of physical memory can be accessed. They allow xv6 to isolate different process’s address spaces and to multiplex them onto a single physical memory. Page tables also provide a level of indirection that allows xv6 to perform a few tricks: mapping the same memory (a trampoline page) in several address spaces, and guarding kernel and user stacks with an unmapped page. The rest of this chapter explains the page tables that the RISC-V hardware provides and how xv6 uses them.</p>
<h2 id="paging-hardware">Paging hardware</h2>
<p>As a reminder, RISC-V instructions (both user and kernel) manipulate virtual addresses. The machine’s RAM, or physical memory, is indexed with physical addresses. The RISC-V page table hardware connects these two kinds of addresses, by mapping each virtual address to a physical address.</p>
<figure>
<embed src="fig/riscv_address.svg" id="fig:riscv_address" /><figcaption>RISC-V virtual and physical addresses.</figcaption>
</figure>
<p>xv6 runs on Sv39 RISC-V, which has 39-bit virtual addresses (see Figure <a href="#fig:riscv_address" data-reference-type="ref" data-reference="fig:riscv_address">3.1</a>). The top 25 bits of a 64-bit virtual address are unused. In this Sv39 configuration, a RISC-V page table is logically an array of <span class="math inline">2<sup>27</sup></span> (134,217,728) <em>page table entries (PTEs)</em>. Each PTE contains a 44-bit physical page number (PPN) and some flags. The paging hardware translates a virtual address by using the top 27 bits of the 39 bits to index into the page table to find a PTE, and making a 56-bit physical address whose top 44 bits come from the PPN in the PTE and whose bottom 12 bits are copied from the original virtual address. Thus a page table gives the operating system control over virtual-to-physical address translations at the granularity of aligned chunks of 4096 (<span class="math inline">2<sup>12</sup></span>) bytes. Such a chunk is called a <em>page</em>.</p>
<figure>
<embed src="fig/riscv_pagetable.svg" id="fig:riscv_pagetable" /><figcaption>RISC-V page table hardware.</figcaption>
</figure>
<p>In Sv39 RISC-V, the top 25 bits of a virtual address are not used for translation; in the future, RISC-V may use those bits to define more levels of translation. Similarly, the physical address has room for growth; in Sv39 it is 56 bits, but could grow to 64 bits.</p>
<p>As shown in Figure <a href="#fig:riscv_pagetable" data-reference-type="ref" data-reference="fig:riscv_pagetable">3.2</a>, the actual translation happens in three steps. A page table is stored in physical memory as a three-level tree. The root of the tree is a 4096-byte page-table page that contains 512 PTEs, which contain the physical addresses for page-table pages in the next level of the tree. Each of those pages contains 512 PTEs for the final level in the tree. The paging hardware uses the top 9 bits of the 27 bits to select a PTE in the root ptable-table page, the middle 9 bits to select a PTE in a page-table page in the next level of the tree, and the bottom 9 bits to select the final PTE.</p>
<p>If any of the three PTEs required to translate an address is not present, the paging hardware raises a fault. This three-level structure allows a page table to omit entire page table pages in the common case in which large ranges of virtual addresses have no mappings.</p>
<p>Each PTE contains flag bits that tell the paging hardware how the associated virtual address is allowed to be used. <code>PTE_V</code> indicates whether the PTE is present: if it is not set, a reference to the page causes a fault (i.e. is not allowed). <code>PTE_R</code> controls whether instructions are allowed to read to the page. <code>PTE_W</code> controls whether instructions are allowed to write to the page. <code>PTE_X</code> controls whether the CPU may interpret the content of the page as instructions and execute them. <code>PTE_U</code> controls whether instructions in user mode are allowed to access the page; if <code>PTE_U</code> is not set, the PTE can be used only in supervisor mode. Figure <a href="#fig:riscv_pagetable" data-reference-type="ref" data-reference="fig:riscv_pagetable">3.2</a> shows how it all works. The flags and all other page hardware-related structures are defined in <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/riscv.h"><span>(kernel/riscv.h)</span></a></p>
<p>To tell the hardware to use a page table, the kernel must write the physical address of the root page-table page into the <code>satp</code> register. Each CPU has its own <code>satp</code>. A CPU will translate all addresses generated by subsequent instructions using the page table pointed to by its own <code>satp</code>. Each CPU has its own <code>satp</code> so that different CPUs can run different processes, each with a private address space described by its own page table.</p>
<p>A few notes about terms. Physical memory refers to storage cells in DRAM. A byte of physical memory has an address, called a physical address. Instructions use only virtual addresses, which the paging hardware translates to physical addresses, and then sends to the DRAM hardware to read or write storage. Virtual memory refers to the collection of abstractions and mechanisms the kernel provides to manage physical memory and virtual addresses.</p>
<figure>
<embed src="fig/xv6_layout.svg" id="fig:xv6_layout" /><figcaption>On the left, xv6’s kernel address space. <span><span>RWX</span></span> refer to PTE read, write, and execute permissions. On the right, the RISC-V physical address space that xv6 expects to see.</figcaption>
</figure>
<h2 id="kernel-address-space">Kernel address space</h2>
<p>The kernel has its own page table. When a process enters the kernel, xv6 switches to the kernel page table, and when the kernel returns to user space, it switches to the page table of the user process. The memory of the kernel is private.</p>
<p>Figure <a href="#fig:xv6_layout" data-reference-type="ref" data-reference="fig:xv6_layout">3.3</a> shows the layout of the kernel address space, and the mapping from virtual addresses to physical addresses. The file <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/memlayout.h"><span>(kernel/memlayout.h)</span></a> declares the constants for xv6’s kernel memory layout.</p>
<p>QEMU simulates a computer that includes I/O devices such as a disk interface. QEMU exposes the device interfaces to software as <em>memory-mapped</em> control registers that sit below <code>0x80000000</code> in physical memory. The kernel can interact with the devices by reading/writing these memory locations. Chapter <a href="#CH:TRAP" data-reference-type="ref" data-reference="CH:TRAP">4</a> explains how xv6 interacts with devices.</p>
<p>The kernel uses an identity mapping for most virtual addresses; that is, most of the kernel’s address space is “direct-mapped.” For example, the kernel itself is located at <code>KERNBASE</code> in both the virtual address space and in physical memory. This direct mapping simplifies kernel code that needs to both read or write a page (with its virtual address) and also manipulate a PTE that refers to the same page (with its physical address). There are a couple of virtual addresses that aren’t direct-mapped:</p>
<ul>
<li><p>The trampoline page. It is mapped at the top of the virtual address space; user page tables have this same mapping. In Chapter <a href="#CH:TRAP" data-reference-type="ref" data-reference="CH:TRAP">4</a>, we will discuss the role of the trampoline page, but we see here an interesting use case of page tables; a physical page (holding the trampoline code) is mapped twice in the virtual address space of the kernel: once at top of the virtual address space and once in the kernel text.</p></li>
<li><p>The kernel stack pages. Each process has its own kernel stack, which is mapped high so that below it xv6 can leave an unmapped <em>guard page</em>. The guard page’s PTE is invalid (i.e., <code>PTE_V</code> is not set), which ensures that if the kernel overflows a kernel stack, it will likely cause a fault and the kernel will panic. Without a guard page an overflowing stack would overwrite other kernel memory, resulting in incorrect operation. A panic crash is preferable.</p></li>
</ul>
<p>While the kernel uses its stacks via the high-memory mappings, they are also accessible to the kernel through a direct-mapped address. An alternate design might have just the direct mapping, and use the stacks at the direct-mapped address. In that arrangement, however, providing guard pages would involve unmapping virtual addresses that would otherwise refer to physical memory, which would then be hard to use.</p>
<p>The kernel maps the pages for the trampoline and the kernel text with the permissions <code>PTE_R</code> and <code>PTE_X</code>. The kernel reads and executes instructions from these pages. The kernel maps the other pages with the permissions <code>PTE_R</code> and <code>PTE_W</code>, so that it can read and write the memory in those pages. The mappings for the guard pages are invalid.</p>
<h2 id="code-creating-an-address-space">Code: creating an address space</h2>
<p>Most of the xv6 code for manipulating address spaces and page tables resides in <span><code>vm.c</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/vm.c#L1"><span>(kernel/vm.c:1)</span></a>. The central data structure is <span><code>pagetable_t</code></span>, which is really a pointer to a RISC-V root page-table page; a <span><code>pagetable_t</code></span> may be either the kernel page table, or one of the per-process page tables. The central functions are <span><code>walk</code></span>, which finds the PTE for a virtual address, and <span><code>mappages</code></span>, which installs PTEs for new mappings. Functions starting with <span><code>kvm</code></span> manipulate the kernel page table; functions starting with <span><code>uvm</code></span> manipulate a user page table; other functions are used for both. <span><code>copyout</code></span> and <span><code>copyin</code></span> copy data to and from user virtual addresses provided as system call arguments; they are in <span><code>vm.c</code></span> because they need to explicitly translate those addresses in order to find the corresponding physical memory.</p>
<p>Early in the boot sequence, <code>main</code> calls <code>kvminit</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/vm.c#L24"><span>(kernel/vm.c:24)</span></a> to create the kernel’s page table. This call occurs before xv6 has enabled paging on the RISC-V, so addresses refer directly to physical memory. <code>Kvminit</code> first allocates a page of physical memory to hold the root page-table page. Then it calls <code>kvmmap</code> to install the translations that the kernel needs. The translations include the kernel’s instructions and data, physical memory up to <code>PHYSTOP</code>, and memory ranges which are actually devices.</p>
<p><code>kvmmap</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/vm.c#L120"><span>(kernel/vm.c:120)</span></a> calls <code>mappages</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/vm.c#L151"><span>(kernel/vm.c:151)</span></a>, which installs mappings into a page table for a range of virtual addresses to a corresponding range of physical addresses. It does this separately for each virtual address in the range, at page intervals. For each virtual address to be mapped, <code>mappages</code> calls <code>walk</code> to find the address of the PTE for that address. It then initializes the PTE to hold the relevant physical page number, the desired permissions (<code>PTE_W</code>, <code>PTE_X</code>, and/or <code>PTE_R</code>), and <code>PTE_V</code> to mark the PTE as valid <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/vm.c#L163"><span>(kernel/vm.c:163)</span></a>.</p>
<p><code>walk</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/vm.c#L74"><span>(kernel/vm.c:74)</span></a> mimics the RISC-V paging hardware as it looks up the PTE for a virtual address (see Figure <a href="#fig:riscv_pagetable" data-reference-type="ref" data-reference="fig:riscv_pagetable">3.2</a>). <code>walk</code> descends the 3-level page table 9 bits at the time. It uses each level’s 9 bits of virtual address to find the PTE of either the next-level page table or the final page <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/vm.c#L80"><span>(kernel/vm.c:80)</span></a>. If the PTE isn’t valid, then the required page hasn’t yet been allocated; if the <code>alloc</code> argument is set, <code>walk</code> allocates a new page-table page and puts its physical address in the PTE. It returns the address of the PTE in the lowest layer in the tree <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/vm.c#L90"><span>(kernel/vm.c:90)</span></a>.</p>
<p>The above code depends on physical memory being direct-mapped into the kernel virtual address space. For example, as <code>walk</code> descends levels of the page table, it pulls the (physical) address of the next-level-down page table from a PTE <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/vm.c#L82"><span>(kernel/vm.c:82)</span></a>, and then uses that address as a virtual address to fetch the PTE at the next level down <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/vm.c#L80"><span>(kernel/vm.c:80)</span></a>.</p>
<p><code>main</code> calls <code>kvminithart</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/vm.c#L55"><span>(kernel/vm.c:55)</span></a> to install the kernel page table. It writes the physical address of the root page-table page into the register <code>satp</code>. After this the CPU will translate addresses using the kernel page table. Since the kernel uses an identity mapping, the now virtual address of the next instruction will map to the right physical memory address.</p>
<p><code>procinit</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L24"><span>(kernel/proc.c:24)</span></a>, which is called from <code>main</code>, allocates a kernel stack for each process. It maps each stack at the virtual address generated by <code>KSTACK</code>, which leaves room for the invalid stack-guard pages. <code>kvmmap</code> adds the mapping PTEs to the kernel page table, and the call to <code>kvminithart</code> reloads the kernel page table into <code>satp</code> so that the hardware knows about the new PTEs.</p>
<p>Each RISC-V core caches page table entries in a <em>Translation Look-aside Buffer (TLB)</em>, and when xv6 changes a page table, it must tell the CPU to invalidate corresponding cached TLB entries. If it didn’t, then at some point later the TLB might use an old cached mapping, pointing to a physical page that in the meantime has been allocated to another process, and as a result, a process might be able to scribble on some other process’s memory. The RISC-V has an instruction <code>sfence.vma</code> that flushes the current core’s TLB. xv6 executes <span><code>sfence.vma</code></span> in <span><code>kvminithart</code></span> after reloading the <code>satp</code> register, and in the trampoline code that switches to a user page table before returning to user space <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/trampoline.S#L79"><span>(kernel/trampoline.S:79)</span></a>.</p>
<h2 id="physical-memory-allocation">Physical memory allocation</h2>
<p>The kernel must allocate and free physical memory at run-time for page tables, user memory, kernel stacks, and pipe buffers.</p>
<p>xv6 uses the physical memory between the end of the kernel and <code>PHYSTOP</code> for run-time allocation. It allocates and frees whole 4096-byte pages at a time. It keeps track of which pages are free by threading a linked list through the pages themselves. Allocation consists of removing a page from the linked list; freeing consists of adding the freed page to the list.</p>
<h2 id="code-physical-memory-allocator">Code: Physical memory allocator</h2>
<p>The allocator resides in <span><code>kalloc.c</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/kalloc.c#L1"><span>(kernel/kalloc.c:1)</span></a>. The allocator’s data structure is a <em>free list</em> of physical memory pages that are available for allocation. Each free page’s list element is a <code>struct run</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/kalloc.c#L17"><span>(kernel/kalloc.c:17)</span></a>. Where does the allocator get the memory to hold that data structure? It store each free page’s <code>run</code> structure in the free page itself, since there’s nothing else stored there. The free list is protected by a spin lock <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/kalloc.c#L21-L24">(kernel/kalloc.c:21-24)</a>. The list and the lock are wrapped in a struct to make clear that the lock protects the fields in the struct. For now, ignore the lock and the calls to <code>acquire</code> and <code>release</code>; Chapter <a href="#CH:LOCK" data-reference-type="ref" data-reference="CH:LOCK">5</a> will examine locking in detail.</p>
<p>The function <code>main</code> calls <code>kinit</code> to initialize the allocator <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/kalloc.c#L27"><span>(kernel/kalloc.c:27)</span></a>. <code>kinit</code> initializes the free list to hold every page between the end of the kernel and <span><code>PHYSTOP</code></span>. <code>xv6</code> ought to determine how much physical memory is available by parsing configuration information. Instead xv6 assumes that the machine has 128 megabytes of RAM. <code>kinit</code> calls <code>freerange</code> to add memory to the free list via per-page calls to <code>kfree</code>. A PTE can only refer to a physical address that is aligned on a 4096-byte boundary (is a multiple of 4096), so <code>freerange</code> uses <code>PGROUNDUP</code> to ensure that it frees only aligned physical addresses. The allocator starts with no memory; these calls to <code>kfree</code> give it some to manage.</p>
<p>The allocator sometimes treats addresses as integers in order to perform arithmetic on them (e.g., traversing all pages in <code>freerange</code>), and sometimes uses addresses as pointers to read and write memory (e.g., manipulating the <code>run</code> structure stored in each page); this dual use of addresses is the main reason that the allocator code is full of C type casts. The other reason is that freeing and allocation inherently change the type of the memory.</p>
<p>The function <code>kfree</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/kalloc.c#L47"><span>(kernel/kalloc.c:47)</span></a> begins by setting every byte in the memory being freed to the value 1. This will cause code that uses memory after freeing it (uses “dangling references”) to read garbage instead of the old valid contents; hopefully that will cause such code to break faster. Then <code>kfree</code> prepends the page to the free list: it casts <code>pa</code> to a pointer to <code>struct</code> <code>run</code>, records the old start of the free list in <code>r-&gt;next</code>, and sets the free list equal to <code>r</code>. <code>kalloc</code> removes and returns the first element in the free list.</p>
<h2 id="process-address-space">Process address space</h2>
<p>Each process has a separate page table, and when xv6 switches between processes, it also changes page tables. As shown in Figure <a href="#fig:as" data-reference-type="ref" data-reference="fig:as">2.2</a>, a process’s user memory starts at virtual address zero and can grow up to <code>MAXVA</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/riscv.h#L349"><span>(kernel/riscv.h:349)</span></a>, allowing a process to address in principle 256 Gigabytes of memory.</p>
<p>When a process asks xv6 for more user memory, xv6 first uses <span><code>kalloc</code></span> to allocate physical pages. It then adds PTEs to the process’s page table that point to the new physical pages. Xv6 sets the <code>PTE_W</code>, <code>PTE_X</code>, <code>PTE_R</code>, <code>PTE_U</code>, and <code>PTE_V</code> flags in these PTEs. Most processes do not use the entire user address space; xv6 leaves <code>PTE_V</code> clear in unused PTEs.</p>
<p>We see here a few nice examples of use of page tables. First, different processes’ page tables translate user addresses to different pages of physical memory, so that each process has private user memory. Second, each process sees its memory as having contiguous virtual addresses starting at zero, while the process’s physical memory can be non-contiguous. Third, the kernel maps a page with trampoline code at the top of the user address space, thus a single page of physical memory shows up in all address spaces.</p>
<figure>
<embed src="fig/processlayout.svg" id="fig:processlayout" /><figcaption>A process’s user address space, with its initial stack.</figcaption>
</figure>
<p>Figure <a href="#fig:processlayout" data-reference-type="ref" data-reference="fig:processlayout">3.4</a> shows the layout of the user memory of an executing process in xv6 in more detail. The stack is a single page, and is shown with the initial contents as created by exec. Strings containing the command-line arguments, as well as an array of pointers to them, are at the very top of the stack. Just under that are values that allow a program to start at <code>main</code> as if the function <code>main(argc</code>, <code>argv)</code> had just been called.</p>
<p>To detect a user stack overflowing the allocated stack memory, xv6 places an invalid guard page right below the stack. If the user stack overflows and the process tries to use an address below the stack, the hardware will generate a page-fault exception because the mapping is not valid. A real-world operating system might instead automatically allocate more memory for the user stack when it overflows.</p>
<h2 id="code-sbrk">Code: sbrk</h2>
<p><code>Sbrk</code> is the system call for a process to shrink or grow its memory. The system call is implemented by the function <code>growproc</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L224"><span>(kernel/proc.c:224)</span></a>. <code>growproc</code> calls <code>uvmalloc</code> or <code>uvmdealloc</code>, depending on whether <code>n</code> is postive or negative. <code>uvmalloc</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/vm.c#L236"><span>(kernel/vm.c:236)</span></a> allocates physical memory with <span><code>kalloc</code></span>, and adds PTEs to the user page table with <span><code>mappages</code></span>. <code>uvmdealloc</code> calls <span><code>uvmunmap</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/vm.c#L176"><span>(kernel/vm.c:176)</span></a>, which uses <span><code>walk</code></span> to find PTEs and <span><code>kfree</code></span> to free the physical memory they refer to.</p>
<p>xv6 uses a process’s page table not just to tell the hardware how to map user virtual addresses, but also as the only record of which physical memory pages are allocated to that process. That is the reason why freeing user memory (in <span><code>uvmunmap</code></span>) requires examination of the user page table.</p>
<h2 id="code-exec">Code: exec</h2>
<p><code>Exec</code> is the system call that creates the user part of an address space. It initializes the user part of an address space from a file stored in the file system. <code>Exec</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/exec.c#L13"><span>(kernel/exec.c:13)</span></a> opens the named binary <code>path</code> using <code>namei</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/exec.c#L26"><span>(kernel/exec.c:26)</span></a>, which is explained in Chapter <a href="#CH:FS" data-reference-type="ref" data-reference="CH:FS">7</a>. Then, it reads the ELF header. Xv6 applications are described in the widely-used <em>ELF format</em>, defined in <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/elf.h"><span>(kernel/elf.h)</span></a>. An ELF binary consists of an ELF header, <code>struct elfhdr</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/elf.h#L6"><span>(kernel/elf.h:6)</span></a>, followed by a sequence of program section headers, <code>struct proghdr</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/elf.h#L25"><span>(kernel/elf.h:25)</span></a>. Each <code>proghdr</code> describes a section of the application that must be loaded into memory; xv6 programs have only one program section header, but other systems might have separate sections for instructions and data.</p>
<p>The first step is a quick check that the file probably contains an ELF binary. An ELF binary starts with the four-byte “magic number” <code>0x7F</code>, .code ’E’, .code ’L’, .code ’F’, or <code>ELF_MAGIC</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/elf.h#L3"><span>(kernel/elf.h:3)</span></a>. If the ELF header has the right magic number, <code>exec</code> assumes that the binary is well-formed.</p>
<p><code>Exec</code> allocates a new page table with no user mappings with <code>proc_pagetable</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/exec.c#L38"><span>(kernel/exec.c:38)</span></a>, allocates memory for each ELF segment with <code>uvmalloc</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/exec.c#L52"><span>(kernel/exec.c:52)</span></a>, and loads each segment into memory with <code>loadseg</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/exec.c#L10"><span>(kernel/exec.c:10)</span></a>. <code>loadseg</code> uses <code>walkaddr</code> to find the physical address of the allocated memory at which to write each page of the ELF segment, and <code>readi</code> to read from the file.</p>
<p>The program section header for <code>/init</code>, the first user program created with <code>exec</code>, looks like this:</p>
<pre><code># objdump -p _init
user/_init:     file format elf64-littleriscv

Program Header:
    LOAD off    0x00000000000000b0 vaddr 0x0000000000000000 
                                      paddr 0x0000000000000000 align 2**3
         filesz 0x0000000000000840 memsz 0x0000000000000858 flags rwx
   STACK off    0x0000000000000000 vaddr 0x0000000000000000 
                                      paddr 0x0000000000000000 align 2**4
         filesz 0x0000000000000000 memsz 0x0000000000000000 flags rw-</code></pre>
<p>The program section header’s <code>filesz</code> may be less than the <code>memsz</code>, indicating that the gap between them should be filled with zeroes (for C global variables) rather than read from the file. For <code>/init</code>, <code>filesz</code> is 2112 bytes and <code>memsz</code> is 2136 bytes, and thus <code>uvmalloc</code> allocates enough physical memory to hold 2136 bytes, but reads only 2112 bytes from the file <code>/init</code>.</p>
<p>Now <code>exec</code> allocates and initializes the user stack. It allocates just one stack page. <code>Exec</code> copies the argument strings to the top of the stack one at a time, recording the pointers to them in <code>ustack</code>. It places a null pointer at the end of what will be the <code>argv</code> list passed to <code>main</code>. The first three entries in <code>ustack</code> are the fake return PC, <code>argc</code>, and <code>argv</code> pointer.</p>
<p><code>Exec</code> places an inaccessible page just below the stack page, so that programs that try to use more than one page will fault. This inaccessible page also allows <code>exec</code> to deal with arguments that are too large; in that situation, the <code>copyout</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/vm.c#L361"><span>(kernel/vm.c:361)</span></a> function that <code>exec</code> uses to copy arguments to the stack will notice that the destination page is not accessible, and will return .</p>
<p>During the preparation of the new memory image, if <code>exec</code> detects an error like an invalid program segment, it jumps to the label <code>bad</code>, frees the new image, and returns . <code>Exec</code> must wait to free the old image until it is sure that the system call will succeed: if the old image is gone, the system call cannot return to it. The only error cases in <code>exec</code> happen during the creation of the image. Once the image is complete, <code>exec</code> can commit to the new page table <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/exec.c#L110"><span>(kernel/exec.c:110)</span></a> and free the old one <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/exec.c#L114"><span>(kernel/exec.c:114)</span></a>.</p>
<p><code>Exec</code> loads bytes from the ELF file into memory at addresses specified by the ELF file. Users or processes can place whatever addresses they want into an ELF file. Thus <code>exec</code> is risky, because the addresses in the ELF file may refer to the kernel, accidentally or on purpose. The consequences for an unwary kernel could range from a crash to a malicious subversion of the kernel’s isolation mechanisms (i.e., a security exploit). xv6 performs a number of checks to avoid these risks. For example <code>if(ph.vaddr + ph.memsz &lt; ph.vaddr)</code> checks for whether the sum overflows a 64-bit integer. The danger is that a user could construct an ELF binary with a <code>ph.vaddr</code> that points to a user-chosen address, and <code>ph.memsz</code> large enough that the sum overflows to 0x1000, which will look like a valid value. In an older version of xv6 in which the user address space also contained the kernel (but not readable/writable in user mode), the user could choose an address that corresponded to kernel memory and would thus copy data from the ELF binary into the kernel. In the RISC-V version of xv6 this cannot happen, because the kernel has its own separate page table; <code>loadseg</code> loads into the process’s page table, not in the kernel’s page table.</p>
<p>It is easy for a kernel developer to omit a crucial check, and real-world kernels have a long history of missing checks whose absence can be exploited by user programs to obtain kernel privileges. It is likely that xv6 doesn’t do a complete job of validating user-level data supplied to the kernel, which a malicious user program might be able to exploit to circumvent xv6’s isolation.</p>
<h2 id="real-world-2">Real world</h2>
<p>Like most operating systems, xv6 uses the paging hardware for memory protection and mapping. Most operating systems make far more sophisticated use of paging than xv6; for example, xv6 lacks demand paging from disk, copy-on-write fork, shared memory, lazily-allocated pages, automatically extending stacks, and memory-mapped files.</p>
<p>RISC-V supports protection at the level of physical addresses, but xv6 doesn’t use that feature.</p>
<p>On machines with lots of memory it might make sense to use RISC-V’s support for “super pages.” Small pages make sense when physical memory is small, to allow allocation and page-out to disk with fine granularity. For example, if a program uses only 8 kilobytes of memory, giving it a whole 4-megabyte super-page of physical memory is wasteful. Larger pages make sense on machines with lots of RAM, and may reduce overhead for page-table manipulation.</p>
<p>The xv6 kernel’s lack of a <span><code>malloc</code></span>-like allocator that can provide memory for small objects prevents the kernel from using sophisticated data structures that would require dynamic allocation.</p>
<p>Memory allocation was a hot topic a long time ago, the basic problems being efficient use of limited memory and preparing for unknown future requests <span class="citation" data-cites="knuth"></span>. Today people care more about speed than space efficiency. In addition, a more elaborate kernel would likely allocate many different sizes of small blocks, rather than (as in xv6) just 4096-byte blocks; a real kernel allocator would need to handle small allocations as well as large ones.</p>
<h2 id="exercises-2">Exercises</h2>
<ol>
<li><p>Parse RISC-V’s device tree to find the amount of physical memory the computer has.</p></li>
<li><p>Write a user program that grows its address space by one byte by calling <code>sbrk(1)</code>. Run the program and investigate the page table for the program before the call to <code>sbrk</code> and after the call to <code>sbrk</code>. How much space has the kernel allocated? What does the <code>pte</code> for the new memory contain?</p></li>
<li><p>Modify xv6 to use super pages for the kernel.</p></li>
<li><p>Modify xv6 so that when a user program dereferences a null pointer, it will receive a fault. That is, modify xv6 so that virtual address 0 isn’t mapped for user programs.</p></li>
<li><p>Unix implementations of <code>exec</code> traditionally include special handling for shell scripts. If the file to execute begins with the text <code>#!</code>, then the first line is taken to be a program to run to interpret the file. For example, if <code>exec</code> is called to run <code>myprog</code> <code>arg1</code> and <code>myprog</code> ’s first line is <code>#!/interp</code>, then <code>exec</code> runs <code>/interp</code> with command line <code>/interp</code> <code>myprog</code> <code>arg1</code>. Implement support for this convention in xv6.</p></li>
<li><p>Implement address space randomization for the kernel.</p></li>
</ol>
<h1 id="CH:TRAP">Traps and device drivers</h1>
<p>There are three situations in which some event causes the CPU to set aside its ordinary sequential execution of instructions and forces a transfer of control to special code that handles the event. One situation is a system call, when a user program executes the <span><code>ecall</code></span> instruction to ask the kernel to do something for it. Another situation is an <em>exception</em>: an instruction (user or kernel) does something illegal, such as divide by zero or use an invalid virtual address. The third situation is a device <em>interrupt</em>, when a device signals that it needs attention, for example when the disk hardware finishes a read or write request.</p>
<p>This book uses <em>trap</em> as a generic term for these situations. Typically whatever code was executing at the time of the trap will later need to resume, and shouldn’t need to be aware that anything special happened. That is, we often want traps to be transparent; this is particularly important for interrupts, which the interrupted code typically doesn’t expect. Thus the usual sequence is that a trap forces a transfer of control into the kernel; the kernel saves registers and other state so that execution can be resumed; the kernel executes appropriate handler code (e.g., a system call implementation or device driver); the kernel restores the saved state and returns from the trap; and the original code resumes where it left off.</p>
<p>The xv6 kernel handles all traps. This is natural for system calls; it makes sense for interrupts since isolation demands that user processes not directly use devices, and because only the kernel has the state needed for device handling; and it makes sense for exceptions since xv6 responds to all exceptions from user space by killing the offending program.</p>
<p>Xv6 trap handling proceeds in four stages: hardware actions taken by the RISC-V CPU, an assembly “vector” that prepares the way for kernel C code, a C trap handler that decides what to do with the trap, and the system call or device-driver service routine. While commonality among the three trap types suggests that a kernel could handle all traps with a single code path, it turns out to be convenient to have separate assembly vectors and C trap handlers for three distinct cases: traps from kernel space, traps from user space, and timer interrupts.</p>
<p>This chapter ends with a discussion of device drivers. Device handling is a different topic than traps, but is included here because the kernel’s interaction with device hardware is often driven by interrupts.</p>
<h2 id="risc-v-trap-machinery">RISC-V trap machinery</h2>
<p>RISC-V supports a number of control registers that the kernel writes to tell the CPU how to handle interrupts, and that the kernel can read to find out about an interrupt that has occured. The RISC-V documents contain the full story <span class="citation" data-cites="riscv:priv"></span>. <span><code>riscv.h</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/riscv.h#L1"><span>(kernel/riscv.h:1)</span></a> contains definitions that xv6 uses. Here’s an outline of the most important registers:</p>
<ul>
<li><p><span><strong>stvec</strong></span>: The kernel writes the address of its trap handler here; the RISC-V jumps here to handle a trap.</p></li>
<li><p><span><strong>sepc</strong></span>: When a trap occurs, RISC-V saves the program counter here (since the <span><code>pc</code></span> is then replaced with <span><code>stvec</code></span>). The <span><code>sret</code></span> (return from trap) instruction copies <span><code>sepc</code></span> to the <span><code>pc</code></span>. The kernel can write to <span><code>sepc</code></span> to control where <span><code> sret</code></span> goes.</p></li>
<li><p><span><strong>scause</strong></span>: The RISC-V puts a number here that describes the reason for the trap.</p></li>
<li><p><span><strong>sscratch</strong></span>: The kernel places a value here that comes in handy at the very start of a trap handler.</p></li>
<li><p><span><strong>sstatus</strong></span>: The SIE bit controls whether device interrupts are enabled. If the kernel clears SIE, the RISC-V will defer device interrupts until the kernel sets SIE. The SPP bit indicates whether a trap came from user mode or supervisor mode, and controls to what mode <span><code>sret</code></span> returns.</p></li>
</ul>
<p>The above relate to interrupts handled in supervisor mode, and they cannot be read or written in user mode. There is an equivalent set of control registers for interrupts handled in machine mode; xv6 uses them only for the special case of timer interrupts.</p>
<p>When it needs to force a trap, the RISC-V hardware does the following for all trap types (other than timer interrupts):</p>
<ol>
<li><p>If the trap is a device interrupt, and the <span><code>sstatus</code></span> SIE bit is clear, don’t do any of the following.</p></li>
<li><p>Disable interrupts by clearing SIE.</p></li>
<li><p>Copy the <span><code>pc</code></span> to <span><code>sepc</code></span>.</p></li>
<li><p>Save the current mode (user or supervisor) in the SPP bit in <span><code>sstatus</code></span>.</p></li>
<li><p>Set <span><code>scause</code></span> to reflect the interrupt’s cause.</p></li>
<li><p>Set the mode to supervisor.</p></li>
<li><p>Copy <span><code>stvec</code></span> to the <span><code>pc</code></span>.</p></li>
<li><p>Start executing at the new <span><code>pc</code></span>.</p></li>
</ol>
<p>It is important that the CPU performs all these steps as a single operation. Consider if one of these steps were omitted: for example, the CPU didn’t switch program counters. Then, a trap could switch to supervisor mode while still running user instructions. Those user instructions could break the user/kernel isolation, for example by modifying the <span><code>satp</code></span> register to point to a page table that allowed accessing all of physical memory. It is thus important that the kernel specify the trap entry point, and not the user program.</p>
<p>Note that the CPU doesn’t switch to the kernel page table, doesn’t switch to a stack in the kernel, and doesn’t save any registers other than the <span><code>pc</code></span>. The kernel must perform these tasks if necessary. One reason that the CPU does minimal work during a trap is to provide flexibility to software; for example, in some situations no page table switch is needed, which may increase performance.</p>
<h2 id="traps-from-kernel-space">Traps from kernel space</h2>
<p>When the xv6 kernel is executing on a CPU, two types of traps can occur: exceptions and device interrupts. The previous section outlined the CPU’s response to such traps.</p>
<p>When the kernel is executing, it points <span><code>stvec</code></span> to the assembly code at <span><code>kernelvec</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/kernelvec.S#L10"><span>(kernel/kernelvec.S:10)</span></a>. Since xv6 is already in the kernel, <span><code>kernelvec</code></span> can rely on <span><code>satp</code></span> being set to the kernel page table, and on the stack pointer referring to a valid kernel stack. <span><code>kernelvec</code></span> saves all registers so that we can eventually resume the interrupted code without disturbing it.</p>
<p><span><code>kernelvec</code></span> saves the registers on the stack of the interrupted kernel thread, which makes sense because the register values belong to that thread. This is particularly important if the trap causes a switch to a different thread – in that case the trap will actually return on the stack of the new thread, leaving the interrupted thread’s saved registers safely on its stack.</p>
<p><span><code>kernelvec</code></span> jumps to <span><code>kerneltrap</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/trap.c#L134"><span>(kernel/trap.c:134)</span></a> after saving registers. <span><code>kerneltrap</code></span> is prepared for two types of traps: device interrrupts and exceptions. It calls <span><code>devintr</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/trap.c#L177"><span>(kernel/trap.c:177)</span></a> to check for and handle the former. If the trap isn’t a device interrupt, it is an exception, and that is always a fatal error if it occurs in the kernel.</p>
<p>If <span><code>kerneltrap</code></span> was called due to a timer interrupt, and a process’s kernel thread is running (rather than a scheduler thread), <span><code>kerneltrap</code></span> calls <span><code>yield</code></span> to give other threads a chance to run. At some point one of those threads will yield, and let our thread and its <span><code>kerneltrap</code></span> resume again. Chapter <a href="#CH:SCHED" data-reference-type="ref" data-reference="CH:SCHED">6</a> explains what happens in <span><code>yield</code></span>.</p>
<p>When <span><code>kerneltrap</code></span>’s work is done, it needs to return to whatever code was interrupted by the trap. Because a <span><code>yield</code></span> may have disturbed the saved <span><code>sepc</code></span> and the saved previous mode in <span><code>sstatus</code></span>, <span><code>kerneltrap</code></span> saves them when it starts. It now restores those control registers and returns to <span><code>kernelvec</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/kernelvec.S#L48"><span>(kernel/kernelvec.S:48)</span></a>. <span><code>kernelvec</code></span> pops the saved registers from the stack and executes <span><code>sret</code></span>, which copies <span><code>sepc</code></span> to <span><code>pc</code></span> and resumes the interrupted kernel code.</p>
<p>It’s worth thinking through how the trap return happens if <span><code>kerneltrap</code></span> called <span><code>yield</code></span> due to a timer interrupt.</p>
<p>Xv6 sets a CPU’s <span><code>stvec</code></span> to <span><code>kernelvec</code></span> when that CPU enters the kernel from user space; you can see this in <span><code>usertrap</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/trap.c#L29"><span>(kernel/trap.c:29)</span></a>. There’s a window of time when the kernel is executing but <span><code>stvec</code></span> has the wrong value, and it’s crucial that device interrupts be disabled during that window. Luckily the RISC-V always disables interrupts when it starts to take a trap, and xv6 doesn’t enable them again until after it sets <span><code>stvec</code></span>.</p>
<h2 id="traps-from-user-space">Traps from user space</h2>
<p>A trap may occur while executing in user space if the user program makes a system call (<span><code>ecall</code></span> instruction), does something illegal, or if a device interrupts. The high-level path of a trap from user space is <span><code>uservec</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/trampoline.S#L16"><span>(kernel/trampoline.S:16)</span></a>, then <span><code>usertrap</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/trap.c#L37"><span>(kernel/trap.c:37)</span></a>; and when returning, <span><code>usertrapret</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/trap.c#L90"><span>(kernel/trap.c:90)</span></a> and then <span><code>userret</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/trampoline.S#L16"><span>(kernel/trampoline.S:16)</span></a>.</p>
<p>Traps from user code are more challenging than from the kernel, since <span><code>satp</code></span> points to a user page table that doesn’t map the kernel, and the stack pointer may contain an invalid or even malicious value.</p>
<p>Because the RISC-V hardware doesn’t switch page tables during a trap, we need the user page table to include a mapping for the trap vector instructions that <span><code>stvec</code></span> points to. Further, the trap vector must switch <span><code> satp</code></span> to point to the kernel page table, and in order to avoid a crash, the vector instructions must be mapped at the same address in the kernel page table as in the user page table.</p>
<p>Xv6 satisfies these constraints with a <em>trampoline</em> page that contains the trap vector code. Xv6 maps the trampoline page at the same virtual address in the kernel page table and in every user page table. This virtual address is <code>TRAMPOLINE</code> (as we saw in Figure <a href="#fig:as" data-reference-type="ref" data-reference="fig:as">2.2</a> and in Figure <a href="#fig:xv6_layout" data-reference-type="ref" data-reference="fig:xv6_layout">3.3</a>). The trampoline contents are set in <span><code>trampoline.S</code></span>, and (when executing user code) <span><code>stvec</code></span> is set to <span><code>uservec</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/trampoline.S#L16"><span>(kernel/trampoline.S:16)</span></a>.</p>
<p>When <span><code>uservec</code></span> starts, every register contains a value owned by the interrupting code. But <span><code>uservec</code></span> needs to be able to modify some registers in order to set <span><code>satp</code></span> and generate addresses at which to save the registers. RISC-V provides a helping hand in the form of the <span><code>sscratch</code></span> register. The <span><code>csrrw</code></span> instruction at the start of <span><code>uservec</code></span> swaps the contents of <span><code>a0</code></span> and <span><code> sscratch</code></span>. Now the user code’s <span><code>a0</code></span> is saved; <span><code>uservec</code></span> has one register (<span><code>a0</code></span>) to play with; and <span><code>a0</code></span> contains whatever value the kernel previously placed in <span><code>sscratch</code></span>.</p>
<p><span><code>uservec</code></span>’s next task is to save the user registers. Before entering user space, the kernel sets <span><code>sscratch</code></span> to point to a per-process <span><code>trapframe</code></span> that (among other things) has space to save all the user registers <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.h#L44"><span>(kernel/proc.h:44)</span></a>. Because <span><code>satp</code></span> still refers to the user page table, <span><code>uservec</code></span> needs the trapframe to be mapped in the user address space. When creating each process, xv6 allocates a page for the process’s trapframe, and arranges for it always to be mapped at user virtual address <span><code>TRAPFRAME</code></span>, which is just below <span><code>TRAMPOLINE</code></span>. The process’s <span><code>p-&gt;tf</code></span> points to the trapframe, though at its physical address where it can be accessed through the kernel page table.</p>
<p>Thus after swapping <span><code>a0</code></span> and <span><code>sscratch</code></span>, <span><code>a0</code></span> holds a pointer to the current process’s trapframe. <span><code>uservec</code></span> now saves all user registers there, including the user’s <span><code>a0</code></span>, read from <span><code>sscratch</code></span>.</p>
<p>The <span><code>trapframe</code></span> contains pointers to the current process’s kernel stack, the current CPU’s hartid, the address of <span><code>usertrap</code></span>, and the address of the kernel page table. <span><code>uservec</code></span> retrieves these values, switches <span><code>satp</code></span> to the kernel page table, and calls <span><code>usertrap</code></span>.</p>
<p>The job of <span><code>usertrap</code></span>, like <span><code>kerneltrap</code></span> is to determine the cause of the trap, process it, and return <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/trap.c#L37"><span>(kernel/trap.c:37)</span></a>. As mentioned above, it first changes <span><code>stvec</code></span> to process traps from kernel mode in <span><code>kernelvec</code></span>. It saves the <span><code>sepc</code></span>, again because there might be a process switch in <span><code>usertrap</code></span> that could cause <span><code>sepc</code></span> to be overwritten. If the trap is a system call, <span><code>syscall</code></span> handles it; if a device interrupt, <span><code>devintr</code></span>; otherwise it’s an exception, and the kernel kills the faulting process. The system call path adds four to the saved user <span><code>pc</code></span> because RISC-V, in the case of a system call, leaves the program pointer pointing to the <span><code>ecall</code></span> instruction. On the way out, <span><code>usertrap</code></span> checks if the process has been killed or should yield the CPU (if this trap is a timer interrupt).</p>
<p>The first step in returning to user space is the call to <span><code>usertrapret</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/trap.c#L90"><span>(kernel/trap.c:90)</span></a>. This function sets up the RISC-V control registers to prepare for a future trap from user space. This involves changing <span><code>stvec</code></span> to refer to <span><code>uservec</code></span>, preparing the trapframe fields that <span><code>uservec</code></span> relies on, and setting <span><code>sepc</code></span> to the previously saved user program counter. At the end, <span><code>usertrapret</code></span> calls <span><code>userret</code></span> on the trampoline page that is mapped in both user and kernel page tables; the reason is that assembly code in <span><code>userret</code></span> will switch page tables.</p>
<p><span><code>usertrapret</code></span>’s call to <span><code>userret</code></span> passes a pointer to the process’s user page table in <span><code>a0</code></span> and <span><code>TRAPFRAME</code></span> in <span><code>a1</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/trampoline.S#L88"><span>(kernel/trampoline.S:88)</span></a>. <span><code>userret</code></span> switches <span><code>satp</code></span> to the process’s user page table. Recall that the user page table maps both the trampoline page and <span><code>TRAPFRAME</code></span>, but nothing else from the kernel. Again, the fact that the trampoline page is mapped at the same virtual address in user and kernel page tables is what allows <span><code>uservec</code></span> to keep executing after changing <span><code>satp</code></span>. <span><code>trapret</code></span> copies the trapframe’s saved user <span><code>a0</code></span> to <span><code>sscratch</code></span> in preparation for a later swap with with TRAPFRAME. From this point on, the only data <span><code>userret</code></span> can use is the register contents and the content of the trapframe. Next <span><code>userret</code></span> restores saved user registers from the trapframe, does a final swap of <span><code>a0</code></span> and <span><code>sscratch</code></span> to restore the user <span><code>a0</code></span> and save <span><code>TRAPFRAME</code></span> for the next trap, and uses <span><code>sret</code></span> to return to user space.</p>
<h2 id="timer-interrupts">Timer interrupts</h2>
<p>Xv6 uses timer interrupts to maintain its clock and to enable it to switch among compute-bound processes; the <span><code>yield</code></span> calls in <span><code> usertrap</code></span> and <span><code>kerneltrap</code></span> cause this switching. Timer interrupts come from clock hardware attached to each RISC-V CPU. Xv6 programs this clock hardware to interrupt each CPU periodically.</p>
<p>RISC-V requires that timer interrupts be taken in machine mode, not supervisor mode. RISC-V machine mode executes without paging, and with a separate set of control registers, so it’s not practical to run ordinary xv6 kernel code in machine mode. As a result, xv6 handles timer interrupts completely separately from the trap mechanism laid out above.</p>
<p>Code executed in machine mode in <span><code>start.c</code></span>, before <span><code>main</code></span>, sets up to receive timer interrupts <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/start.c#L56"><span>(kernel/start.c:56)</span></a>. Part of the job is to program the CLINT hardware (core-local interruptor) to generate an interrupt after a certain delay. Another part is to set up a scratch area, analogous to the trapframe, for the timer interrupt handler to save registers in and find the address of the CLINT registers. Finally, <span><code>start</code></span> sets <span><code>mtvec</code></span> to <span><code>timervec</code></span> and enables timer interrupts.</p>
<p>A timer interrupt can occur at any point when user or kernel code is executing; there’s no way for the kernel to disable timer interrupts during critical operations. Thus the timer interrupt handler must do its job in a way guaranteed not to disturb interrupted kernel code. The basic strategy is for the timer interrupt to ask the RISC-V to raise a “software interrupt” and immediately return. The RISC-V delivers software interrupts to the kernel with the ordinary trap mechanism, and allows the kernel to disable them. The code to handle the software interrupt generated by a timer interrupt can be seen in <span><code>devintr</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/trap.c#L197"><span>(kernel/trap.c:197)</span></a>.</p>
<p>The machine-mode timer interrupt vector is <span><code>timervec</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/kernelvec.S#L93"><span>(kernel/kernelvec.S:93)</span></a>. It saves a few registers in the scratch area prepared by <span><code>start</code></span>, tells the CLINT when to generate the next timer interrupt, asks the RISC-V to raise a software interrupt, restores registers, and returns. There’s no C code involved in a timer interrupt.</p>
<h2 id="code-calling-system-calls">Code: Calling system calls</h2>
<p>Chapter <a href="#CH:FIRST" data-reference-type="ref" data-reference="CH:FIRST">2</a> ended with <code>initcode.S</code> invoking the <span><code>exec</code></span> system call <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//user/initcode.S#L11"><span>(user/initcode.S:11)</span></a>. Let’s look at how the user call makes its way to the <span><code>exec</code></span> system call’s implementation in the kernel.</p>
<p>The user code places the arguments for <code>exec</code> in registers <span><code>a0</code></span> and <span><code>a1</code></span>, and puts the system call number in <code>a7</code>. System call numbers match the entries in the syscalls array, a table of function pointers <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/syscall.c#L108"><span>(kernel/syscall.c:108)</span></a>. The <code>ecall</code> instruction traps into the kernel and executes <span><code>uservec</code></span>, <span><code>usertrap</code></span>, and then <span><code>syscall</code></span>, as we saw above.</p>
<p><code>Syscall</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/syscall.c#L133"><span>(kernel/syscall.c:133)</span></a> loads the system call number from the trapframe, which contains the saved <code>a7</code>, and indexes into the system call table. For the first system call, <code>a7</code> contains the value <code>SYS_exec</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/syscall.h#L8"><span>(kernel/syscall.h:8)</span></a>, and <code>syscall</code> will call the <code>SYS_exec</code>’th entry of the system call table, which corresponds to calling <code>sys_exec</code>.</p>
<p><code>Syscall</code> records the return value of the system call function in <code>p-&gt;tf-&gt;a0</code>. When the system call returns to user space, <code>userret</code> will load the values from <code>p-&gt;tf</code> into the machine registers and return to user space using <code>sret</code>. Thus, when <code>exec</code> returns in user space, it will return in <code>a0</code> the value that the system call handler returned <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/syscall.c#L140"><span>(kernel/syscall.c:140)</span></a>. System calls conventionally return negative numbers to indicate errors, and zero or positive numbers for success. If the system call number is invalid, <code>syscall</code> prints an error and returns <span class="math inline"> − 1</span>.</p>
<h2 id="code-system-call-arguments">Code: System call arguments</h2>
<p>Later chapters will examine the implementation of particular system calls. This chapter is concerned with the system call mechanism. There is one bit of mechanism left: finding the system call arguments.</p>
<p>The C calling convention on RISC-V specifies that arguments are passed in registers. During a system call, these registers (the saved user registers) are available in the trapframe, <span><code>p-&gt;tf</code></span>. The functions <code>argint</code>, <code>argaddr</code>, and <code>argfd</code> retrieve the <em>n</em> ’th system call argument, as either an integer, pointer, or a file descriptor. They all call <span><code>argraw</code></span> to retrieve one of the saved user registers <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/syscall.c#L35"><span>(kernel/syscall.c:35)</span></a>.</p>
<p>Some system calls pass pointers as arguments, and the kernel must use those pointers to read or write user memory. The <span><code>exec</code></span> system call, for example, passes the kernel an array of pointers referring to string arguments in user space. These pointers pose two challenges. First, the user program may be buggy or malicious, and may pass the kernel an invalid pointer or a pointer intended to trick the kernel into accessing kernel memory instead of user memory. Second, the xv6 kernel page table mappings are not the same as the user page table mappings, so the kernel cannot use ordinary instructions to load or store from user-supplied addresses.</p>
<p>A number of kernel functions need to perform safe reads from user space; <span><code>fetchstr</code></span> is an example <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/syscall.c#L25"><span>(kernel/syscall.c:25)</span></a>. File system calls such as <span><code>exec</code></span> use <span><code>fetchstr</code></span> to retrieve string arguments from user space. <code>fetchstr</code> calls <code>copyinstr</code>, which looks up a virtual address in a user page table, turn it into an address the kernel can use, and copies a string from the address into the kernel.</p>
<p><code>copyinstr</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/vm.c#L412"><span>(kernel/vm.c:412)</span></a> copies up to <code>max</code> bytes to <code>dst</code> from virtual address <code>srcva</code> in the user page table <code>pagetable</code>. It uses <span><code>walkaddr</code></span> (which calls <span><code>walk</code></span>) to walk the page table in software to determine the physical address <code>pa0</code> for <code>srcva</code>. Since the kernel maps all physical RAM addresses to the same kernel virtual address, <span><code>copyinstr</code></span> can directly copy string bytes from <span><code>pa0</code></span> to <span><code>dst</code></span>. <span><code>walkaddr</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/vm.c#L97"><span>(kernel/vm.c:97)</span></a> checks that the user-supplied virtual address is part of the process’s user address space, so programs cannot trick the kernel into reading other memory. A similar function, <span><code>copyout</code></span>, copies data from the kernel to a user-supplied address.</p>
<h2 id="device-drivers">Device drivers</h2>
<p>A <em>driver</em> is the code in an operating system that manages a particular device: it tells the device hardware to perform operations, configures the device to generate interrupts when done, handles the resulting interrupts, and interacts with processes that may be waiting for I/O from the device. Driver code can be tricky because a driver executes concurrently with the device that it manages. In addition, the driver must understand the device’s hardware interface, which can be complex and poorly documented.</p>
<p>Devices that need attention from the operating system can usually be configured to generate interrupts, which are one type of trap. The kernel trap handling code must be able to recognize when the device has raised an interrupt and call the driver’s interrupt handler; in xv6, this dispatch happens in <span><code>devintr</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/trap.c#L177"><span>(kernel/trap.c:177)</span></a>.</p>
<p>Many device drivers have two main parts: code that runs as part of a process, and code that runs at interrupt time. The process-level code is driven by system calls such as <span><code>read</code></span> and <span><code>write</code></span> that want the device to perform I/O. This code may ask the hardware to start an operation (e.g., ask the disk to read a block); then the code waits for the operation to complete. Eventually the device completes the operation and raises an interrupt. The driver’s interrupt handler figures out what operation (if any) has completed, wakes up a waiting process if appropriate, and perhaps tells the hardware to start work on any waiting next operation.</p>
<h2 id="code-the-console-driver">Code: The console driver</h2>
<p>The console driver is a simple illustration of driver structure. The console driver accepts characters typed by a human, via the <em>UART</em> serial-port hardware attached to the RISC-V. The driver accumulates a line of input at a time, processing special input characters such as backspace and control-u. User processes, such as the shell, can use the <span><code>read</code></span> system call to fetch lines of input from the console. When you type input to xv6 in QEMU, your keystrokes are delivered to xv6 by way of QEMU’s simulated UART hardware.</p>
<p>The UART hardware that the driver talks to is a 16550 chip <span class="citation" data-cites="ns16550a"></span> emulated by QEMU. On a real computer, a 16550 would manage an RS232 serial link connecting to a terminal or other computer. When running QEMU, it’s connected to your keyboard and display.</p>
<p>The UART hardware appears to software as a set of memory-mapped control registers. That is, there are some physical addresses that RISC-V hardware connects to the UART device, so that loads and stores interact with the device hardware rather than RAM. The memory-mapped address for the UART is 0x10000000, or <span><code>UART0</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/memlayout.h#L21"><span>(kernel/memlayout.h:21)</span></a>. There are a handful of UART control registers, each the width of a byte. Their offsets from <span><code>UART0</code></span> are defined in <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/uart.c#L22"><span>(kernel/uart.c:22)</span></a>. For example, the <span><code>LSR</code></span> register contain bits that indicate whether input characters are waiting to be read by the software. These characters (if any) are available for reading from the <span><code>RHR</code></span> register. Each time one is read, the UART hardware deletes it from an internal FIFO of waiting characters, and clears the “ready” bit in <span><code>LSR</code></span> when the FIFO is empty.</p>
<p>Xv6’s <span><code>main</code></span> calls <span><code>consoleinit</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/console.c#L189"><span>(kernel/console.c:189)</span></a> to initialize the UART hardware, and to configure the UART hardware to generate input interrupts <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/uart.c#L34"><span>(kernel/uart.c:34)</span></a>.</p>
<p>The xv6 shell reads from the console by way of a file descriptor opened by <span><code>init.c</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//user/init.c#L15"><span>(user/init.c:15)</span></a>. Calls to the <span><code>read</code></span> system call make their way through the kernel to <span><code> consoleread</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/console.c#L87"><span>(kernel/console.c:87)</span></a>. <span><code> consoleread</code></span> waits for input to arrive (via interrupts) and be buffered in <span><code>cons.buf</code></span>, copies the input to user space, and (after a whole line has arrived) returns to the user process. If the user hasn’t typed a full line yet, any reading processes will wait in the <span><code>sleep</code></span> call <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/console.c#L103"><span>(kernel/console.c:103)</span></a> (Chapter <a href="#CH:SCHED" data-reference-type="ref" data-reference="CH:SCHED">6</a> explains the details of <span><code>sleep</code></span>).</p>
<p>When the user types a character, the UART hardware asks the RISC-V to raise an interrupt. The RISC-V and xv6 process the interrupt as described above, and xv6’s trap handling code calls <span><code>devintr</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/trap.c#L177"><span>(kernel/trap.c:177)</span></a>. <span><code>devintr</code></span> looks at the RISC-V <span><code>scause</code></span> register to discover that the interrupt is from an external device. Then it asks a hardware unit called the PLIC <span class="citation" data-cites="riscv:priv"></span> to tell it which device interrupted <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/trap.c#L186"><span>(kernel/trap.c:186)</span></a>. If it was the UART, <span><code>devintr</code></span> calls <span><code>uartintr</code></span>.</p>
<p><span><code>uartintr</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/uart.c#L84"><span>(kernel/uart.c:84)</span></a> reads any waiting input characters from the UART hardware and hands them to <span><code>consoleintr</code></span>; it doesn’t wait for characters, since future input will raise a new interrupt. The job of <span><code>consoleintr</code></span> is to accumulate input characters in <span><code>cons.buf</code></span> until a whole line arrives. <span><code>consoleintr</code></span> treats backspace and a few other characters specially. When a newline arrives, <span><code>consoleintr</code></span> wakes up a waiting <span><code>consoleread</code></span> (if there is one).</p>
<p>Once woken, <span><code>consoleread</code></span> will observe a full line in <span><code> cons.buf</code></span>, copy it to user space, and return (via the system call machinery) to user space.</p>
<p>On a multi-core machine, the interrupt may be delivered to any of the CPUs; the PLIC manages this decision. The interrupt may arrive on the same CPU that is running the process reading from the console; or it may arrive on a CPU that is doing something entirely unrelated. Thus interrupt handlers are not allowed to think about the process or code that they have interrupted.</p>
<h2 id="real-world-3">Real world</h2>
<p>The need for special trampoline pages could be eliminated if kernel memory were mapped into every process’s user page table. That would also eliminate the need for a page table switch when trapping from user space into the kernel. That in turn would allow system call implementations in the kernel to take advantage of the current process’s user memory being mapped, allowing kernel code to directly dereference user pointers. Many operating systems use these ideas to increase efficiency. Xv6 avoids them in order to reduce the chances of security bugs in the kernel due to inadvertent use of user pointers, and to reduce some complexity that would be required to ensure that user and kernel virtual addresses don’t overlap.</p>
<p>Xv6 allows device and timer interrupts while executing in the kernel, as well as when executing user programs. Timer interrupts force a thread switch (a call to <span><code>yield</code></span>) from the timer interrupt handler, even when executing in the kernel. The ability to time-slice the CPU fairly among kernel threads is useful if kernel threads sometimes spend a lot of time computing, without returning to user space. However, the need for kernel code to be mindful that it might be suspended (due to a timer interrupt) and later resume on a different CPU is the source of some complexity in xv6. The kernel could be made somewhat simpler if device and timer interrupts only occurred while executing user code.</p>
<p>Supporting all the devices on a typical computer in its full glory is much work, because there are many devices, the devices have many features, and the protocol between device and driver can be complex and poorly documented. In many operating systems, the drivers account for more code than the core kernel.</p>
<p>The UART driver retrieves data a byte at a time by reading the UART control registers; this pattern is called <em>programmed I/O</em>, since software is driving the data movement. Programmed I/O is simple, but too slow to be used at high data rates. Devices that need to move lots of data at high speed typically use <em>direct memory access (DMA)</em>. DMA device hardware directly writes incoming data to RAM, and reads outgoing data from RAM. Modern disk and network devices use DMA. A driver for a DMA device would prepare data in RAM, and then use a single write to a control register to tell the device to process the prepared data.</p>
<p>Interrupts make sense when a device needs attention at unpredictable times, and not too often. But interrupts have high CPU overhead. Thus high speed devices, such networks and disk controllers, use tricks that reduce the need for interrupts. One trick is to raise a single interrupt for a whole batch of incoming or outgoing requests. Another trick is for the driver to disable interrupts entirely, and to check the device periodically to see if it needs attention. This technique is called <em>polling</em>. Polling makes sense if the device performs operations very quickly, but it wastes CPU time if the device is mostly idle. Some drivers dynamically switch between polling and interrupts depending on the current device load.</p>
<p>The UART driver copies incoming data first to a buffer in the kernel, and then to user space. This makes sense at low data rates, but such a double copy can significantly reduce performance for devices that generate or consume data very quickly. Some operating systems are able to directly move data between user-space buffers and device hardware, often with DMA.</p>
<h2 id="exercises-3">Exercises</h2>
<ol>
<li><p><code>uartputc</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/uart.c#L61"><span>(kernel/uart.c:61)</span></a> polls the UART device to wait for it to finish with the previous output character. Convert it to use interrupts instead.</p></li>
<li><p>Add a driver for an Ethernet card.</p></li>
</ol>
<h1 id="CH:LOCK">Locking</h1>
<p>Most kernels, including xv6, interleave the execution of multiple activities. One source of interleaving is multiprocessor hardware: computers with multiple CPUs executing independently, such as xv6’s RISC-V. These multiple CPUs share physical RAM, and xv6 exploits the sharing to maintain data structures that all CPUs read and write. This sharing raises the possibility of one CPU reading a data structure while another CPU is mid-way through updating it, or even multiple CPUs updating the same data simultaneously; without careful design such parallel access is likely to yield incorrect results or a broken data structure. Even on a uniprocessor, the kernel may switch the CPU among a number of threads, causing their execution to be interleaved. Finally, a device interrupt handler that modifies the same data as some interruptible code could damage the data if the interrupt occurs at just the wrong time. The word <em>concurrency</em> refers to situations in which multiple instruction streams are interleaved, due to multiprocessor parallelism, thread switching, or interrupts.</p>
<p>Kernels are full of concurrently-accessed data. For example, two CPUs could simultaneously call <span><code>kalloc</code></span>, thereby concurrently popping from the head of the free list. Kernel designers like to allow for lots of concurrency, since it can yield increased performance though parallelism, and increased responsiveness. However, as a result kernel designers spend a lot of effort convincing themselves of correctness despite such concurrency. There are many ways to arrive at correct code, some easier to reason about than others. Strategies aimed at correctness under concurrency, and abstractions that support them, are called <em>concurrency control</em> techniques. Xv6 uses a number of concurrency control techniques, depending on the situation; many more are possible. This chapter focuses on a widely used technique: the <em>lock</em>.</p>
<p>A lock provides mutual exclusion, ensuring that only one CPU at a time can hold the lock. If the programmer associates a lock with each shared data item, and the code always holds the associated lock when using an item, then the item will be used by only one CPU at a time. In this situation, we say that the lock protects the data item.</p>
<p>The rest of this chapter explains why xv6 needs locks, how xv6 implements them, and how it uses them.</p>
<h2 id="race-conditions">Race conditions</h2>
<p>As an example of why we need locks, consider a linked list accessible from any CPU on a multiprocessor. The list supports push and pop operations, which may be called concurrently. Xv6’s memory allocator works in much this way; <code>kalloc()</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/kalloc.c#L69"><span>(kernel/kalloc.c:69)</span></a> pops a page of memory from a list of free pages, and <code>kfree()</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/kalloc.c#L47"><span>(kernel/kalloc.c:47)</span></a> pushes a page onto the free list.</p>
<p>If there were no concurrent requests, you might implement a list <code>push</code> operation as follows:</p>
<div class="sourceCode" id="cb19" data-numbers="left" data-startFrom="1"><pre class="sourceCode numberSource numberLines"><code class="sourceCode"><span id="cb19-1"><a href="#cb19-1"></a>struct element {</span>
<span id="cb19-2"><a href="#cb19-2"></a>      int data;</span>
<span id="cb19-3"><a href="#cb19-3"></a>      struct element *next;</span>
<span id="cb19-4"><a href="#cb19-4"></a>    };</span>
<span id="cb19-5"><a href="#cb19-5"></a>    </span>
<span id="cb19-6"><a href="#cb19-6"></a>    struct element *list = 0;</span>
<span id="cb19-7"><a href="#cb19-7"></a>    </span>
<span id="cb19-8"><a href="#cb19-8"></a>    void</span>
<span id="cb19-9"><a href="#cb19-9"></a>    push(int data)</span>
<span id="cb19-10"><a href="#cb19-10"></a>    {</span>
<span id="cb19-11"><a href="#cb19-11"></a>      struct element *l;</span>
<span id="cb19-12"><a href="#cb19-12"></a>   </span>
<span id="cb19-13"><a href="#cb19-13"></a>      l = malloc(sizeof *l);</span>
<span id="cb19-14"><a href="#cb19-14"></a>      l-&gt;data = data;</span>
<span id="cb19-15"><a href="#cb19-15"></a>      l-&gt;next = list; (*@\label{line:next}@*)</span>
<span id="cb19-16"><a href="#cb19-16"></a>      list = l;  (*@\label{line:list}@*)</span>
<span id="cb19-17"><a href="#cb19-17"></a>   }</span></code></pre></div>
<figure>
<embed src="fig/race.svg" id="fig:race" /><figcaption>Example race</figcaption>
</figure>
<p>This implementation is correct if executed in isolation. However, the code is not correct if more than one copy executes concurrently. If two CPUs execute <code>push</code> at the same time, both might execute line <a href="#line:next" data-reference-type="ref" data-reference="line:next">[line:next]</a>, before either executes line <a href="#line:list" data-reference-type="ref" data-reference="line:list">[line:list]</a>, (see Figure <a href="#fig:race" data-reference-type="ref" data-reference="fig:race">5.1</a>). There would then be two list elements with <code>next</code> set to the former value of <code>list</code>. When the two assignments to <code>list</code> happen at line  <a href="#line:list" data-reference-type="ref" data-reference="line:list">[line:list]</a>, the second one will overwrite the first; the element involved in the first assignment will be lost.</p>
<p>The lost update at line <a href="#line:list" data-reference-type="ref" data-reference="line:list">[line:list]</a> is an example of a <em>race condition</em>. A race condition is a situation in which a memory location is accessed concurrently, and at least one access is a write. A race is often a sign of a bug, either a lost update (if the accesses are writes) or a read of an incompletely-updated data structure. The outcome of a race depends on the exact timing of the two CPUs involved and how their memory operations are ordered by the memory system, which can make race-induced errors difficult to reproduce and debug. For example, adding print statements while debugging <code>push</code> might change the timing of the execution enough to make the race disappear.</p>
<p>The usual way to avoid races is to use a lock. Locks ensure <em>mutual exclusion</em>, so that only one CPU at a time can execute the sensitive lines of <code>push</code>; this makes the scenario above impossible. The correctly locked version of the above code adds just a few lines (highlighted in yellow):</p>
<div class="sourceCode" id="cb20" data-numbers="left" data-startFrom="6"><pre class="sourceCode numberSource numberLines"><code class="sourceCode" style="counter-reset: source-line 5;"><span id="cb20-6"><a href="#cb20-6"></a>struct element *list = 0;</span>
<span id="cb20-7"><a href="#cb20-7"></a>   (*@\hl{struct lock listlock;}@*)</span>
<span id="cb20-8"><a href="#cb20-8"></a>    	</span>
<span id="cb20-9"><a href="#cb20-9"></a>   void</span>
<span id="cb20-10"><a href="#cb20-10"></a>   push(int data)</span>
<span id="cb20-11"><a href="#cb20-11"></a>   {</span>
<span id="cb20-12"><a href="#cb20-12"></a>     struct element *l;</span>
<span id="cb20-13"><a href="#cb20-13"></a>     l = malloc(sizeof *l); (*@\label{line:malloc}@*)</span>
<span id="cb20-14"><a href="#cb20-14"></a>     l-&gt;data = data;</span>
<span id="cb20-15"><a href="#cb20-15"></a>   </span>
<span id="cb20-16"><a href="#cb20-16"></a>     (*@\hl{acquire(\&amp;listlock);} @*)</span>
<span id="cb20-17"><a href="#cb20-17"></a>     l-&gt;next = list;     (*@\label{line:next1}@*)</span>
<span id="cb20-18"><a href="#cb20-18"></a>     list = l;           (*@\label{line:list1}@*)</span>
<span id="cb20-19"><a href="#cb20-19"></a>     (*@\hl{release(\&amp;listlock)}; @*)</span>
<span id="cb20-20"><a href="#cb20-20"></a>   }</span></code></pre></div>
<p>The sequence of instructions between <code>acquire</code> and <code>release</code> is often called a <em>critical section</em>. The lock is typically said to be protecting <code>list</code>.</p>
<p>When we say that a lock protects data, we really mean that the lock protects some collection of invariants that apply to the data. Invariants are properties of data structures that are maintained across operations. Typically, an operation’s correct behavior depends on the invariants being true when the operation begins. The operation may temporarily violate the invariants but must reestablish them before finishing. For example, in the linked list case, the invariant is that <code>list</code> points at the first element in the list and that each element’s <code>next</code> field points at the next element. The implementation of <code>push</code> violates this invariant temporarily: in line <a href="#line:next1" data-reference-type="ref" data-reference="line:next1">[line:next1]</a>, <code>l</code> points to the next list element, but <code>list</code> does not point at <code>l</code> yet (reestablished at line <a href="#line:list1" data-reference-type="ref" data-reference="line:list1">[line:list1]</a>). The race condition we examined above happened because a second CPU executed code that depended on the list invariants while they were (temporarily) violated. Proper use of a lock ensures that only one CPU at a time can operate on the data structure in the critical section, so that no CPU will execute a data structure operation when the data structure’s invariants do not hold.</p>
<p>You can think of a lock as <em>serializing</em> concurrent critical sections so that they run one at a time, and thus preserve invariants (assuming the critical sections are correct in isolation). You can also think of critical sections guarded by the same lock as being atomic with respect to each other, so that each sees only the complete set of changes from earlier critical sections, and never sees partially-completed updates. We say that multiple processes <em>conflict</em> if they want the same lock at the same time, or that the lock experiences <em>contention</em>.</p>
<p>Note that it would be correct to move <code>acquire</code> earlier in <code>push.</code> For example, it is fine to move the call to <code>acquire</code> up to before line <a href="#line:malloc" data-reference-type="ref" data-reference="line:malloc">[line:malloc]</a>. This may reduce parallelism because then the calls to <code>malloc</code> are also serialized. The section “Using locks” below provides some guidelines for where to insert <code>acquire</code> and <code>release</code> invocations.</p>
<h2 id="code-locks">Code: Locks</h2>
<p>Xv6 has two types of locks: spinlocks and sleep-locks. We’ll start with spinlocks. Xv6 represents a spinlock as a <code>struct spinlock</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/spinlock.h#L2"><span>(kernel/spinlock.h:2)</span></a>. The important field in the structure is <code>locked</code>, a word that is zero when the lock is available and non-zero when it is held. Logically, xv6 should acquire a lock by executing code like</p>
<div class="sourceCode" id="cb21" data-numbers="left" data-startFrom="21"><pre class="sourceCode numberSource numberLines"><code class="sourceCode" style="counter-reset: source-line 20;"><span id="cb21-21"><a href="#cb21-21"></a>void</span>
<span id="cb21-22"><a href="#cb21-22"></a>   acquire(struct spinlock *lk) // does not work!</span>
<span id="cb21-23"><a href="#cb21-23"></a>   {</span>
<span id="cb21-24"><a href="#cb21-24"></a>     for(;;) {</span>
<span id="cb21-25"><a href="#cb21-25"></a>       if(lk-&gt;locked == 0) {  (*@\label{line:test}@*)</span>
<span id="cb21-26"><a href="#cb21-26"></a>         lk-&gt;locked = 1;      (*@\label{line:assign}@*)</span>
<span id="cb21-27"><a href="#cb21-27"></a>         break;</span>
<span id="cb21-28"><a href="#cb21-28"></a>       }</span>
<span id="cb21-29"><a href="#cb21-29"></a>     }</span>
<span id="cb21-30"><a href="#cb21-30"></a>   }</span></code></pre></div>
<p>Unfortunately, this implementation does not guarantee mutual exclusion on a multiprocessor. It could happen that two CPUs simultaneously reach line <a href="#line:test" data-reference-type="ref" data-reference="line:test">[line:test]</a>, see that <code>lk-&gt;locked</code> is zero, and then both grab the lock by executing line <a href="#line:assign" data-reference-type="ref" data-reference="line:assign">[line:assign]</a>. At this point, two different CPUs hold the lock, which violates the mutual exclusion property. What we need is a way to make lines <a href="#line:test" data-reference-type="ref" data-reference="line:test">[line:test]</a> and <a href="#line:assign" data-reference-type="ref" data-reference="line:assign">[line:assign]</a> execute as an <em>atomic</em> (i.e., indivisible) step.</p>
<p>Because locks are widely used, multi-core processors usually provide instructions that implement an atomic version of lines <a href="#line:test" data-reference-type="ref" data-reference="line:test">[line:test]</a> and <a href="#line:assign" data-reference-type="ref" data-reference="line:assign">[line:assign]</a>. On the RISC-V this instruction is <code>amoswap r, a</code>. <code>amoswap</code> reads the value at the memory address <span><code>a</code></span>, writes the contents of register <span><code>r</code></span> to that address, and puts the value it read into <span><code>r</code></span>. That is, it swaps the contents of the register and the memory address. It performs this sequence atomically, using special hardware to prevent any other CPU from using the memory address between the read and the write.</p>
<p>Xv6’s <code>acquire</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/spinlock.c#L22"><span>(kernel/spinlock.c:22)</span></a> uses the portable C library call <code>__sync_lock_test_and_set</code>, which boils down to the <code>amoswap</code> instruction; the return value is the old (swapped) contents of <code>lk-&gt;locked</code>. The <code>acquire</code> function wraps the swap in a loop, retrying (spinning) until it has acquired the lock. Each iteration swaps one into <code>lk-&gt;locked</code> and checks the previous value; if the previous value is zero, then we’ve acquired the lock, and the swap will have set <code>lk-&gt;locked</code> to one. If the previous value is one, then some other CPU holds the lock, and the fact that we atomically swapped one into <code>lk-&gt;locked</code> didn’t change its value.</p>
<p>Once the lock is acquired, <code>acquire</code> records, for debugging, the CPU that acquired the lock. The <code>lk-&gt;cpu</code> field is protected by the lock and must only be changed while holding the lock.</p>
<p>The function <code>release</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/spinlock.c#L46"><span>(kernel/spinlock.c:46)</span></a> is the opposite of <code>acquire</code>: it clears the <code>lk-&gt;cpu</code> field and then releases the lock. Conceptually, the release just requires assigning zero to <code>lk-&gt;locked</code>. The C standard allows compilers to implement an assignment with multiple store instructions, so a C assignment might be non-atomic with respect to concurrent code. Instead, <code>release</code> uses the C library function <code>__sync_lock_release</code> that performs an atomic assignment. This function also boils down to a RISC-V <code>amoswap</code> instruction.</p>
<h2 id="code-using-locks">Code: Using locks</h2>
<p>Xv6 uses locks in many places to avoid race conditions. To see a simple example much like <code>push</code> above, look at <code>kalloc</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/kalloc.c#L69"><span>(kernel/kalloc.c:69)</span></a> and <code>free</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/kalloc.c#L34"><span>(kernel/kalloc.c:34)</span></a>. Try Exercises 1 and 2 to see what happens if those functions omit the locks. You’ll likely find that it’s difficult to trigger incorrect behavior, suggesting that it’s hard to reliably test whether code is free from locking errors and races. It is not unlikely that xv6 has some races.</p>
<p>A hard part about using locks is deciding how many locks to use and which data and invariants each lock should protect. There are a few basic principles. First, any time a variable can be written by one CPU at the same time that another CPU can read or write it, a lock should be used to keep the two operations from overlapping. Second, remember that locks protect invariants: if an invariant involves multiple memory locations, typically all of them need to be protected by a single lock to ensure the invariant is maintained.</p>
<p>The rules above say when locks are necessary but say nothing about when locks are unnecessary, and it is important for efficiency not to lock too much, because locks reduce parallelism. If parallelism isn’t important, then one could arrange to have only a single thread and not worry about locks. A simple kernel can do this on a multiprocessor by having a single lock that must be acquired on entering the kernel and released on exiting the kernel (though system calls such as pipe reads or <code>wait</code> would pose a problem). Many uniprocessor operating systems have been converted to run on multiprocessors using this approach, sometimes called a “big kernel lock,” but the approach sacrifices parallelism: only one CPU can execute in the kernel at a time. If the kernel does any heavy computation, it would be more efficient to use a larger set of more fine-grained locks, so that the kernel could execute on multiple CPUs simultaneously.</p>
<p>As an example of coarse-grained locking, xv6’s <code>kalloc.c</code> allocator has a single free list protected by a single lock. If multiple processes on different CPUs try to allocate pages at the same time, each will have to wait for its turn by spinning in <span><code> acquire</code></span>. Spinning reduces performance, since it’s not useful work. If contention for the lock wasted a significant fraction of CPU time, perhaps performance could be improved by changing the allocator design to have multiple free lists, each with its own lock, to allow truly parallel allocation.</p>
<p>As an example of fine-grained locking, xv6 has a separate lock for each file, so that processes that manipulate different files can often proceed without waiting for each other’s locks. The file locking scheme could be made even more fine-grained if one wanted to allow processes to simultaneously write different areas of the same file. Ultimately lock granularity decisions need to be driven by performance measurements as well as complexity considerations.</p>
<p>As subsequent chapters explain each part of xv6, they will mention examples of xv6’s use of locks to deal with concurrency. As a preview, Figure <a href="#fig:locktable" data-reference-type="ref" data-reference="fig:locktable">[fig:locktable]</a> lists all of the locks in xv6.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;"><span><strong>Lock</strong></span></th>
<th style="text-align: left;"><span><strong>Description</strong></span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">bcache.lock</td>
<td style="text-align: left;">Protects allocation of block buffer cache entries</td>
</tr>
<tr class="even">
<td style="text-align: left;">cons.lock</td>
<td style="text-align: left;">Serializes access to console hardware, avoids intermixed output</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ftable.lock</td>
<td style="text-align: left;">Serializes allocation of a struct file in file table</td>
</tr>
<tr class="even">
<td style="text-align: left;">icache.lock</td>
<td style="text-align: left;">Protects allocation of inode cache entries</td>
</tr>
<tr class="odd">
<td style="text-align: left;">vdisk_lock</td>
<td style="text-align: left;">Serializes access to disk hardware and queue of DMA descriptors</td>
</tr>
<tr class="even">
<td style="text-align: left;">kmem.lock</td>
<td style="text-align: left;">Serializes allocation of memory</td>
</tr>
<tr class="odd">
<td style="text-align: left;">log.lock</td>
<td style="text-align: left;">Serializes operations on the transaction log</td>
</tr>
<tr class="even">
<td style="text-align: left;">pipe’s pi-&gt;lock</td>
<td style="text-align: left;">Serializes operations on each pipe</td>
</tr>
<tr class="odd">
<td style="text-align: left;">pid_lock</td>
<td style="text-align: left;">Serializes increments of next_pid</td>
</tr>
<tr class="even">
<td style="text-align: left;">proc’s p-&gt;lock</td>
<td style="text-align: left;">Serializes changes to process’s state</td>
</tr>
<tr class="odd">
<td style="text-align: left;">tickslock</td>
<td style="text-align: left;">Serializes operations on the ticks counter</td>
</tr>
<tr class="even">
<td style="text-align: left;">inode’s ip-&gt;lock</td>
<td style="text-align: left;">Serializes operations on each inode and its content</td>
</tr>
<tr class="odd">
<td style="text-align: left;">buf’s b-&gt;lock</td>
<td style="text-align: left;">Serializes operations on each block buffer</td>
</tr>
</tbody>
</table>
<h2 id="deadlock-and-lock-ordering">Deadlock and lock ordering</h2>
<p>If a code path through the kernel must hold several locks at the same time, it is important that all code paths acquire those locks in the same order. If they don’t, there is a risk of <em>deadlock</em>. Let’s say two code paths in xv6 need locks A and B, but code path 1 acquires locks in the order A then B, and the other path acquires them in the order B then A. Suppose thread T1 executes code path 1 and acquires lock A, and thread T2 executes code path 2 and acquires lock B. Next T1 will try to acquire lock B, and T2 will try to acquire lock A. Both acquires will block indefinitely, because in both cases the other thread holds the needed lock, and won’t release it until its acquire returns. To avoid such deadlocks, all code paths must acquire locks in the same order. The need for a global lock acquisition order means that locks are effectively part of each function’s specification: callers must invoke functions in a way that causes locks to be acquired in the agreed-on order.</p>
<p>Xv6 has many lock-order chains of length two involving per-process locks (the lock in each <code>struct proc</code>) due to the way that <code>sleep</code> works (see Chapter <a href="#CH:SCHED" data-reference-type="ref" data-reference="CH:SCHED">6</a>). For example, <code>consoleintr</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/console.c#L143"><span>(kernel/console.c:143)</span></a> is the interrupt routine which handles typed characters. When a newline arrives, any process that is waiting for console input should be woken up. To do this, <code>consoleintr</code> holds <code>cons.lock</code> while calling <code>wakeup</code>, which acquires the waiting process’s lock in order to wake it up. In consequence, the global deadlock-avoiding lock order includes the rule that <code>cons.lock</code> must be acquired before any process lock. The file-system code contains xv6’s longest lock chains. For example, creating a file requires simultaneously holding a lock on the directory, a lock on the new file’s inode, a lock on a disk block buffer, the disk driver’s <code>vdisk_lock</code>, and the calling process’s <code>p-&gt;lock</code>. To avoid deadlock, file-system code always acquires locks in the order mentioned in the previous sentence.</p>
<p>Honoring a global deadlock-avoiding order can be surprisingly difficult. Sometimes the lock order conflicts with logical program structure, e.g., perhaps code module M1 calls module M2, but the lock order requires that a lock in M2 be acquired before a lock in M1. Sometimes the identities of locks aren’t known in advance, perhaps because one lock must be held in order to discover the identity of the lock to be acquired next. This kind of situation arises in the file system as it looks up successive components in a path name, and in the code for <span><code>wait</code></span> and <span><code>exit</code></span> as they search the table of processes looking for child processes. Finally, the danger of deadlock is often a constraint on how fine-grained one can make a locking scheme, since more locks often means more opportunity for deadlock. The need to avoid deadlock is often a major factor in kernel implementation.</p>
<h2 id="locks-and-interrupt-handlers">Locks and interrupt handlers</h2>
<p>Some xv6 spinlocks protect data that is used by both threads and interrupt handlers. For example, the <code>clockintr</code> timer interrupt handler might increment <code>ticks</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/trap.c#L163"><span>(kernel/trap.c:163)</span></a> at about the same time that a kernel thread reads <code>ticks</code> in <code>sys_sleep</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/sysproc.c#L64"><span>(kernel/sysproc.c:64)</span></a>. The lock <code>tickslock</code> serializes the two accesses.</p>
<p>The interaction of spinlocks and interrupts raises a potential danger. Suppose <code>sys_sleep</code> holds <code>tickslock</code>, and its CPU is interrupted by a timer interrupt. <code>clockintr</code> would try to acquire <code>tickslock</code>, see it was held, and wait for it to be released. In this situation, <code>tickslock</code> will never be released: only <code>sys_sleep</code> can release it, but <code>sys_sleep</code> will not continue running until <code>clockintr</code> returns. So the CPU will deadlock, and any code that needs either lock will also freeze.</p>
<p>To avoid this situation, if a spinlock is used by an interrupt handler, a CPU must never hold that lock with interrupts enabled. Xv6 is more conservative: when a CPU acquires any lock, xv6 always disables interrupts on that CPU. Interrupts may still occur on other CPUs, so an interrupt’s <code>acquire</code> can wait for a thread to release a spinlock; just not on the same CPU.</p>
<p>xv6 re-enables interrupts when a CPU holds no spinlocks; it must do a little book-keeping to cope with nested critical sections. <code>acquire</code> calls <code>push_off</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/spinlock.c#L87"><span>(kernel/spinlock.c:87)</span></a> and <code>release</code> calls <code>pop_off</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/spinlock.c#L98"><span>(kernel/spinlock.c:98)</span></a> to track the nesting level of locks on the current CPU. When that count reaches zero, <code>pop_off</code> restores the interrupt enable state that existed at the start of the outermost critical section. The <code>intr_off</code> and <code>intr_on</code> functions execute RISC-V instructions to disable and enable interrupts, respectively.</p>
<p>It is important that <code>acquire</code> call <code>push_off</code> strictly before setting <code>lk-&gt;locked</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/spinlock.c#L28"><span>(kernel/spinlock.c:28)</span></a>. If the two were reversed, there would be a brief window when the lock was held with interrupts enabled, and an unfortunately timed interrupt would deadlock the system. Similarly, it is important that <code>release</code> call <code>pop_off</code> only after releasing the lock <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/spinlock.c#L63"><span>(kernel/spinlock.c:63)</span></a>.</p>
<h2 id="instruction-and-memory-ordering">Instruction and memory ordering</h2>
<p>It is natural to think of programs executing in the order in which source code statements appear. Many compilers and CPUs, however, execute code out of order to achieve higher performance. If an instruction takes many cycles to complete, a CPU may issue the instruction early so that it can overlap with other instructions and avoid CPU stalls. For example, a CPU may notice that in a serial sequence of instructions A and B are not dependent on each other. The CPU may start instruction B first, either because its inputs are ready before A’s inputs, or in order to overlap execution of A and B. A compiler may perform a similar re-ordering by emitting instructions for one statement before the instructions for a statement that precedes it in the source.</p>
<p>Compilers and CPUs follow rules when they re-order to ensure that they don’t change the results of correctly-written serial code. However, the rules do allow re-ordering that changes the results of concurrent code, and can easily lead to incorrect behavior on multiprocessors <span class="citation" data-cites="riscv:user boehm04"></span>. The CPU’s ordering rules are called the <em>memory model</em>.</p>
<p>For example, in this code for <code>push</code>, it would be a disaster if the compiler or CPU moved the store corresponding to line <a href="#line:next2" data-reference-type="ref" data-reference="line:next2">[line:next2]</a> to a point after the <code>release</code> on line <a href="#line:release" data-reference-type="ref" data-reference="line:release">[line:release]</a>:</p>
<div class="sourceCode" id="cb22" data-numbers="left" data-startFrom="1"><pre class="sourceCode numberSource numberLines"><code class="sourceCode"><span id="cb22-1"><a href="#cb22-1"></a>l = malloc(sizeof *l);</span>
<span id="cb22-2"><a href="#cb22-2"></a>      l-&gt;data = data;</span>
<span id="cb22-3"><a href="#cb22-3"></a>      acquire(&amp;listlock);</span>
<span id="cb22-4"><a href="#cb22-4"></a>      l-&gt;next = list;   (*@\label{line:next2}@*)</span>
<span id="cb22-5"><a href="#cb22-5"></a>      list = l;      </span>
<span id="cb22-6"><a href="#cb22-6"></a>      release(&amp;listlock);  (*@\label{line:release}@*)</span></code></pre></div>
<p>If such a re-ordering occurred, there would be a window during which another CPU could acquire the lock and observe the updated <code>list</code>, but see an uninitialized <code>list-&gt;next</code>.</p>
<p>To tell the hardware and compiler not to perform such re-orderings, xv6 uses <code>__sync_synchronize()</code> in both <code>acquire</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/spinlock.c#L22"><span>(kernel/spinlock.c:22)</span></a> and <code>release</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/spinlock.c#L46"><span>(kernel/spinlock.c:46)</span></a>. <code>__sync_synchronize()</code> is a <em>memory barrier</em>: it tells the compiler and CPU to not reorder loads or stores across the barrier. The barriers in xv6’s <code>acquire</code> and <code>release</code> force order in almost all cases where it matters, since xv6 uses locks around accesses to shared data. Chapter <a href="#CH:LOCK2" data-reference-type="ref" data-reference="CH:LOCK2">8</a> discusses a few exceptions.</p>
<h2 id="sleep-locks">Sleep locks</h2>
<p>Sometimes xv6 needs to hold a lock for a long time. For example, the file system (Chapter <a href="#CH:FS" data-reference-type="ref" data-reference="CH:FS">7</a>) keeps a file locked while reading and writing its content on the disk, and these disk operations can take tens of milliseconds. Holding a spinlock that long would lead to waste if another process wanted to acquire it, since the acquiring process would waste CPU for a long time while spinning. Another drawback of spinlocks is that a process cannot yield the CPU while retaining a spinlock; we’d like to do this so that other processes can use the CPU while the process with the lock waits for the disk. Yielding while holding a spinlock is illegal because it might lead to deadlock if a second thread then tried to acquire the spinlock; since <span><code>acquire</code></span> doesn’t yield the CPU, the second thread’s spinning might prevent the first thread from running and releasing the lock. Yielding while holding a lock would also violate the requirement that interrupts must be off while a spinlock is held. Thus we’d like a type of lock that yields the CPU while waiting to acquire, and allows yields (and interrupts) while the lock is held.</p>
<p>Xv6 provides such locks in the form of <em>sleep-locks</em>. <code>acquiresleep</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/sleeplock.c#L22"><span>(kernel/sleeplock.c:22)</span></a> yields the CPU while waiting, using techniques that will be explained in Chapter <a href="#CH:SCHED" data-reference-type="ref" data-reference="CH:SCHED">6</a>. At a high level, a sleep-lock has a <code>locked</code> field that is protected by a spinlock, and <code>acquiresleep</code> ’s call to <code>sleep</code> atomically yields the CPU and releases the spinlock. The result is that other threads can execute while <code>acquiresleep</code> waits.</p>
<p>Because sleep-locks leave interrupts enabled, they cannot be used in interrupt handlers. Because <code>acquiresleep</code> may yield the CPU, sleep-locks cannot be used inside spinlock critical sections (though spinlocks can be used inside sleep-lock critical sections).</p>
<p>Spin-locks are best suited to short critical sections, since waiting for them wastes CPU time; sleep-locks work well for lengthy operations.</p>
<h2 id="real-world-4">Real world</h2>
<p>Programming with locks remains challenging despite years of research into concurrency primitives and parallelism. It is often best to conceal locks within higher-level constructs like synchronized queues, although xv6 does not do this. If you program with locks, it is wise to use a tool that attempts to identify race conditions, because it is easy to miss an invariant that requires a lock.</p>
<p>Most operating systems support POSIX threads (Pthreads), which allow a user process to have several threads running concurrently on different CPUs. Pthreads has support for user-level locks, barriers, etc. Supporting Pthreads requires support from the operating system. For example, it should be the case that if one pthread blocks in a system call, another pthread of the same process should be able to run on that CPU. As another example, if a pthread changes its process’s address space (e.g., maps or unmaps memory), the kernel must arrange that other CPUs that run threads of the same process update their hardware page tables to reflect the change in the address space.</p>
<p>It is possible to implement locks without atomic instructions, but it is expensive, and most operating systems use atomic instructions.</p>
<p>Locks can be expensive if many CPUs try to acquire the same lock at the same time. If one CPU has a lock cached in its local cache, and another CPU must acquire the lock, then the atomic instruction to update the cache line that holds the lock must move the line from the one CPU’s cache to the other CPU’s cache, and perhaps invalidate any other copies of the cache line. Fetching a cache line from another CPU’s cache can be orders of magnitude more expensive than fetching a line from a local cache.</p>
<p>To avoid the expenses associated with locks, many operating systems use lock-free data structures and algorithms. For example, it is possible to implement a linked list like the one in the beginning of the chapter that requires no locks during list searches, and one atomic instruction to insert an item in a list. Lock-free programming is more complicated, however, than programming locks; for example, one must worry about instruction and memory reordering. Programming with locks is already hard, so xv6 avoids the additional complexity of lock-free programming.</p>
<h2 id="exercises-4">Exercises</h2>
<ol>
<li><p>Comment out the calls to <code>acquire</code> and <code>release</code> in <code>kalloc</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/kalloc.c#L69"><span>(kernel/kalloc.c:69)</span></a>. This seems like it should cause problems for kernel code that calls <code>kalloc</code>; what symptoms do you expect to see? When you run xv6, do you see these symptoms? How about when running <code>usertests</code>? If you don’t see a problem, why not? See if you can provoke a problem by inserting dummy loops into the critical section of <code>kalloc</code>.</p></li>
<li><p>Suppose that you instead commented out the locking in <code>kfree</code> (after restoring locking in <code>kalloc</code>). What might now go wrong? Is lack of locks in <code>kfree</code> less harmful than in <code>kalloc</code>?</p></li>
<li><p>If two CPUs call <code>kalloc</code> at the same time, one will have to wait for the other, which is bad for performance. Modify <code>kalloc.c</code> to have more parallelism, so that simultaneous calls to <code>kalloc</code> from different CPUs can proceed without waiting for each other.</p></li>
<li><p>Write a parallel program using POSIX threads, which is supported on most operating systems. For example, implement a parallel hash table and measure if the number of puts/gets scales with increasing number of cores.</p></li>
<li><p>Implement a subset of Pthreads in xv6. That is, implement a user-level thread library so that a user process can have more than 1 thread and arrange that these threads can run in parallel on different CPUs. Come up with a design that correctly handles a thread making a blocking system call and changing its shared address space.</p></li>
</ol>
<h1 id="CH:SCHED">Scheduling</h1>
<p>Any operating system is likely to run with more processes than the computer has CPUs, so a plan is needed to time-share the CPUs among the processes. Ideally the sharing would be transparent to user processes. A common approach is to provide each process with the illusion that it has its own virtual CPU by <em>multiplexing</em> the processes onto the hardware CPUs. This chapter explains how xv6 achieves this multiplexing.</p>
<h2 id="multiplexing">Multiplexing</h2>
<p>Xv6 multiplexes by switching each CPU from one process to another in two situations. First, xv6’s <code>sleep</code> and <code>wakeup</code> mechanism switches when a process waits for device or pipe I/O to complete, or waits for a child to exit, or waits in the <code>sleep</code> system call. Second, xv6 periodically forces a switch to cope with processes that compute for long periods without sleeping. This multiplexing creates the illusion that each process has its own CPU, just as xv6 uses the memory allocator and hardware page tables to create the illusion that each process has its own memory.</p>
<p>Implementing multiplexing poses a few challenges. First, how to switch from one process to another? Although the idea of context switching is simple, the implementation is some of the most opaque code in xv6. Second, how to force switches in a way that is transparent to user processes? Xv6 uses the standard technique of driving context switches with timer interrupts. Third, many CPUs may be switching among processes concurrently, and a locking plan is necessary to avoid races. Fourth, a process’s memory and other resources must be freed when the process exits, but it cannot do all of this itself because (for example) it can’t free its own kernel stack while still using it. Fifth, each core of a multi-core machine must remember which process it is executing so that system calls affect the correct process’s kernel state. Finally, <code>sleep</code> and <code>wakeup</code> allow a process to give up the CPU and sleep waiting for an event, and allows another process to wake the first process up. Care is needed to avoid races that result in the loss of wakeup notifications. Xv6 tries to solve these problems as simply as possible, but nevertheless the resulting code is tricky.</p>
<h2 id="code-context-switching">Code: Context switching</h2>
<figure>
<embed src="fig/switch.svg" id="fig:switch" /><figcaption>Switching from one user process to another. In this example, xv6 runs with one CPU (and thus one scheduler thread).</figcaption>
</figure>
<p>Figure <a href="#fig:switch" data-reference-type="ref" data-reference="fig:switch">6.1</a> outlines the steps involved in switching from one user process to another: a user-kernel transition (system call or interrupt) to the old process’s kernel thread, a context switch to the current CPU’s scheduler thread, a context switch to a new process’s kernel thread, and a trap return to the user-level process. The xv6 scheduler has a dedicated thread (saved registers and stack) per CPU because it is sometimes not safe for it execute on any process’s kernel stack; we’ll see an example in <code>exit</code><span style="color: red"><strong>MFK:</strong> since xv6 doesn’t free kernel stacks anymore, this point is less obvious now and we don’t spell it out in 6.8.</span>. In this section we’ll examine the mechanics of switching between a kernel thread and a scheduler thread.</p>
<p>Switching from one thread to another involves saving the old thread’s CPU registers, and restoring the previously-saved registers of the new thread; the fact that the stack pointer and program counter are saved and restored means that the CPU will switch stacks and switch what code it is executing.</p>
<p>The function <code>swtch</code> performs the saves and restores for a thread switch. <code>swtch</code> doesn’t directly know about threads; it just saves and restores register sets, called <em>contexts</em>. When it is time for a process to give up the CPU, the process’s kernel thread calls <code>swtch</code> to save its own context and return to the scheduler context. Each context is contained in a <code>struct</code> <code>context</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.h#L2"><span>(kernel/proc.h:2)</span></a>, itself contained in a process’s <code>struct proc</code> or a CPU’s <code>struct cpu</code>. <code>Swtch</code> takes two arguments: <code>struct context</code> <code>*old</code> and <code>struct</code> <code>context</code> <code>*new</code>. It saves the current registers in <code>old</code>, loads registers from <code>new</code>, and returns.</p>
<p>Let’s follow a process through <code>swtch</code> into the scheduler. We saw in Chapter <a href="#CH:TRAP" data-reference-type="ref" data-reference="CH:TRAP">4</a> that one possibility at the end of an interrupt is that <code>usertrap</code> calls <code>yield</code>. <code>Yield</code> in turn calls <code>sched</code>, which calls <code>swtch</code> to save the current context in <code>p-&gt;context</code> and switch to the scheduler context previously saved in <code>cpu-&gt;scheduler</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L494"><span>(kernel/proc.c:494)</span></a>.</p>
<p><code>Swtch</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/swtch.S#L3"><span>(kernel/swtch.S:3)</span></a> saves only callee-saved registers; caller-saved registers are saved on the stack (if needed) by the calling C code. <code>Swtch</code> knows the offset of each register’s field in <code>struct context</code>. It does not save the program counter. Instead, <code>swtch</code> saves the <code>ra</code> register, which holds the return address from which <code>swtch</code> was called. Now <code>swtch</code> restores registers from the new context, which holds register values saved by a previous <code>swtch</code>. When <code>swtch</code> returns, it returns to the instructions pointed to by the restored <code>ra</code> register, that is, the instruction from which the new thread previously called <code>swtch</code>. In addition, it returns on the new thread’s stack.</p>
<p>In our example, <code>sched</code> called <code>swtch</code> to switch to <code>cpu-&gt;scheduler</code>, the per-CPU scheduler context. That context had been saved by <code>scheduler</code>’s call to <code>swtch</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L460"><span>(kernel/proc.c:460)</span></a>. When the <code>swtch</code> we have been tracing returns, it returns not to <code>sched</code> but to <code>scheduler</code>, and its stack pointer points at the current CPU’s scheduler stack.</p>
<h2 id="code-scheduling">Code: Scheduling</h2>
<p>The last section looked at the low-level details of <code>swtch</code>; now let’s take <code>swtch</code> as a given and examine switching from a process through the scheduler to another process. The scheduler exists in the form of a special thread per CPU, each running the <code>scheduler</code> function. This function is in charge of choosing which process to run next. A process that wants to give up the CPU must acquire its own process lock <code>p-&gt;lock</code>, release any other locks it is holding, update its own state (<code>p-&gt;state</code>), and then call <code>sched</code>. <code>Yield</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L500"><span>(kernel/proc.c:500)</span></a> follows this convention, as do <code>sleep</code> and <code>exit</code>, which we will examine later. <code>Sched</code> double-checks those conditions <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L484-L489">(kernel/proc.c:484-489)</a> and then an implication of those conditions: since a lock is held, interrupts should be disabled. Finally, <code>sched</code> calls <code>swtch</code> to save the current context in <code>p-&gt;context</code> and switch to the scheduler context in <code>cpu-&gt;scheduler</code>. <code>Swtch</code> returns on the scheduler’s stack as though <code>scheduler</code>’s <code>swtch</code> had returned <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L460"><span>(kernel/proc.c:460)</span></a>. The scheduler continues the <code>for</code> loop, finds a process to run, switches to it, and the cycle repeats.</p>
<p>We just saw that xv6 holds <code>p-&gt;lock</code> across calls to <code>swtch</code>: the caller of <code>swtch</code> must already hold the lock, and control of the lock passes to the switched-to code. This convention is unusual with locks; usually the thread that acquires a lock is also responsible for releasing the lock, which makes it easier to reason about correctness. For context switching it is necessary to break this convention because <code>p-&gt;lock</code> protects invariants on the process’s <code>state</code> and <code>context</code> fields that are not true while executing in <code>swtch</code>. One example of a problem that could arise if <code>p-&gt;lock</code> were not held during <code>swtch</code>: a different CPU might decide to run the process after <code>yield</code> had set its state to <code>RUNNABLE</code>, but before <code>swtch</code> caused it to stop using its own kernel stack. The result would be two CPUs running on the same stack, which cannot be right.</p>
<p>A kernel thread always gives up its CPU in <code>sched</code> and always switches to the same location in the scheduler, which (almost) always switches to some kernel thread that previously called <code>sched</code>. Thus, if one were to print out the line numbers where xv6 switches threads, one would observe the following simple pattern: <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L460"><span>(kernel/proc.c:460)</span></a>, <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L494"><span>(kernel/proc.c:494)</span></a>, <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L460"><span>(kernel/proc.c:460)</span></a>, <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L494"><span>(kernel/proc.c:494)</span></a>, and so on. The procedures in which this stylized switching between two threads happens are sometimes referred to as <em>coroutines</em>; in this example, <code>sched</code> and <code>scheduler</code> are co-routines of each other.</p>
<p>There is one case when the scheduler’s call to <code>swtch</code> does not end up in <code>sched</code>. When a new process is first scheduled, it begins at <code>forkret</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L512"><span>(kernel/proc.c:512)</span></a>. <code>Forkret</code> exists to release the <code>p-&gt;lock</code>; otherwise, the new process could start at <code>usertrapret</code>.</p>
<p><code>Scheduler</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L442"><span>(kernel/proc.c:442)</span></a> runs a simple loop: find a process to run, run it until it yields, repeat. The scheduler loops over the process table looking for a runnable process, one that has <code>p-&gt;state</code> <code>==</code> <code>RUNNABLE</code>. Once it finds a process, it sets the per-CPU current process variable <code>c-&gt;proc</code>, marks the process as <code>RUNNING</code>, and then calls <code>swtch</code> to start running it <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L455-L460">(kernel/proc.c:455-460)</a>.</p>
<p>One way to think about the structure of the scheduling code is that it enforces a set of invariants about each process, and holds <code>p-&gt;lock</code> whenever those invariants are not true. One invariant is that if a process is <code>RUNNING</code>, a timer interrupt’s <code>yield</code> must be able to safely switch away from the process; this means that the CPU registers must hold the process’s register values (i.e. <code>swtch</code> hasn’t moved them to a <code>context</code>), and <code>c-&gt;proc</code> must refer to the process. Another invariant is that if a process is <code>RUNNABLE</code>, it must be safe for an idle CPU’s <code>scheduler</code> to run it; this means that <code>p-&gt;context</code> must hold the process’s registers (i.e., they are not actually in the real registers), that no CPU is executing on the process’s kernel stack, and that no CPU’s <code>c-&gt;proc</code> refers to the process. Observe that these properties are often not true while <code>p-&gt;lock</code> is held.</p>
<p>Maintaining the above invariants is the reason why xv6 often acquires <code>p-&gt;lock</code> in one thread and releases it in other, for example acquiring in <code>yield</code> and releasing in <code>scheduler</code>. Once <code>yield</code> has started to modify a running process’s state to make it <code>RUNNABLE</code>, the lock must remain held until the invariants are restored: the earliest correct release point is after <code>scheduler</code> (running on its own stack) clears <code>c-&gt;proc</code>. Similarly, once <code>scheduler</code> starts to convert a runnable process to <code>RUNNING</code>, the lock cannot be released until the kernel thread is completely running (after the <code>swtch</code>, for example in <code>yield</code>).</p>
<p><code>p-&gt;lock</code> protects other things as well: the interplay between <code>exit</code> and <code>wait</code>, the machinery to avoid lost wakeups (see Section <a href="#sec:sleep" data-reference-type="ref" data-reference="sec:sleep">6.5</a>), and avoidance of races between a process exiting and other processes reading or writing its state (e.g., the <code>exit</code> system call looking at <code>p-&gt;pid</code> and setting <code>p-&gt;killed</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L596"><span>(kernel/proc.c:596)</span></a>). It might be worth thinking about whether the different functions of <code>p-&gt;lock</code> could be split up, for clarity and perhaps for performance.</p>
<h2 id="code-mycpu-and-myproc">Code: mycpu and myproc</h2>
<p>Xv6 often needs a pointer to the current process’s <code>proc</code> structure. On a uniprocessor one could have a global variable pointing to the current <code>proc</code>. This doesn’t work on a multi-core machine, since each core executes a different process. The way to solve this problem is to exploit the fact that each core has its own set of registers; we can use one of those registers to help find per-core information.</p>
<p>Xv6 maintains a <code>struct cpu</code> for each CPU <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.h#L22"><span>(kernel/proc.h:22)</span></a>, which records the process currently running on that CPU (if any), saved registers for the CPU’s scheduler thread, and the count of nested spinlocks needed to manage interrupt disabling. The function <code>mycpu</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L58"><span>(kernel/proc.c:58)</span></a> returns a pointer to the current CPU’s <code>struct cpu</code>. RISC-V numbers its CPUs, giving each a <em>hartid</em>. Xv6 ensures that each CPU’s hartid is stored in that CPU’s <code>tp</code> register while in the kernel. This allows <code>mycpu</code> to use <code>tp</code> to index an array of <code>cpu</code> structures to find the right one.</p>
<p>Ensuring that a CPU’s <code>tp</code> always holds the CPU’s hartid is a little involved. <code>mstart</code> sets the <code>tp</code> register early in the CPU’s boot sequence, while still in machine mode <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/start.c#L45"><span>(kernel/start.c:45)</span></a>. <code>usertrapret</code> saves <code>tp</code> in the trampoline page, because the user process might modify <code>tp</code>. Finally, <code>uservec</code> restores that saved <code>tp</code> when entering the kernel from user space <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/trampoline.S#L70"><span>(kernel/trampoline.S:70)</span></a>. The compiler guarantees never to use the <code>tp</code> register. It would be more convenient if RISC-V allowed xv6 to read the current hartid directly, but that is allowed only in machine mode, not in supervisor mode.</p>
<p>The return values of <code>cpuid</code> and <code>mycpu</code> are fragile: if the timer were to interrupt and cause the thread to yield and then move to a different CPU, the return value would no longer be correct. To avoid this problem, xv6 requires that callers disable interrupts, and only enable them after they finish using the returned <code>struct cpu</code>.</p>
<p>The function <code>myproc</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L66"><span>(kernel/proc.c:66)</span></a> returns the <code>struct proc</code> pointer for the process that is running on the current CPU. <code>myproc</code> disables interrupts, invokes <code>mycpu</code>, fetches the current process pointer (<code>c-&gt;proc</code>) out of the <code>struct cpu</code>, and then enables interrupts. The return value of <code>myproc</code> is safe to use even if interrupts are enabled: if a timer interrupt moves the calling process to a different CPU, its <code>struct proc</code> pointer will stay the same.</p>
<h2 id="sec:sleep">Sleep and wakeup</h2>
<p>Scheduling and locks help conceal the existence of one process from another, but so far we have no abstractions that help processes intentionally interact. Many mechanisms have been invented to solve this problem. Xv6 uses one called sleep and wakeup, which allow one process to sleep waiting for an event and another process to wake it up once the event has happened. Sleep and wakeup are often called <em>sequence coordination</em> or <em>conditional synchronization</em> mechanisms.</p>
<p>To illustrate what we mean, let’s consider a simple synchronization mechanism called a <em>semaphore</em> <span class="citation" data-cites="dijkstra65"></span> to coordinate a producer and a consumer. A semaphore maintains a count and provides two operations. The “V” operation (for the producer) increments the count. The “P” operation (for the consumer) waits until the count is non-zero, and then decrements it and returns. If there were only one producer thread and one consumer thread, and they executed on different CPUs, and the compiler didn’t optimize too aggressively, this implementation would be correct:</p>
<div class="sourceCode" id="cb23" data-numbers="left" data-startFrom="100"><pre class="sourceCode numberSource numberLines"><code class="sourceCode" style="counter-reset: source-line 99;"><span id="cb23-100"><a href="#cb23-100"></a>struct semaphore {</span>
<span id="cb23-101"><a href="#cb23-101"></a>    struct spinlock lock;</span>
<span id="cb23-102"><a href="#cb23-102"></a>    int count;</span>
<span id="cb23-103"><a href="#cb23-103"></a>  };</span>
<span id="cb23-104"><a href="#cb23-104"></a></span>
<span id="cb23-105"><a href="#cb23-105"></a>  void</span>
<span id="cb23-106"><a href="#cb23-106"></a>  V(struct semaphore *s)</span>
<span id="cb23-107"><a href="#cb23-107"></a>  {</span>
<span id="cb23-108"><a href="#cb23-108"></a>     acquire(&amp;s-&gt;lock);</span>
<span id="cb23-109"><a href="#cb23-109"></a>     s-&gt;count += 1;</span>
<span id="cb23-110"><a href="#cb23-110"></a>     release(&amp;s-&gt;lock);</span>
<span id="cb23-111"><a href="#cb23-111"></a>  }</span>
<span id="cb23-112"><a href="#cb23-112"></a></span>
<span id="cb23-113"><a href="#cb23-113"></a>  void</span>
<span id="cb23-114"><a href="#cb23-114"></a>  P(struct semaphore *s)</span>
<span id="cb23-115"><a href="#cb23-115"></a>  {</span>
<span id="cb23-116"><a href="#cb23-116"></a>     while(s-&gt;count == 0)</span>
<span id="cb23-117"><a href="#cb23-117"></a>       ;</span>
<span id="cb23-118"><a href="#cb23-118"></a>     acquire(&amp;s-&gt;lock);</span>
<span id="cb23-119"><a href="#cb23-119"></a>     s-&gt;count -= 1;</span>
<span id="cb23-120"><a href="#cb23-120"></a>     release(&amp;s-&gt;lock);</span>
<span id="cb23-121"><a href="#cb23-121"></a>  }</span></code></pre></div>
<p>The implementation above is expensive. If the producer acts rarely, the consumer will spend most of its time spinning in the <code>while</code> loop hoping for a non-zero count. The consumer’s CPU could find more productive work than with <em>busy waiting</em> by repeatedly <em>polling</em> <code>s-&gt;count</code>. Avoiding busy waiting requires a way for the consumer to yield the CPU and resume only after <code>V</code> increments the count.</p>
<p>Here’s a step in that direction, though as we will see it is not enough. Let’s imagine a pair of calls, <code>sleep</code> and <code>wakeup</code>, that work as follows. <code>Sleep(chan)</code> sleeps on the arbitrary value <code>chan</code>, called the <em>wait channel</em>. <code>Sleep</code> puts the calling process to sleep, releasing the CPU for other work. <code>Wakeup(chan)</code> wakes all processes sleeping on <code>chan</code> (if any), causing their <code>sleep</code> calls to return. If no processes are waiting on <code>chan</code>, <code>wakeup</code> does nothing. We can change the semaphore implementation to use <code>sleep</code> and <code>wakeup</code> (changes highlighted in yellow):</p>
<div class="sourceCode" id="cb24" data-numbers="left" data-startFrom="200"><pre class="sourceCode numberSource numberLines"><code class="sourceCode" style="counter-reset: source-line 199;"><span id="cb24-200"><a href="#cb24-200"></a>void</span>
<span id="cb24-201"><a href="#cb24-201"></a>  V(struct semaphore *s)</span>
<span id="cb24-202"><a href="#cb24-202"></a>  {</span>
<span id="cb24-203"><a href="#cb24-203"></a>     acquire(&amp;s-&gt;lock);</span>
<span id="cb24-204"><a href="#cb24-204"></a>     s-&gt;count += 1;</span>
<span id="cb24-205"><a href="#cb24-205"></a>     (*@\hl{wakeup(s);}@*)</span>
<span id="cb24-206"><a href="#cb24-206"></a>     release(&amp;s-&gt;lock);</span>
<span id="cb24-207"><a href="#cb24-207"></a>  }</span>
<span id="cb24-208"><a href="#cb24-208"></a>  </span>
<span id="cb24-209"><a href="#cb24-209"></a>  void</span>
<span id="cb24-210"><a href="#cb24-210"></a>  P(struct semaphore *s)</span>
<span id="cb24-211"><a href="#cb24-211"></a>  {</span>
<span id="cb24-212"><a href="#cb24-212"></a>    while(s-&gt;count == 0)    (*@\label{line:test}@*)</span>
<span id="cb24-213"><a href="#cb24-213"></a>      (*@\hl{sleep(s);}@*)  (*@\label{line:sleep}@*)</span>
<span id="cb24-214"><a href="#cb24-214"></a>    acquire(&amp;s-&gt;lock);</span>
<span id="cb24-215"><a href="#cb24-215"></a>    s-&gt;count -= 1;</span>
<span id="cb24-216"><a href="#cb24-216"></a>    release(&amp;s-&gt;lock);</span>
<span id="cb24-217"><a href="#cb24-217"></a>  }</span></code></pre></div>
<p><code>P</code> now gives up the CPU instead of spinning, which is nice. However, it turns out not to be straightforward to design <code>sleep</code> and <code>wakeup</code> with this interface without suffering from what is known as the <em>lost wake-up</em> problem. Suppose that <code>P</code> finds that <code>s-&gt;count</code> <code>==</code> <code>0</code> on line <a href="#line:test" data-reference-type="ref" data-reference="line:test">[line:test]</a>. While <code>P</code> is between lines <a href="#line:test" data-reference-type="ref" data-reference="line:test">[line:test]</a> and <a href="#line:sleep" data-reference-type="ref" data-reference="line:sleep">[line:sleep]</a>, <code>V</code> runs on another CPU: it changes <code>s-&gt;count</code> to be nonzero and calls <code>wakeup</code>, which finds no processes sleeping and thus does nothing. Now <code>P</code> continues executing at line <a href="#line:sleep" data-reference-type="ref" data-reference="line:sleep">[line:sleep]</a>: it calls <code>sleep</code> and goes to sleep. This causes a problem: <code>P</code> is asleep waiting for a <code>V</code> call that has already happened. Unless we get lucky and the producer calls <code>V</code> again, the consumer will wait forever (it is <em>deadlocked</em>) even though the count is non-zero.</p>
<p>The root of this problem is that the invariant that <code>P</code> only sleeps when <code>s-&gt;count</code> <code>==</code> <code>0</code> is violated by <code>V</code> running at just the wrong moment. An incorrect way to protect the invariant would be to move the lock acquisition (highlighted in yellow below) in <code>P</code> so that its check of the count and its call to <code>sleep</code> are atomic:</p>
<div class="sourceCode" id="cb25" data-numbers="left" data-startFrom="300"><pre class="sourceCode numberSource numberLines"><code class="sourceCode" style="counter-reset: source-line 299;"><span id="cb25-300"><a href="#cb25-300"></a>void</span>
<span id="cb25-301"><a href="#cb25-301"></a>  V(struct semaphore *s)</span>
<span id="cb25-302"><a href="#cb25-302"></a>  {</span>
<span id="cb25-303"><a href="#cb25-303"></a>    acquire(&amp;s-&gt;lock);</span>
<span id="cb25-304"><a href="#cb25-304"></a>    s-&gt;count += 1;</span>
<span id="cb25-305"><a href="#cb25-305"></a>    wakeup(s);</span>
<span id="cb25-306"><a href="#cb25-306"></a>    release(&amp;s-&gt;lock);</span>
<span id="cb25-307"><a href="#cb25-307"></a>  }</span>
<span id="cb25-308"><a href="#cb25-308"></a>  </span>
<span id="cb25-309"><a href="#cb25-309"></a>  void</span>
<span id="cb25-310"><a href="#cb25-310"></a>  P(struct semaphore *s)</span>
<span id="cb25-311"><a href="#cb25-311"></a>  {</span>
<span id="cb25-312"><a href="#cb25-312"></a>    (*@\hl{acquire(\&amp;s-&gt;lock);}@*)</span>
<span id="cb25-313"><a href="#cb25-313"></a>    while(s-&gt;count == 0)    (*@\label{line:test1}@*)</span>
<span id="cb25-314"><a href="#cb25-314"></a>      sleep(s);             (*@\label{line:sleep1}@*)</span>
<span id="cb25-315"><a href="#cb25-315"></a>    s-&gt;count -= 1;</span>
<span id="cb25-316"><a href="#cb25-316"></a>    release(&amp;s-&gt;lock);</span>
<span id="cb25-317"><a href="#cb25-317"></a>  }</span></code></pre></div>
<p>One might hope that this version of <code>P</code> would avoid the lost wakeup because the lock prevents <code>V</code> from executing between lines <a href="#line:test1" data-reference-type="ref" data-reference="line:test1">[line:test1]</a> and <a href="#line:sleep1" data-reference-type="ref" data-reference="line:sleep1">[line:sleep1]</a>. It does that, but it also deadlocks: <code>P</code> holds the lock while it sleeps, so <code>V</code> will block forever waiting for the lock.</p>
<p>We’ll fix the preceding scheme by changing <code>sleep</code>’s interface: the caller must pass the lock to <code>sleep</code> so it can release the lock after the calling process is marked as asleep and waiting on the sleep channel. The lock will force a concurrent <code>V</code> to wait until <code>P</code> has finished putting itself to sleep, so that the <code>wakeup</code> will find the sleeping consumer and wake it up. Once the consumer is awake again <code>sleep</code> reacquires the lock before returning. Our new correct sleep/wakeup scheme is usable as follows (change highlighted in yellow):</p>
<div class="sourceCode" id="cb26" data-numbers="left" data-startFrom="400"><pre class="sourceCode numberSource numberLines"><code class="sourceCode" style="counter-reset: source-line 399;"><span id="cb26-400"><a href="#cb26-400"></a>void</span>
<span id="cb26-401"><a href="#cb26-401"></a>  V(struct semaphore *s)</span>
<span id="cb26-402"><a href="#cb26-402"></a>  {</span>
<span id="cb26-403"><a href="#cb26-403"></a>    acquire(&amp;s-&gt;lock);</span>
<span id="cb26-404"><a href="#cb26-404"></a>    s-&gt;count += 1;</span>
<span id="cb26-405"><a href="#cb26-405"></a>    wakeup(s);</span>
<span id="cb26-406"><a href="#cb26-406"></a>    release(&amp;s-&gt;lock);</span>
<span id="cb26-407"><a href="#cb26-407"></a>  }</span>
<span id="cb26-408"><a href="#cb26-408"></a></span>
<span id="cb26-409"><a href="#cb26-409"></a>  void</span>
<span id="cb26-410"><a href="#cb26-410"></a>  P(struct semaphore *s)</span>
<span id="cb26-411"><a href="#cb26-411"></a>  {</span>
<span id="cb26-412"><a href="#cb26-412"></a>    acquire(&amp;s-&gt;lock);</span>
<span id="cb26-413"><a href="#cb26-413"></a>    while(s-&gt;count == 0)</span>
<span id="cb26-414"><a href="#cb26-414"></a>       (*@\hl{sleep(s, \&amp;s-&gt;lock);}@*)</span>
<span id="cb26-415"><a href="#cb26-415"></a>    s-&gt;count -= 1;</span>
<span id="cb26-416"><a href="#cb26-416"></a>    release(&amp;s-&gt;lock);</span>
<span id="cb26-417"><a href="#cb26-417"></a>  }</span></code></pre></div>
<p>The fact that <code>P</code> holds <code>s-&gt;lock</code> prevents <code>V</code> from trying to wake it up between <code>P</code>’s check of <code>c-&gt;count</code> and its call to <code>sleep</code>. We need <code>sleep</code> to atomically release <code>s-&gt;lock</code> and put the consuming process to sleep.</p>
<h2 id="code-sleep-and-wakeup">Code: Sleep and wakeup</h2>
<p>Let’s look at the implementation of <code>sleep</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L533"><span>(kernel/proc.c:533)</span></a> and <code>wakeup</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L567"><span>(kernel/proc.c:567)</span></a>. The basic idea is to have <code>sleep</code> mark the current process as <code>SLEEPING</code> and then call <code>sched</code> to release the CPU; <code>wakeup</code> looks for a process sleeping on the given wait channel and marks it as <code>RUNNABLE</code>. Callers of <code>sleep</code> and <code>wakeup</code> can use any mutually convenient number as the channel. Xv6 often uses the address of a kernel data structure involved in the waiting.</p>
<p><code>Sleep</code> acquires <code>p-&gt;lock</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L544"><span>(kernel/proc.c:544)</span></a>. Now the process going to sleep holds both <code>p-&gt;lock</code> and <code>lk</code>. Holding <code>lk</code> was necessary in the caller (in the example, <code>P</code>): it ensured that no other process (in the example, one running <code>V</code>) could start a call to <code>wakeup(chan)</code>. Now that <code>sleep</code> holds <code>p-&gt;lock</code>, it is safe to release <code>lk</code>: some other process may start a call to <code>wakeup(chan)</code>, but <code>wakeup</code> will wait to acquire <code>p-&gt;lock</code>, and thus will wait until <code>sleep</code> has finished putting the process to sleep, keeping the <code>wakeup</code> from missing the <code>sleep</code>.</p>
<p>There is a minor complication: if <code>lk</code> is the same lock as <code>p-&gt;lock</code>, then <code>sleep</code> would deadlock with itself if it tried to acquire <code>p-&gt;lock</code>. But if the process calling <code>sleep</code> already holds <code>p-&gt;lock</code>, it doesn’t need to do anything more in order to avoiding missing a concurrent <code>wakeup</code>. This case arises when <code>wait</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L567"><span>(kernel/proc.c:567)</span></a> calls <code>sleep</code> with <code>p-&gt;lock</code>.</p>
<p>Now that <code>sleep</code> holds <code>p-&gt;lock</code> and no others, it can put the process to sleep by recording the sleep channel, changing the process state, and calling <code>sched</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L549-L552">(kernel/proc.c:549-552)</a>.</p>
<p>At some point later, a process will call <code>wakeup(chan)</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L567"><span>(kernel/proc.c:567)</span></a>. <code>Wakeup</code> loops over the process table. It acquires the <code>p-&gt;lock</code> of each process it inspects, both because it may manipulating that process’s state and because, as we just saw, <code>p-&gt;lock</code> ensures that <code>sleep</code> and <code>wakeup</code> do not miss each other. When it finds a process in state <code>SLEEPING</code> with a matching <code>chan</code>, it changes that process’s state to <code>RUNNABLE</code>. The next time the scheduler runs, it will see that the process is ready to be run.</p>
<p>Xv6 code always calls <code>wakeup</code> while holding the “condition lock”; in the semaphore example that lock is <code>s-&gt;lock</code>. Strictly speaking it is sufficient if <code>wakeup</code> always follows the <code>acquire</code> (that is, one could call <code>wakeup</code> after the <code>release</code>). Why do the locking rules for <code>sleep</code> and <code>wakeup</code> ensure a sleeping process won’t miss a wakeup it needs? The sleeping process holds either the condition lock or its own <code>p-&gt;lock</code> or both from a point before it checks the condition to a point after it is marked as sleeping. If a concurrent thread causes the condition to be true, that thread must either hold the condition lock before the sleeping thread acquired it, or after the sleeping thread released it in <code>sleep</code>. If before, the sleeping thread must have seen the new condition value, and decided to sleep anyway, so it doesn’t matter if it misses the wakeup. If after, then the earliest the waker could acquire the condition lock is after <code>sleep</code> acquires <code>p-&gt;lock</code>, so that <code>wakeup</code>’s acquisition of <code>p-&gt;lock</code> must wait until <code>sleep</code> has completely finished putting the sleeper to sleep. Then <code>wakeup</code> will see the sleeping process and wake it up (unless something else wakes it up first).</p>
<p>It is sometimes the case that multiple processes are sleeping on the same channel; for example, more than one process reading from a pipe. A single call to <code>wakeup</code> will wake them all up. One of them will run first and acquire the lock that <code>sleep</code> was called with, and (in the case of pipes) read whatever data is waiting in the pipe. The other processes will find that, despite being woken up, there is no data to be read. From their point of view the wakeup was “spurious,” and they must sleep again. For this reason <code>sleep</code> is always called inside a loop that checks the condition.</p>
<p>No harm is done if two uses of sleep/wakeup accidentally choose the same channel: they will see spurious wakeups, but looping as described above will tolerate this problem. Much of the charm of sleep/wakeup is that it is both lightweight (no need to create special data structures to act as sleep channels) and provides a layer of indirection (callers need not know which specific process they are interacting with).</p>
<h2 id="code-pipes">Code: Pipes</h2>
<p>A more complex example that uses <code>sleep</code> and <code>wakeup</code> to synchronize producers and consumers is xv6’s implementation of pipes. We saw the interface for pipes in Chapter <a href="#CH:UNIX" data-reference-type="ref" data-reference="CH:UNIX">1</a>: bytes written to one end of a pipe are copied to an in-kernel buffer and then can be read from the other end of the pipe. Future chapters will examine the file descriptor support surrounding pipes, but let’s look now at the implementations of <code>pipewrite</code> and <code>piperead</code>.</p>
<p>Each pipe is represented by a <code>struct pipe</code>, which contains a <code>lock</code> and a <code>data</code> buffer. The fields <code>nread</code> and <code>nwrite</code> count the total number of bytes read from and written to the buffer. The buffer wraps around: the next byte written after <code>buf[PIPESIZE-1]</code> is <code>buf[0]</code>. The counts do not wrap. This convention lets the implementation distinguish a full buffer (<code>nwrite</code> <code>==</code> <code>nread+PIPESIZE</code>) from an empty buffer (<code>nwrite</code> <code>==</code> <code>nread</code>), but it means that indexing into the buffer must use <code>buf[nread</code> <code>%</code> <code>PIPESIZE]</code> instead of just <code>buf[nread]</code> (and similarly for <code>nwrite</code>).</p>
<p>Let’s suppose that calls to <code>piperead</code> and <code>pipewrite</code> happen simultaneously on two different CPUs. <code>Pipewrite</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/pipe.c#L77"><span>(kernel/pipe.c:77)</span></a> begins by acquiring the pipe’s lock, which protects the counts, the data, and their associated invariants. <code>Piperead</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/pipe.c#L103"><span>(kernel/pipe.c:103)</span></a> then tries to acquire the lock too, but cannot. It spins in <code>acquire</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/spinlock.c#L22"><span>(kernel/spinlock.c:22)</span></a> waiting for the lock. While <code>piperead</code> waits, <code>pipewrite</code> loops over the bytes being written (<code>addr[0..n-1]</code>), adding each to the pipe in turn <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/pipe.c#L95"><span>(kernel/pipe.c:95)</span></a>. During this loop, it could happen that the buffer fills <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/pipe.c#L85"><span>(kernel/pipe.c:85)</span></a>. In this case, <code>pipewrite</code> calls <code>wakeup</code> to alert any sleeping readers to the fact that there is data waiting in the buffer and then sleeps on <code>&amp;pi-&gt;nwrite</code> to wait for a reader to take some bytes out of the buffer. <code>Sleep</code> releases <code>pi-&gt;lock</code> as part of putting <code>pipewrite</code>’s process to sleep.</p>
<p>Now that <code>pi-&gt;lock</code> is available, <code>piperead</code> manages to acquire it and enters its critical section: it finds that <code>pi-&gt;nread</code> <code>!=</code> <code>pi-&gt;nwrite</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/pipe.c#L110"><span>(kernel/pipe.c:110)</span></a> (<code>pipewrite</code> went to sleep because <code>pi-&gt;nwrite</code> <code>==</code> <code>pi-&gt;nread+PIPESIZE</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/pipe.c#L85"><span>(kernel/pipe.c:85)</span></a>), so it falls through to the <code>for</code> loop, copies data out of the pipe <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/pipe.c#L117"><span>(kernel/pipe.c:117)</span></a>, and increments <code>nread</code> by the number of bytes copied. That many bytes are now available for writing, so <code>piperead</code> calls <code>wakeup</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/pipe.c#L124"><span>(kernel/pipe.c:124)</span></a> to wake any sleeping writers before it returns. <code>Wakeup</code> finds a process sleeping on <code>&amp;pi-&gt;nwrite</code>, the process that was running <code>pipewrite</code> but stopped when the buffer filled. It marks that process as <code>RUNNABLE</code>.</p>
<p>The pipe code uses separate sleep channels for reader and writer (<code>pi-&gt;nread</code> and <code>pi-&gt;nwrite</code>); this might make the system more efficient in the unlikely event that there are lots of readers and writers waiting for the same pipe. The pipe code sleeps inside a loop checking the sleep condition; if there are multiple readers or writers, all but the first process to wake up will see the condition is still false and sleep again.</p>
<h2 id="code-wait-exit-and-kill">Code: Wait, exit, and kill</h2>
<p><code>Sleep</code> and <code>wakeup</code> can be used for many kinds of waiting. An interesting example, introduced in Chapter <a href="#CH:UNIX" data-reference-type="ref" data-reference="CH:UNIX">1</a>, is the interaction between a child’s <code>exit</code> and its parent’s <code>wait</code>. At the time of the child’s death, the parent may already be sleeping in <span><code>wait</code></span>, or may be doing something else; in the latter case, a subsequent call to <span><code>wait</code></span> must observe the child’s death, perhaps long after it calls <span><code>exit</code></span>. The way that xv6 records the child’s demise until <span><code>wait</code></span> observes it is for <span><code>exit</code></span> to put the caller into the <span><code>ZOMBIE</code></span> state, where it stays until the parent’s <span><code>wait</code></span> notices it, changes the child’s state to <span><code>UNUSED</code></span>, copies the child’s exit status, and returns the child’s process ID to the parent. If the parent exits before the child, the parent gives the child to the <code>init</code> process, which perpetually calls <span><code>wait</code></span>; thus every child has a parent to clean up after it. The main implementation challenge is the possibility of races and deadlock between parent and child <code>wait</code> and <code>exit</code>, as well as <code>exit</code> and <code>exit</code>.</p>
<p><code>Wait</code> uses the calling process’s <code>p-&gt;lock</code> as the condition lock to avoid lost wakeups, and it acquires that lock at the start <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L383"><span>(kernel/proc.c:383)</span></a>. Then it scans the process table. If it finds a child in <code>ZOMBIE</code> state, it frees that child’s resources and its <code>proc</code> structure, copies the child’s exit status to the address supplied to <code>wait</code> (if it is not 0), and returns the child’s process ID. If <code>wait</code> finds children but none have exited, it calls <code>sleep</code> to wait for one of them to exit <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L430"><span>(kernel/proc.c:430)</span></a>, then scans again. Here, the condition lock being released in <code>sleep</code> is <code>p-&gt;lock</code>, the special case mentioned above. Note that <code>wait</code> often holds two locks, and that the deadlock-avoiding order it uses is parent, then child.</p>
<p><code>Wait</code> uses <code>np-&gt;parent</code> without holding <code>np-&gt;lock</code>, which is a violation of the usual rule that shared variables must be protected by locks. It is possible that <code>np</code> is an ancestor of the current process, in which case acquiring <code>np-&gt;lock</code> could cause a deadlock since that would violate the order mentioned above. Examining <code>np-&gt;parent</code> without a lock seems safe in this case; a process’s <code>parent</code> field is only changed by its parent, so if <code>np-&gt;parent==p</code> is true, the value can’t change unless the current process changes it.</p>
<p><code>Exit</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L318"><span>(kernel/proc.c:318)</span></a> records the exit status, frees some resources, gives any children to the <code>init</code> process, wakes up the parent in case it is in <code>wait</code>, marks the caller as a zombie, and permanently yields the CPU. The final sequence is a little tricky. The exiting process must hold its parent’s lock while it sets its state to <code>ZOMBIE</code> and wakes the parent up, since the parent’s lock is the condition lock that guards against lost wakeups in <code>wait</code>. The child must also hold its own <code>p-&gt;lock</code>, since otherwise the parent might see it in state <code>ZOMBIE</code> and free it while it is still running. The lock acquisition order is important to avoid deadlock: since <code>wait</code> acquires the parent’s lock before the child’s lock, <code>exit</code> must use the same order.</p>
<p><code>Exit</code> calls a specialized wakeup function, <code>wakeup1</code>, that wakes up only the parent, and only if it is sleeping in <code>wait</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L583"><span>(kernel/proc.c:583)</span></a>. It may look incorrect for the child to wake up the parent before setting its state to <code>ZOMBIE</code>, but that is safe: although <code>wakeup1</code> may cause the parent to run, the loop in <code>wait</code> cannot examine the child until the child’s <code>p-&gt;lock</code> is released by <span><code>scheduler</code></span>, so <code>wait</code> can’t look at the exiting process until well after <code>exit</code> has set its state to <code>ZOMBIE</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L371"><span>(kernel/proc.c:371)</span></a>.</p>
<p>While <code>exit</code> allows a process to terminate itself, <code>kill</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L596"><span>(kernel/proc.c:596)</span></a> lets one process request that another terminate. It would be too complex for <code>kill</code> to directly destroy the victim process, since the victim might be executing on another CPU, perhaps in the middle of a sensitive sequence of updates to kernel data structures. To address these challenges, <code>kill</code> does very little: it just sets the victim’s <code>p-&gt;killed</code> and, if it is sleeping, wakes it up. Eventually the victim will enter or leave the kernel, at which point code in <code>usertrap</code> will call <code>exit</code> if <code>p-&gt;killed</code> is set. If the victim is running in user space, it will soon enter the kernel by making a system call or because the timer (or some other device) interrupts.</p>
<p>If the victim process is in <code>sleep</code>, <code>kill</code>’s call to <code>wakeup</code> will cause the victim to return from <code>sleep</code>. This is potentially dangerous because the condition being waiting for may not be true. However, xv6 calls to <code>sleep</code> are always wrapped in a <code>while</code> loop that re-tests the condition after <code>sleep</code> returns. Some calls to <code>sleep</code> also test <code>p-&gt;killed</code> in the loop, and abandon the current activity if it is set. This is only done when such abandonment would be correct. For example, the pipe read and write code <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/pipe.c#L86"><span>(kernel/pipe.c:86)</span></a> returns if the killed flag is set; eventually the code will return back to trap, which will again check the flag and exit.</p>
<p>Some xv6 <code>sleep</code> loops do not check <code>p-&gt;killed</code> because the code is in the middle of a multi-step system call that should be atomic. The virtio driver <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/virtio_disk.c#L242"><span>(kernel/virtio_disk.c:242)</span></a> is an example: it does not check <code>p-&gt;killed</code> because a disk operation may be one of a set of writes that are all needed in order for the file system to be left in a correct state. A process that is killed while waiting for disk I/O won’t exit until it completes the current system call and <code>usertrap</code> sees the killed flag.</p>
<h2 id="real-world-5">Real world</h2>
<p>The xv6 scheduler implements a simple scheduling policy, which runs each process in turn. This policy is called <em>round robin</em>. Real operating systems implement more sophisticated policies that, for example, allow processes to have priorities. The idea is that a runnable high-priority process will be preferred by the scheduler over a runnable low-priority process. These policies can become complex quickly because there are often competing goals: for example, the operating might also want to guarantee fairness and high throughput. In addition, complex policies may lead to unintended interactions such as <em>priority inversion</em> and <em>convoys</em>. Priority inversion can happen when a low-priority and high-priority process share a lock, which when acquired by the low-priority process can prevent the high-priority process from making progress. A long convoy of waiting processes can form when many high-priority processes are waiting for a low-priority process that acquires a shared lock; once a convoy has formed it can persist for long time. To avoid these kinds of problems additional mechanisms are necessary in sophisticated schedulers.</p>
<p><code>Sleep</code> and <code>wakeup</code> are a simple and effective synchronization method, but there are many others. The first challenge in all of them is to avoid the “lost wakeups” problem we saw at the beginning of the chapter. The original Unix kernel’s <code>sleep</code> simply disabled interrupts, which sufficed because Unix ran on a single-CPU system. Because xv6 runs on multiprocessors, it adds an explicit lock to <code>sleep</code>. FreeBSD’s <code>msleep</code> takes the same approach. Plan 9’s <code>sleep</code> uses a callback function that runs with the scheduling lock held just before going to sleep; the function serves as a last-minute check of the sleep condition, to avoid lost wakeups. The Linux kernel’s <code>sleep</code> uses an explicit process queue, called a wait queue, instead of a wait channel; the queue has its own internal lock.</p>
<p>Scanning the entire process list in <code>wakeup</code> for processes with a matching <code>chan</code> is inefficient. A better solution is to replace the <code>chan</code> in both <code>sleep</code> and <code>wakeup</code> with a data structure that holds a list of processes sleeping on that structure, such as Linux’s wait queue. Plan 9’s <code>sleep</code> and <code>wakeup</code> call that structure a rendezvous point or <code>Rendez</code>. Many thread libraries refer to the same structure as a condition variable; in that context, the operations <code>sleep</code> and <code>wakeup</code> are called <code>wait</code> and <code>signal</code>. All of these mechanisms share the same flavor: the sleep condition is protected by some kind of lock dropped atomically during sleep.</p>
<p>The implementation of <code>wakeup</code> wakes up all processes that are waiting on a particular channel, and it might be the case that many processes are waiting for that particular channel. The operating system will schedule all these processes and they will race to check the sleep condition. Processes that behave in this way are sometimes called a <em>thundering herd</em>, and it is best avoided. Most condition variables have two primitives for <code>wakeup</code>: <code>signal</code>, which wakes up one process, and <code>broadcast</code>, which wakes up all waiting processes.</p>
<p>Semaphores are often used for synchronization. The count typically corresponds to something like the number of bytes available in a pipe buffer or the number of zombie children that a process has. Using an explicit count as part of the abstraction avoids the “lost wakeup” problem: there is an explicit count of the number of wakeups that have occurred. The count also avoids the spurious wakeup and thundering herd problems.</p>
<p>Terminating processes and cleaning them up introduces much complexity in xv6. In most operating systems it is even more complex, because, for example, the victim process may be deep inside the kernel sleeping, and unwinding its stack requires much careful programming. Many operating systems unwind the stack using explicit mechanisms for exception handling, such as <code>longjmp</code>. Furthermore, there are other events that can cause a sleeping process to be woken up, even though the event it is waiting for has not happened yet. For example, when a Unix process is sleeping, another process may send a <code>signal</code> to it. In this case, the process will return from the interrupted system call with the value -1 and with the error code set to EINTR. The application can check for these values and decide what to do. Xv6 doesn’t support signals and this complexity doesn’t arise.</p>
<p>Xv6’s support for <code>kill</code> is not entirely satisfactory: there are sleep loops which probably should check for <code>p-&gt;killed</code>. A related problem is that, even for <code>sleep</code> loops that check <code>p-&gt;killed</code>, there is a race between <code>sleep</code> and <code>kill</code>; the latter may set <code>p-&gt;killed</code> and try to wake up the victim just after the victim’s loop checks <code>p-&gt;killed</code> but before it calls <code>sleep</code>. If this problem occurs, the victim won’t notice the <code>p-&gt;killed</code> until the condition it is waiting for occurs. This may be quite a bit later (e.g., when the virtio driver returns a disk block that the victim is waiting for) or never (e.g., if the victim is waiting from input from the console, but the user doesn’t type any input).</p>
<p>A real operating system would find free <code>proc</code> structures with an explicit free list in constant time instead of the linear-time search in <code>allocproc</code>; xv6 uses the linear scan for simplicity.</p>
<h2 id="exercises-5">Exercises</h2>
<ol>
<li><p>Sleep has to check <code>lk != &amp;p-&gt;lock</code> to avoid a deadlock <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L543-L546">(kernel/proc.c:543-546)</a>. Suppose the special case were eliminated by replacing</p>
<pre><code>if(lk != &amp;p-&gt;lock){
  acquire(&amp;p-&gt;lock);
  release(lk);
}</code></pre>
<p>with</p>
<pre><code>release(lk);
acquire(&amp;p-&gt;lock);</code></pre>
<p>Doing this would break <code>sleep</code>. How?</p></li>
<li><p>Most process cleanup could be done by either <code>exit</code> or <code>wait</code>, but we saw above that <code>exit</code> must not free <code>p-&gt;kstack</code>. It turns out that <code>exit</code> must be the one to close the open files. Why? The answer involves pipes.</p></li>
<li><p>Implement semaphores in xv6 without using <code>sleep</code> and <code>wakeup</code> (but it is OK to use spin locks). Replace the uses of sleep and wakeup in xv6 with semaphores. Judge the result.</p></li>
<li><p>Fix the race mentioned above between <code>kill</code> and <code>sleep</code>, so that a <code>kill</code> that occurs after the victim’s sleep loop checks <code>p-&gt;killed</code> but before it calls <code>sleep</code> results in the victim abandoning the current system call.</p></li>
<li><p>Design a plan so that every sleep loop checks <code>p-&gt;killed</code> so that, for example, a process that is in the virtio driver can return quickly from the while loop if it is killed by another process.</p></li>
<li><p>Design a plan that uses only one context switch when switching from one user process to another. This plan involves running the scheduler procedure on the kernel stack of the user process, instead of the dedicated scheduler stack. The main challenge is to clean up a user process correctly. Measure the performance benefit of avoiding one context switch.</p></li>
<li><p>Modify xv6’s <code>scheduler</code> to use the RISC-V <code>WFI</code> (wait for interrupt) instruction when no processes are runnable. Try to ensure that, any time there are runnable processes waiting to run, no cores are pausing in <code>WFI</code>.</p></li>
<li><p>The lock <code>p-&gt;lock</code> protects many invariants, and when looking at a particular piece of xv6 code that is protected by <code>p-&gt;lock</code>, it can be difficult to figure out which invariant is being enforced. Design a plan that is more clean by splitting <code>p-&gt;lock</code> into several locks.</p></li>
</ol>
<h1 id="CH:FS">File system</h1>
<p>The purpose of a file system is to organize and store data. File systems typically support sharing of data among users and applications, as well as <em>persistence</em> so that data is still available after a reboot.</p>
<p>The xv6 file system provides Unix-like files, directories, and pathnames (see Chapter <a href="#CH:UNIX" data-reference-type="ref" data-reference="CH:UNIX">1</a>), and stores its data on a virtio disk for persistence (see Chapter <a href="#CH:TRAP" data-reference-type="ref" data-reference="CH:TRAP">4</a>). The file system addresses several challenges:</p>
<ul>
<li><p>The file system needs on-disk data structures to represent the tree of named directories and files, to record the identities of the blocks that hold each file’s content, and to record which areas of the disk are free.</p></li>
<li><p>The file system must support <em>crash recovery</em>. That is, if a crash (e.g., power failure) occurs, the file system must still work correctly after a restart. The risk is that a crash might interrupt a sequence of updates and leave inconsistent on-disk data structures (e.g., a block that is both used in a file and marked free).</p></li>
<li><p>Different processes may operate on the file system at the same time, so the file-system code must coordinate to maintain invariants.</p></li>
<li><p>Accessing a disk is orders of magnitude slower than accessing memory, so the file system must maintain an in-memory cache of popular blocks.</p></li>
</ul>
<p>The rest of this chapter explains how xv6 addresses these challenges.</p>
<h2 id="overview">Overview</h2>
<p>The xv6 file system implementation is organized in seven layers, shown in Figure <a href="#fig:fslayer" data-reference-type="ref" data-reference="fig:fslayer">7.1</a>. The disk layer reads and writes blocks on an virtio hard drive. The buffer cache layer caches disk blocks and synchronizes access to them, making sure that only one kernel process at a time can modify the data stored in any particular block. The logging layer allows higher layers to wrap updates to several blocks in a <em>transaction</em>, and ensures that the blocks are updated atomically in the face of crashes (i.e., all of them are updated or none). The inode layer provides individual files, each represented as an <em>inode</em> with a unique i-number and some blocks holding the file’s data. The directory layer implements each directory as a special kind of inode whose content is a sequence of directory entries, each of which contains a file’s name and i-number. The pathname layer provides hierarchical path names like <code>/usr/rtm/xv6/fs.c</code>, and resolves them with recursive lookup. The file descriptor layer abstracts many Unix resources (e.g., pipes, devices, files, etc.) using the file system interface, simplifying the lives of application programmers.</p>
<figure>
<embed src="fig/fslayer.svg" id="fig:fslayer" /><figcaption>Layers of the xv6 file system.</figcaption>
</figure>
<p>The file system must have a plan for where it stores inodes and content blocks on the disk. To do so, xv6 divides the disk into several sections, as shown in Figure <a href="#fig:fslayout" data-reference-type="ref" data-reference="fig:fslayout">7.2</a>. The file system does not use block 0 (it holds the boot sector). Block 1 is called the <em>superblock</em>; it contains metadata about the file system (the file system size in blocks, the number of data blocks, the number of inodes, and the number of blocks in the log). Blocks starting at 2 hold the log. After the log are the inodes, with multiple inodes per block. After those come bitmap blocks tracking which data blocks are in use. The remaining blocks are data blocks; each is either marked free in the bitmap block, or holds content for a file or directory. The superblock is filled in by a separate program, called <code>mkfs</code>, which builds an initial file system.</p>
<p>The rest of this chapter discusses each layer, starting with the buffer cache. Look out for situations where well-chosen abstractions at lower layers ease the design of higher ones.</p>
<h2 id="buffer-cache-layer">Buffer cache layer</h2>
<p>The buffer cache has two jobs: (1) synchronize access to disk blocks to ensure that only one copy of a block is in memory and that only one kernel thread at a time uses that copy; (2) cache popular blocks so that they don’t need to be re-read from the slow disk. The code is in <code>bio.c</code>.</p>
<p>The main interface exported by the buffer cache consists of <code>bread</code> and <code>bwrite</code>; the former obtains a <em>buf</em> containing a copy of a block which can be read or modified in memory, and the latter writes a modified buffer to the appropriate block on the disk. A kernel thread must release a buffer by calling <code>brelse</code> when it is done with it. The buffer cache uses a per-buffer sleep-lock to ensure that only one thread at a time uses each buffer (and thus each disk block); <code>bread</code> returns a locked buffer, and <code>brelse</code> releases the lock.</p>
<p>Let’s return to the buffer cache. The buffer cache has a fixed number of buffers to hold disk blocks, which means that if the file system asks for a block that is not already in the cache, the buffer cache must recycle a buffer currently holding some other block. The buffer cache recycles the least recently used buffer for the new block. The assumption is that the least recently used buffer is the one least likely to be used again soon.</p>
<figure>
<embed src="fig/fslayout.svg" id="fig:fslayout" /><figcaption>Structure of the xv6 file system. </figcaption>
</figure>
<h2 id="code-buffer-cache">Code: Buffer cache</h2>
<p>The buffer cache is a doubly-linked list of buffers. The function <code>binit</code>, called by <code>main</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/main.c#L27"><span>(kernel/main.c:27)</span></a>, initializes the list with the <code>NBUF</code> buffers in the static array <code>buf</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/bio.c#L42-L51">(kernel/bio.c:42-51)</a>. All other access to the buffer cache refer to the linked list via <code>bcache.head</code>, not the <code>buf</code> array.</p>
<p>A buffer has two state fields associated with it. The field <code>valid</code> indicates that the buffer contains a copy of the block. The field <code>disk</code> indicates that the buffer content has been handed to the disk, which may change the buffer (e.g., write data from the disk into <code>data</code>).</p>
<p><code>Bread</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/bio.c#L91"><span>(kernel/bio.c:91)</span></a> calls <code>bget</code> to get a buffer for the given sector <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/bio.c#L95"><span>(kernel/bio.c:95)</span></a>. If the buffer needs to be read from disk, <code>bread</code> calls <code>virtio_disk_rw</code> to do that before returning the buffer.</p>
<p><code>Bget</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/bio.c#L58"><span>(kernel/bio.c:58)</span></a> scans the buffer list for a buffer with the given device and sector numbers <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/bio.c#L64-L72">(kernel/bio.c:64-72)</a>. If there is such a buffer, <code>bget</code> acquires the sleep-lock for the buffer. <code>Bget</code> then returns the locked buffer.</p>
<p>If there is no cached buffer for the given sector, <code>bget</code> must make one, possibly reusing a buffer that held a different sector. It scans the buffer list a second time, looking for a buffer that is not in use (<code>b-&gt;refcnt = 0</code>); any such buffer can be used. <code>Bget</code> edits the buffer metadata to record the new device and sector number and acquires its sleep-lock. Note that the assignment <code>b-&gt;valid = 0</code> ensures that <code>bread</code> will read the block data from disk rather than incorrectly using the buffer’s previous contents.</p>
<p>It is important that there is at most one cached buffer per disk sector, to ensure that readers see writes, and because the file system uses locks on buffers for synchronization. <code>Bget</code> ensures this invariant by holding the <code>bache.lock</code> continuously from the first loop’s check of whether the block is cached through the second loop’s declaration that the block is now cached (by setting <code>dev</code>, <code>blockno</code>, and <code>refcnt</code>). This causes the check for a block’s presence and (if not present) the designation of a buffer to hold the block to be atomic.</p>
<p>It is safe for <code>bget</code> to acquire the buffer’s sleep-lock outside of the <code>bcache.lock</code> critical section, since the non-zero <code>b-&gt;refcnt</code> prevents the buffer from being re-used for a different disk block. The sleep-lock protects reads and writes of the block’s buffered content, while the <code>bcache.lock</code> protects information about which blocks are cached.</p>
<p>If all the buffers are busy, then too many processes are simultaneously executing file system calls; <code>bget</code> panics. A more graceful response might be to sleep until a buffer became free, though there would then be a possibility of deadlock.</p>
<p>Once <code>bread</code> has read the disk (if needed) and returned the buffer to its caller, the caller has exclusive use of the buffer and can read or write the data bytes. If the caller does modify the buffer, it must call <code>bwrite</code> to write the changed data to disk before releasing the buffer. <code>Bwrite</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/bio.c#L105"><span>(kernel/bio.c:105)</span></a> calls <code>virtio_disk_rw</code> to talk to the disk hardware.</p>
<p>When the caller is done with a buffer, it must call <code>brelse</code> to release it. (The name <code>brelse</code>, a shortening of b-release, is cryptic but worth learning: it originated in Unix and is used in BSD, Linux, and Solaris too.) <code>Brelse</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/bio.c#L115"><span>(kernel/bio.c:115)</span></a> releases the sleep-lock and moves the buffer to the front of the linked list <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/bio.c#L126-L131">(kernel/bio.c:126-131)</a>. Moving the buffer causes the list to be ordered by how recently the buffers were used (meaning released): the first buffer in the list is the most recently used, and the last is the least recently used. The two loops in <code>bget</code> take advantage of this: the scan for an existing buffer must process the entire list in the worst case, but checking the most recently used buffers first (starting at <code>bcache.head</code> and following <code>next</code> pointers) will reduce scan time when there is good locality of reference. The scan to pick a buffer to reuse picks the least recently used buffer by scanning backward (following <code>prev</code> pointers).</p>
<h2 id="logging-layer">Logging layer</h2>
<p>One of the most interesting problems in file system design is crash recovery. The problem arises because many file-system operations involve multiple writes to the disk, and a crash after a subset of the writes may leave the on-disk file system in an inconsistent state. For example, suppose a crash occurs during file truncation (setting the length of a file to zero and freeing its content blocks). Depending on the order of the disk writes, the crash may either leave an inode with a reference to a content block that is marked free, or it may leave an allocated but unreferenced content block.</p>
<p>The latter is relatively benign, but an inode that refers to a freed block is likely to cause serious problems after a reboot. After reboot, the kernel might allocate that block to another file, and now we have two different files pointing unintentionally to the same block. If xv6 supported multiple users, this situation could be a security problem, since the old file’s owner would be able to read and write blocks in the new file, owned by a different user.</p>
<p>Xv6 solves the problem of crashes during file-system operations with a simple form of logging. An xv6 system call does not directly write the on-disk file system data structures. Instead, it places a description of all the disk writes it wishes to make in a <em>log</em> on the disk. Once the system call has logged all of its writes, it writes a special <em>commit</em> record to the disk indicating that the log contains a complete operation. At that point the system call copies the writes to the on-disk file system data structures. After those writes have completed, the system call erases the log on disk.</p>
<p>If the system should crash and reboot, the file-system code recovers from the crash as follows, before running any processes. If the log is marked as containing a complete operation, then the recovery code copies the writes to where they belong in the on-disk file system. If the log is not marked as containing a complete operation, the recovery code ignores the log. The recovery code finishes by erasing the log.</p>
<p>Why does xv6’s log solve the problem of crashes during file system operations? If the crash occurs before the operation commits, then the log on disk will not be marked as complete, the recovery code will ignore it, and the state of the disk will be as if the operation had not even started. If the crash occurs after the operation commits, then recovery will replay all of the operation’s writes, perhaps repeating them if the operation had started to write them to the on-disk data structure. In either case, the log makes operations atomic with respect to crashes: after recovery, either all of the operation’s writes appear on the disk, or none of them appear.</p>
<h2 id="log-design">Log design</h2>
<p>The log resides at a known fixed location, specified in the superblock. It consists of a header block followed by a sequence of updated block copies (“logged blocks”). The header block contains an array of sector numbers, one for each of the logged blocks, and the count of log blocks. The count in the header block on disk is either zero, indicating that there is no transaction in the log, or non-zero, indicating that the log contains a complete committed transaction with the indicated number of logged blocks. Xv6 writes the header block when a transaction commits, but not before, and sets the count to zero after copying the logged blocks to the file system. Thus a crash midway through a transaction will result in a count of zero in the log’s header block; a crash after a commit will result in a non-zero count.</p>
<p>Each system call’s code indicates the start and end of the sequence of writes that must be atomic with respect to crashes. To allow concurrent execution of file-system operations by different processes, the logging system can accumulate the writes of multiple system calls into one transaction. Thus a single commit may involve the writes of multiple complete system calls. To avoid splitting a system call across transactions, the logging system only commits when no file-system system calls are underway.</p>
<p>The idea of committing several transactions together is known as <em>group commit</em>. Group commit reduces the number of disk operations because it amortizes the fixed cost of a commit over multiple operations. Group commit also hands the disk system more concurrent writes at the same time, perhaps allowing the disk to write them all during a single disk rotation. Xv6’s virtio driver doesn’t support this kind of <em>batching</em>, but xv6’s file system design allows for it.</p>
<p>Xv6 dedicates a fixed amount of space on the disk to hold the log. The total number of blocks written by the system calls in a transaction must fit in that space. This has two consequences. No single system call can be allowed to write more distinct blocks than there is space in the log. This is not a problem for most system calls, but two of them can potentially write many blocks: <code>write</code> and <code>unlink</code>. A large file write may write many data blocks and many bitmap blocks as well as an inode block; unlinking a large file might write many bitmap blocks and an inode. Xv6’s write system call breaks up large writes into multiple smaller writes that fit in the log, and <code>unlink</code> doesn’t cause problems because in practice the xv6 file system uses only one bitmap block. The other consequence of limited log space is that the logging system cannot allow a system call to start unless it is certain that the system call’s writes will fit in the space remaining in the log.</p>
<h2 id="code-logging">Code: logging</h2>
<p>A typical use of the log in a system call looks like this:</p>
<pre><code>begin_op();
  ...
  bp = bread(...);
  bp-&gt;data[...] = ...;
  log_write(bp);
  ...
  end_op();</code></pre>
<p><code>begin_op</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/log.c#L126"><span>(kernel/log.c:126)</span></a> waits until the logging system is not currently committing, and until there is enough unreserved log space to hold the writes from this call. <code>log.outstanding</code> counts the number of system calls that have reserved log space; the total reserved space is <code>log.outstanding</code> times <code>MAXOPBLOCKS</code>. Incrementing <code>log.outstanding</code> both reserves space and prevents a commit from occuring during this system call. The code conservatively assumes that each system call might write up to <code>MAXOPBLOCKS</code> distinct blocks.</p>
<p><code>log_write</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/log.c#L214"><span>(kernel/log.c:214)</span></a> acts as a proxy for <code>bwrite</code>. It records the block’s sector number in memory, reserving it a slot in the log on disk, and pins the buffer in the block cache to prevent the block cache from evicting it. The block must stay in the cache until committed: until then, the cached copy is the only record of the modification; it cannot be written to its place on disk until after commit; and other reads in the same transaction must see the modifications. <code>log_write</code> notices when a block is written multiple times during a single transaction, and allocates that block the same slot in the log. This optimization is often called <em>absorption</em>. It is common that, for example, the disk block containing inodes of several files is written several times within a transaction. By absorbing several disk writes into one, the file system can save log space and can achieve better performance because only one copy of the disk block must be written to disk.</p>
<p><code>end_op</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/log.c#L146"><span>(kernel/log.c:146)</span></a> first decrements the count of outstanding system calls. If the count is now zero, it commits the current transaction by calling <code>commit().</code> There are four stages in this process. <code>write_log()</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/log.c#L178"><span>(kernel/log.c:178)</span></a> copies each block modified in the transaction from the buffer cache to its slot in the log on disk. <code>write_head()</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/log.c#L102"><span>(kernel/log.c:102)</span></a> writes the header block to disk: this is the commit point, and a crash after the write will result in recovery replaying the transaction’s writes from the log. <code>install_trans</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/log.c#L69"><span>(kernel/log.c:69)</span></a> reads each block from the log and writes it to the proper place in the file system. Finally <code>end_op</code> writes the log header with a count of zero; this has to happen before the next transaction starts writing logged blocks, so that a crash doesn’t result in recovery using one transaction’s header with the subsequent transaction’s logged blocks.</p>
<p><code>recover_from_log</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/log.c#L116"><span>(kernel/log.c:116)</span></a> is called from <code>initlog</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/log.c#L55"><span>(kernel/log.c:55)</span></a>, which is called from <code>fsinit</code><a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L43"><span>(kernel/fs.c:43)</span></a> during boot before the first user process runs <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L524"><span>(kernel/proc.c:524)</span></a>. It reads the log header, and mimics the actions of <code>end_op</code> if the header indicates that the log contains a committed transaction.</p>
<p>An example use of the log occurs in <code>filewrite</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/file.c#L135"><span>(kernel/file.c:135)</span></a>. The transaction looks like this:</p>
<pre><code>begin_op();
      ilock(f-&gt;ip);
      r = writei(f-&gt;ip, ...);
      iunlock(f-&gt;ip);
      end_op();</code></pre>
<p>This code is wrapped in a loop that breaks up large writes into individual transactions of just a few sectors at a time, to avoid overflowing the log. The call to <code>writei</code> writes many blocks as part of this transaction: the file’s inode, one or more bitmap blocks, and some data blocks.</p>
<h2 id="code-block-allocator">Code: Block allocator</h2>
<p>File and directory content is stored in disk blocks, which must be allocated from a free pool. xv6’s block allocator maintains a free bitmap on disk, with one bit per block. A zero bit indicates that the corresponding block is free; a one bit indicates that it is in use. The program <code>mkfs</code> sets the bits corresponding to the boot sector, superblock, log blocks, inode blocks, and bitmap blocks.</p>
<p>The block allocator provides two functions: <code>balloc</code> allocates a new disk block, and <code>bfree</code> frees a block. <code>Balloc</code> The loop in <code>balloc</code> at <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L72"><span>(kernel/fs.c:72)</span></a> considers every block, starting at block 0 up to <code>sb.size</code>, the number of blocks in the file system. It looks for a block whose bitmap bit is zero, indicating that it is free. If <code>balloc</code> finds such a block, it updates the bitmap and returns the block. For efficiency, the loop is split into two pieces. The outer loop reads each block of bitmap bits. The inner loop checks all <code>BPB</code> bits in a single bitmap block. The race that might occur if two processes try to allocate a block at the same time is prevented by the fact that the buffer cache only lets one process use any one bitmap block at a time.</p>
<p><code>Bfree</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L91"><span>(kernel/fs.c:91)</span></a> finds the right bitmap block and clears the right bit. Again the exclusive use implied by <code>bread</code> and <code>brelse</code> avoids the need for explicit locking.</p>
<p>As with much of the code described in the remainder of this chapter, <code>balloc</code> and <code>bfree</code> must be called inside a transaction.</p>
<h2 id="inode-layer">Inode layer</h2>
<p>The term <em>inode</em> can have one of two related meanings. It might refer to the on-disk data structure containing a file’s size and list of data block numbers. Or “inode” might refer to an in-memory inode, which contains a copy of the on-disk inode as well as extra information needed within the kernel.</p>
<p>The on-disk inodes are packed into a contiguous area of disk called the inode blocks. Every inode is the same size, so it is easy, given a number n, to find the nth inode on the disk. In fact, this number n, called the inode number or i-number, is how inodes are identified in the implementation.</p>
<p>The on-disk inode is defined by a <code>struct dinode</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.h#L32"><span>(kernel/fs.h:32)</span></a>. The <code>type</code> field distinguishes between files, directories, and special files (devices). A type of zero indicates that an on-disk inode is free. The <code>nlink</code> field counts the number of directory entries that refer to this inode, in order to recognize when the on-disk inode and its data blocks should be freed. The <code>size</code> field records the number of bytes of content in the file. The <code>addrs</code> array records the block numbers of the disk blocks holding the file’s content.</p>
<p>The kernel keeps the set of active inodes in memory; <code>struct inode</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/file.h#L17"><span>(kernel/file.h:17)</span></a> is the in-memory copy of a <code>struct</code> <code>dinode</code> on disk. The kernel stores an inode in memory only if there are C pointers referring to that inode. The <code>ref</code> field counts the number of C pointers referring to the in-memory inode, and the kernel discards the inode from memory if the reference count drops to zero. The <code>iget</code> and <code>iput</code> functions acquire and release pointers to an inode, modifying the reference count. Pointers to an inode can come from file descriptors, current working directories, and transient kernel code such as <code>exec</code>.</p>
<p>There are four lock or lock-like mechanisms in xv6’s inode code. <code>icache.lock</code> protects the invariant that an inode is present in the cache at most once, and the invariant that a cached inode’s <code>ref</code> field counts the number of in-memory pointers to the cached inode. Each in-memory inode has a <code>lock</code> field containing a sleep-lock, which ensures exclusive access to the inode’s fields (such as file length) as well as to the inode’s file or directory content blocks. An inode’s <code>ref</code>, if it is greater than zero, causes the system to maintain the inode in the cache, and not re-use the cache entry for a different inode. Finally, each inode contains a <code>nlink</code> field (on disk and copied in memory if it is cached) that counts the number of directory entries that refer to a file; xv6 won’t free an inode if its link count is greater than zero.</p>
<p>A <code>struct</code> <code>inode</code> pointer returned by <code>iget()</code> is guaranteed to be valid until the corresponding call to <code>iput()</code>; the inode won’t be deleted, and the memory referred to by the pointer won’t be re-used for a different inode. <code>iget()</code> provides non-exclusive access to an inode, so that there can be many pointers to the same inode. Many parts of the file-system code depend on this behavior of <code>iget()</code>, both to hold long-term references to inodes (as open files and current directories) and to prevent races while avoiding deadlock in code that manipulates multiple inodes (such as pathname lookup).</p>
<p>The <code>struct</code> <code>inode</code> that <code>iget</code> returns may not have any useful content. In order to ensure it holds a copy of the on-disk inode, code must call <code>ilock</code>. This locks the inode (so that no other process can <code>ilock</code> it) and reads the inode from the disk, if it has not already been read. <code>iunlock</code> releases the lock on the inode. Separating acquisition of inode pointers from locking helps avoid deadlock in some situations, for example during directory lookup. Multiple processes can hold a C pointer to an inode returned by <code>iget</code>, but only one process can lock the inode at a time.</p>
<p>The inode cache only caches inodes to which kernel code or data structures hold C pointers. Its main job is really synchronizing access by multiple processes; caching is secondary. If an inode is used frequently, the buffer cache will probably keep it in memory if it isn’t kept by the inode cache. The inode cache is <em>write-through</em>, which means that code that modifies a cached inode must immediately write it to disk with <code>iupdate</code>.</p>
<h2 id="code-inodes">Code: Inodes</h2>
<p>To allocate a new inode (for example, when creating a file), xv6 calls <code>ialloc</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L197"><span>(kernel/fs.c:197)</span></a>. <code>Ialloc</code> is similar to <code>balloc</code>: it loops over the inode structures on the disk, one block at a time, looking for one that is marked free. When it finds one, it claims it by writing the new <code>type</code> to the disk and then returns an entry from the inode cache with the tail call to <code>iget</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L211"><span>(kernel/fs.c:211)</span></a>. The correct operation of <code>ialloc</code> depends on the fact that only one process at a time can be holding a reference to <code>bp</code>: <code>ialloc</code> can be sure that some other process does not simultaneously see that the inode is available and try to claim it.</p>
<p><code>Iget</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L244"><span>(kernel/fs.c:244)</span></a> looks through the inode cache for an active entry (<code>ip-&gt;ref</code> <code>&gt;</code> <code>0</code>) with the desired device and inode number. If it finds one, it returns a new reference to that inode <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L253-L257">(kernel/fs.c:253-257)</a>. As <code>iget</code> scans, it records the position of the first empty slot <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L258-L259">(kernel/fs.c:258-259)</a>, which it uses if it needs to allocate a cache entry.</p>
<p>Code must lock the inode using <code>ilock</code> before reading or writing its metadata or content. <code>Ilock</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L290"><span>(kernel/fs.c:290)</span></a> uses a sleep-lock for this purpose. Once <code>ilock</code> has exclusive access to the inode, it reads the inode from disk (more likely, the buffer cache) if needed. The function <code>iunlock</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L318"><span>(kernel/fs.c:318)</span></a> releases the sleep-lock, which may cause any processes sleeping to be woken up.</p>
<p><code>Iput</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L334"><span>(kernel/fs.c:334)</span></a> releases a C pointer to an inode by decrementing the reference count <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L357"><span>(kernel/fs.c:357)</span></a>. If this is the last reference, the inode’s slot in the inode cache is now free and can be re-used for a different inode.</p>
<p>If <code>iput</code> sees that there are no C pointer references to an inode and that the inode has no links to it (occurs in no directory), then the inode and its data blocks must be freed. <code>Iput</code> calls <code>itrunc</code> to truncate the file to zero bytes, freeing the data blocks; sets the inode type to 0 (unallocated); and writes the inode to disk <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L339"><span>(kernel/fs.c:339)</span></a>.</p>
<p>The locking protocol in <code>iput</code> in the case in which it frees the inode deserves a closer look. One danger is that a concurrent thread might be waiting in <code>ilock</code> to use this inode (e.g., to read a file or list a directory), and won’t be prepared to find that the inode is not longer allocated. This can’t happen because there is no way for a system call to get a pointer to a cached inode if it has no links to it and <code>ip-&gt;ref</code> is one. That one reference is the reference owned by the thread calling <code>iput</code>. It’s true that <code>iput</code> checks that the reference count is one outside of its <code>icache.lock</code> critical section, but at that point the link count is known to be zero, so no thread will try to acquire a new reference. The other main danger is that a concurrent call to <code>ialloc</code> might choose the same inode that <code>iput</code> is freeing. This can only happen after the <code>iupdate</code> writes the disk so that the inode has type zero. This race is benign; the allocating thread will politely wait to acquire the inode’s sleep-lock before reading or writing the inode, at which point <code>iput</code> is done with it.</p>
<p><code>iput()</code> can write to the disk. This means that any system call that uses the file system may write the disk, because the system call may be the last one having a reference to the file. Even calls like <code>read()</code> that appear to be read-only, may end up calling <code>iput().</code> This, in turn, means that even read-only system calls must be wrapped in transactions if they use the file system.</p>
<p>There is a challenging interaction between <code>iput()</code> and crashes. <code>iput()</code> doesn’t truncate a file immediately when the link count for the file drops to zero, because some process might still hold a reference to the inode in memory: a process might still be reading and writing to the file, because it successfully opened it. But, if a crash happens before the last process closes the file descriptor for the file, then the file will be marked allocated on disk but no directory entry will point to it.</p>
<p>File systems handle this case in one of two ways. The simple solution is that on recovery, after reboot, the file system scans the whole file system for files that are marked allocated, but have no directory entry pointing to them. If any such file exists, then it can free those files.</p>
<p>The second solution doesn’t require scanning the file system. In this solution, the file system records on disk (e.g., in the super block) the inode inumber of a file whose link count drops to zero but whose reference count isn’t zero. If the file system removes the file when its reference counts reaches 0, then it updates the on-disk list by removing that inode from the list. On recovery, the file system frees any file in the list.</p>
<p>Xv6 implements neither solution, which means that inodes may be marked allocated on disk, even though they are not in use anymore. This means that over time xv6 runs the risk that it may run out of disk space.</p>
<h2 id="code-inode-content">Code: Inode content</h2>
<figure>
<embed src="fig/inode.svg" id="fig:inode" /><figcaption>The representation of a file on disk.</figcaption>
</figure>
<p>The on-disk inode structure, <code>struct dinode</code>, contains a size and an array of block numbers (see Figure <a href="#fig:inode" data-reference-type="ref" data-reference="fig:inode">7.3</a>). The inode data is found in the blocks listed in the <code>dinode</code> ’s <code>addrs</code> array. The first <code>NDIRECT</code> blocks of data are listed in the first <code>NDIRECT</code> entries in the array; these blocks are called <em>direct blocks</em>. The next <code>NINDIRECT</code> blocks of data are listed not in the inode but in a data block called the <em>indirect block</em>. The last entry in the <code>addrs</code> array gives the address of the indirect block. Thus the first 12 kB ( <code>NDIRECT</code> <code>x</code> <code>BSIZE</code>) bytes of a file can be loaded from blocks listed in the inode, while the next <code>256</code> kB ( <code>NINDIRECT</code> <code>x</code> <code>BSIZE</code>) bytes can only be loaded after consulting the indirect block. This is a good on-disk representation but a complex one for clients. The function <code>bmap</code> manages the representation so that higher-level routines such as <code>readi</code> and <code>writei</code>, which we will see shortly. <code>Bmap</code> returns the disk block number of the <code>bn</code>’th data block for the inode <code>ip</code>. If <code>ip</code> does not have such a block yet, <code>bmap</code> allocates one.</p>
<p>The function <code>bmap</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L379"><span>(kernel/fs.c:379)</span></a> begins by picking off the easy case: the first <code>NDIRECT</code> blocks are listed in the inode itself <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L384-L388">(kernel/fs.c:384-388)</a>. The next <code>NINDIRECT</code> blocks are listed in the indirect block at <code>ip-&gt;addrs[NDIRECT]</code>. <code>Bmap</code> reads the indirect block <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L395"><span>(kernel/fs.c:395)</span></a> and then reads a block number from the right position within the block <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L396"><span>(kernel/fs.c:396)</span></a>. If the block number exceeds <code>NDIRECT</code>+NINDIRECT, <code>bmap</code> panics; <code>writei</code> contains the check that prevents this from happening <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L494"><span>(kernel/fs.c:494)</span></a>.</p>
<p><code>Bmap</code> allocates blocks as needed. An <code>ip-&gt;addrs[]</code> or indirect entry of zero indicates that no block is allocated. As <code>bmap</code> encounters zeros, it replaces them with the numbers of fresh blocks, allocated on demand <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L385-L386">(kernel/fs.c:385-386)</a> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L393-L394">(kernel/fs.c:393-394)</a>.</p>
<p><code>itrunc</code> frees a file’s blocks, resetting the inode’s size to zero. <code>Itrunc</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L414"><span>(kernel/fs.c:414)</span></a> starts by freeing the direct blocks <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L420-L425">(kernel/fs.c:420-425)</a>, then the ones listed in the indirect block <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L430-L433">(kernel/fs.c:430-433)</a>, and finally the indirect block itself <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L435-L436">(kernel/fs.c:435-436)</a>.</p>
<p><code>Bmap</code> makes it easy for <code>readi</code> and <code>writei</code> to get at an inode’s data. <code>Readi</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L460"><span>(kernel/fs.c:460)</span></a> starts by making sure that the offset and count are not beyond the end of the file. Reads that start beyond the end of the file return an error <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L465-L466">(kernel/fs.c:465-466)</a> while reads that start at or cross the end of the file return fewer bytes than requested <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L467-L468">(kernel/fs.c:467-468)</a>. The main loop processes each block of the file, copying data from the buffer into <code>dst</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L470-L478">(kernel/fs.c:470-478)</a>. <code>writei</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L487"><span>(kernel/fs.c:487)</span></a> is identical to <code>readi</code>, with three exceptions: writes that start at or cross the end of the file grow the file, up to the maximum file size <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L494-L495">(kernel/fs.c:494-495)</a>; the loop copies data into the buffers instead of out <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L37"><span>(kernel/fs.c:37)</span></a>; and if the write has extended the file, <code>writei</code> must update its size <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L508-L515">(kernel/fs.c:508-515)</a>.</p>
<p>Both <code>readi</code> and <code>writei</code> begin by checking for <code>ip-&gt;type</code> <code>==</code> <code>T_DEV</code>. This case handles special devices whose data does not live in the file system; we will return to this case in the file descriptor layer.</p>
<p>The function <code>stati</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L446"><span>(kernel/fs.c:446)</span></a> copies inode metadata into the <code>stat</code> structure, which is exposed to user programs via the <code>stat</code> system call.</p>
<h2 id="code-directory-layer">Code: directory layer</h2>
<p>A directory is implemented internally much like a file. Its inode has type <code>T_DIR</code> and its data is a sequence of directory entries. Each entry is a <code>struct dirent</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.h#L56"><span>(kernel/fs.h:56)</span></a>, which contains a name and an inode number. The name is at most <code>DIRSIZ</code> (14) characters; if shorter, it is terminated by a NUL (0) byte. Directory entries with inode number zero are free.</p>
<p>The function <code>dirlookup</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L531"><span>(kernel/fs.c:531)</span></a> searches a directory for an entry with the given name. If it finds one, it returns a pointer to the corresponding inode, unlocked, and sets <code>*poff</code> to the byte offset of the entry within the directory, in case the caller wishes to edit it. If <code>dirlookup</code> finds an entry with the right name, it updates <code>*poff</code> and returns an unlocked inode obtained via <code>iget</code>. <code>Dirlookup</code> is the reason that <code>iget</code> returns unlocked inodes. The caller has locked <code>dp</code>, so if the lookup was for <code>.</code>, an alias for the current directory, attempting to lock the inode before returning would try to re-lock <code>dp</code> and deadlock. (There are more complicated deadlock scenarios involving multiple processes and <code>..</code>, an alias for the parent directory; <code>.</code> is not the only problem.) The caller can unlock <code>dp</code> and then lock <code>ip</code>, ensuring that it only holds one lock at a time.</p>
<p>The function <code>dirlink</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L558"><span>(kernel/fs.c:558)</span></a> writes a new directory entry with the given name and inode number into the directory <code>dp</code>. If the name already exists, <code>dirlink</code> returns an error <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L564-L568">(kernel/fs.c:564-568)</a>. The main loop reads directory entries looking for an unallocated entry. When it finds one, it stops the loop early <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L542-L543">(kernel/fs.c:542-543)</a>, with <code>off</code> set to the offset of the available entry. Otherwise, the loop ends with <code>off</code> set to <code>dp-&gt;size</code>. Either way, <code>dirlink</code> then adds a new entry to the directory by writing at offset <code>off</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L578-L581">(kernel/fs.c:578-581)</a>.</p>
<h2 id="code-path-names">Code: Path names</h2>
<p>Path name lookup involves a succession of calls to <code>dirlookup</code>, one for each path component. <code>Namei</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L665"><span>(kernel/fs.c:665)</span></a> evaluates <code>path</code> and returns the corresponding <code>inode</code>. The function <code>nameiparent</code> is a variant: it stops before the last element, returning the inode of the parent directory and copying the final element into <code>name</code>. Both call the generalized function <code>namex</code> to do the real work.</p>
<p><code>Namex</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L630"><span>(kernel/fs.c:630)</span></a> starts by deciding where the path evaluation begins. If the path begins with a slash, evaluation begins at the root; otherwise, the current directory <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L634-L637">(kernel/fs.c:634-637)</a>. Then it uses <code>skipelem</code> to consider each element of the path in turn <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L639"><span>(kernel/fs.c:639)</span></a>. Each iteration of the loop must look up <code>name</code> in the current inode <code>ip</code>. The iteration begins by locking <code>ip</code> and checking that it is a directory. If not, the lookup fails <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L640-L644">(kernel/fs.c:640-644)</a>. (Locking <code>ip</code> is necessary not because <code>ip-&gt;type</code> can change underfoot—it can’t—but because until <code>ilock</code> runs, <code>ip-&gt;type</code> is not guaranteed to have been loaded from disk.) If the call is <code>nameiparent</code> and this is the last path element, the loop stops early, as per the definition of <code>nameiparent</code>; the final path element has already been copied into <code>name</code>, so <code>namex</code> need only return the unlocked <code>ip</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L645-L649">(kernel/fs.c:645-649)</a>. Finally, the loop looks for the path element using <code>dirlookup</code> and prepares for the next iteration by setting <code>ip = next</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L650-L655">(kernel/fs.c:650-655)</a>. When the loop runs out of path elements, it returns <code>ip</code>.</p>
<p>The procedure <code>namex</code> may take a long time to complete: it could involve several disk operations to read inodes and directory blocks for the directories traversed in the pathname (if they are not in the buffer cache). Xv6 is carefully designed so that if an invocation of <code>namex</code> by one kernel thread is blocked on a disk I/O, another kernel thread looking up a different pathname can proceed concurrently. <code>Namex</code> locks each directory in the path separately so that lookups in different directories can proceed in parallel.</p>
<p>This concurrency introduces some challenges. For example, while one kernel thread is looking up a pathname another kernel thread may be changing the directory tree by unlinking a directory. A potential risk is that a lookup may be searching a directory that has been deleted by another kernel thread and its blocks have been re-used for another directory or file.</p>
<p>Xv6 avoids such races. For example, when executing <code>dirlookup</code> in <code>namex</code>, the lookup thread holds the lock on the directory and <code>dirlookup</code> returns an inode that was obtained using <code>iget</code>. <code>Iget</code> increases the reference count of the inode. Only after receiving the inode from <code>dirlookup</code> does <code>namex</code> release the lock on the directory. Now another thread may unlink the inode from the directory but xv6 will not delete the inode yet, because the reference count of the inode is still larger than zero.</p>
<p>Another risk is deadlock. For example, <code>next</code> points to the same inode as <code>ip</code> when looking up ".". Locking <code>next</code> before releasing the lock on <code>ip</code> would result in a deadlock. To avoid this deadlock, <code>namex</code> unlocks the directory before obtaining a lock on <code>next</code>. Here again we see why the separation between <code>iget</code> and <code>ilock</code> is important.</p>
<h2 id="file-descriptor-layer">File descriptor layer</h2>
<p>A cool aspect of the Unix interface is that most resources in Unix are represented as files, including devices such as the console, pipes, and of course, real files. The file descriptor layer is the layer that achieves this uniformity.</p>
<p>Xv6 gives each process its own table of open files, or file descriptors, as we saw in Chapter <a href="#CH:UNIX" data-reference-type="ref" data-reference="CH:UNIX">1</a>. Each open file is represented by a <code>struct file</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/file.h#L1"><span>(kernel/file.h:1)</span></a>, which is a wrapper around either an inode or a pipe, plus an I/O offset. Each call to <code>open</code> creates a new open file (a new <code>struct</code> <code>file</code>): if multiple processes open the same file independently, the different instances will have different I/O offsets. On the other hand, a single open file (the same <code>struct</code> <code>file</code>) can appear multiple times in one process’s file table and also in the file tables of multiple processes. This would happen if one process used <code>open</code> to open the file and then created aliases using <code>dup</code> or shared it with a child using <code>fork</code>. A reference count tracks the number of references to a particular open file. A file can be open for reading or writing or both. The <code>readable</code> and <code>writable</code> fields track this.</p>
<p>All the open files in the system are kept in a global file table, the <code>ftable</code>. The file table has functions to allocate a file (<code>filealloc</code>), create a duplicate reference (<code>filedup</code>), release a reference (<code>fileclose</code>), and read and write data (<code>fileread</code> and <code>filewrite</code>).</p>
<p>The first three follow the now-familiar form. <code>Filealloc</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/file.c#L30"><span>(kernel/file.c:30)</span></a> scans the file table for an unreferenced file (<code>f-&gt;ref</code> <code>==</code> <code>0</code>) and returns a new reference; <code>filedup</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/file.c#L48"><span>(kernel/file.c:48)</span></a> increments the reference count; and <code>fileclose</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/file.c#L60"><span>(kernel/file.c:60)</span></a> decrements it. When a file’s reference count reaches zero, <code>fileclose</code> releases the underlying pipe or inode, according to the type.</p>
<p>The functions <code>filestat</code>, <code>fileread</code>, and <code>filewrite</code> implement the <code>stat</code>, <code>read</code>, and <code>write</code> operations on files. <code>Filestat</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/file.c#L88"><span>(kernel/file.c:88)</span></a> is only allowed on inodes and calls <code>stati</code>. <code>Fileread</code> and <code>filewrite</code> check that the operation is allowed by the open mode and then pass the call through to either the pipe or inode implementation. If the file represents an inode, <code>fileread</code> and <code>filewrite</code> use the I/O offset as the offset for the operation and then advance it <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/file.c#L122-L123">(kernel/file.c:122-123)</a> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/file.c#L153-L154">(kernel/file.c:153-154)</a>. Pipes have no concept of offset. Recall that the inode functions require the caller to handle locking <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/file.c#L94-L96">(kernel/file.c:94-96)</a> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/file.c#L121-L124">(kernel/file.c:121-124)</a> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/file.c#L163-L166">(kernel/file.c:163-166)</a>. The inode locking has the convenient side effect that the read and write offsets are updated atomically, so that multiple writing to the same file simultaneously cannot overwrite each other’s data, though their writes may end up interlaced.</p>
<h2 id="code-system-calls">Code: System calls</h2>
<p>With the functions that the lower layers provide the implementation of most system calls is trivial (see <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/sysfile.c"><span>(kernel/sysfile.c)</span></a>). There are a few calls that deserve a closer look.</p>
<p>The functions <code>sys_link</code> and <code>sys_unlink</code> edit directories, creating or removing references to inodes. They are another good example of the power of using transactions. <code>Sys_link</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/sysfile.c#L120"><span>(kernel/sysfile.c:120)</span></a> begins by fetching its arguments, two strings <code>old</code> and <code>new</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/sysfile.c#L125"><span>(kernel/sysfile.c:125)</span></a>. Assuming <code>old</code> exists and is not a directory <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/sysfile.c#L129-L132">(kernel/sysfile.c:129-132)</a>, <code>sys_link</code> increments its <code>ip-&gt;nlink</code> count. Then <code>sys_link</code> calls <code>nameiparent</code> to find the parent directory and final path element of <code>new</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/sysfile.c#L145"><span>(kernel/sysfile.c:145)</span></a> and creates a new directory entry pointing at <code>old</code> ’s inode <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/sysfile.c#L148"><span>(kernel/sysfile.c:148)</span></a>. The new parent directory must exist and be on the same device as the existing inode: inode numbers only have a unique meaning on a single disk. If an error like this occurs, <code>sys_link</code> must go back and decrement <code>ip-&gt;nlink</code>.</p>
<p>Transactions simplify the implementation because it requires updating multiple disk blocks, but we don’t have to worry about the order in which we do them. They either will all succeed or none. For example, without transactions, updating <code>ip-&gt;nlink</code> before creating a link, would put the file system temporarily in an unsafe state, and a crash in between could result in havoc. With transactions we don’t have to worry about this.</p>
<p><code>Sys_link</code> creates a new name for an existing inode. The function <code>create</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/sysfile.c#L242"><span>(kernel/sysfile.c:242)</span></a> creates a new name for a new inode. It is a generalization of the three file creation system calls: <code>open</code> with the <code>O_CREATE</code> flag makes a new ordinary file, <code>mkdir</code> makes a new directory, and <code>mkdev</code> makes a new device file. Like <code>sys_link</code>, <code>create</code> starts by caling <code>nameiparent</code> to get the inode of the parent directory. It then calls <code>dirlookup</code> to check whether the name already exists <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/sysfile.c#L252"><span>(kernel/sysfile.c:252)</span></a>. If the name does exist, <code>create</code>’s behavior depends on which system call it is being used for: <code>open</code> has different semantics from <code>mkdir</code> and <code>mkdev</code>. If <code>create</code> is being used on behalf of <code>open</code> (<code>type</code> <code>==</code> <code>T_FILE</code>) and the name that exists is itself a regular file, then <code>open</code> treats that as a success, so <code>create</code> does too <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/sysfile.c#L256"><span>(kernel/sysfile.c:256)</span></a>. Otherwise, it is an error <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/sysfile.c#L257-L258">(kernel/sysfile.c:257-258)</a>. If the name does not already exist, <code>create</code> now allocates a new inode with <code>ialloc</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/sysfile.c#L261"><span>(kernel/sysfile.c:261)</span></a>. If the new inode is a directory, <code>create</code> initializes it with <code>.</code> and <code>..</code> entries. Finally, now that the data is initialized properly, <code>create</code> can link it into the parent directory <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/sysfile.c#L274"><span>(kernel/sysfile.c:274)</span></a>. <code>Create</code>, like <code>sys_link</code>, holds two inode locks simultaneously: <code>ip</code> and <code>dp</code>. There is no possibility of deadlock because the inode <code>ip</code> is freshly allocated: no other process in the system will hold <code>ip</code> ’s lock and then try to lock <code>dp</code>.</p>
<p>Using <code>create</code>, it is easy to implement <code>sys_open</code>, <code>sys_mkdir</code>, and <code>sys_mknod</code>. <code>Sys_open</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/sysfile.c#L287"><span>(kernel/sysfile.c:287)</span></a> is the most complex, because creating a new file is only a small part of what it can do. If <code>open</code> is passed the <code>O_CREATE</code> flag, it calls <code>create</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/sysfile.c#L301"><span>(kernel/sysfile.c:301)</span></a>. Otherwise, it calls <code>namei</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/sysfile.c#L307"><span>(kernel/sysfile.c:307)</span></a>. <code>Create</code> returns a locked inode, but <code>namei</code> does not, so <code>sys_open</code> must lock the inode itself. This provides a convenient place to check that directories are only opened for reading, not writing. Assuming the inode was obtained one way or the other, <code>sys_open</code> allocates a file and a file descriptor <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/sysfile.c#L325"><span>(kernel/sysfile.c:325)</span></a> and then fills in the file <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/sysfile.c#L337-L342">(kernel/sysfile.c:337-342)</a>. Note that no other process can access the partially initialized file since it is only in the current process’s table.</p>
<p>Chapter <a href="#CH:SCHED" data-reference-type="ref" data-reference="CH:SCHED">6</a> examined the implementation of pipes before we even had a file system. The function <code>sys_pipe</code> connects that implementation to the file system by providing a way to create a pipe pair. Its argument is a pointer to space for two integers, where it will record the two new file descriptors. Then it allocates the pipe and installs the file descriptors.</p>
<h2 id="real-world-6">Real world</h2>
<p>The buffer cache in a real-world operating system is significantly more complex than xv6’s, but it serves the same two purposes: caching and synchronizing access to the disk. Xv6’s buffer cache, like V6’s, uses a simple least recently used (LRU) eviction policy; there are many more complex policies that can be implemented, each good for some workloads and not as good for others. A more efficient LRU cache would eliminate the linked list, instead using a hash table for lookups and a heap for LRU evictions. Modern buffer caches are typically integrated with the virtual memory system to support memory-mapped files.</p>
<p>Xv6’s logging system is inefficient. A commit cannot occur concurrently with file-system system calls. The system logs entire blocks, even if only a few bytes in a block are changed. It performs synchronous log writes, a block at a time, each of which is likely to require an entire disk rotation time. Real logging systems address all of these problems.</p>
<p>Logging is not the only way to provide crash recovery. Early file systems used a scavenger during reboot (for example, the UNIX <code>fsck</code> program) to examine every file and directory and the block and inode free lists, looking for and resolving inconsistencies. Scavenging can take hours for large file systems, and there are situations where it is not possible to resolve inconsistencies in a way that causes the original system calls to be atomic. Recovery from a log is much faster and causes system calls to be atomic in the face of crashes.</p>
<p>Xv6 uses the same basic on-disk layout of inodes and directories as early UNIX; this scheme has been remarkably persistent over the years. BSD’s UFS/FFS and Linux’s ext2/ext3 use essentially the same data structures. The most inefficient part of the file system layout is the directory, which requires a linear scan over all the disk blocks during each lookup. This is reasonable when directories are only a few disk blocks, but is expensive for directories holding many files. Microsoft Windows’s NTFS, Mac OS X’s HFS, and Solaris’s ZFS, just to name a few, implement a directory as an on-disk balanced tree of blocks. This is complicated but guarantees logarithmic-time directory lookups.</p>
<p>Xv6 is naive about disk failures: if a disk operation fails, xv6 panics. Whether this is reasonable depends on the hardware: if an operating systems sits atop special hardware that uses redundancy to mask disk failures, perhaps the operating system sees failures so infrequently that panicking is okay. On the other hand, operating systems using plain disks should expect failures and handle them more gracefully, so that the loss of a block in one file doesn’t affect the use of the rest of the file system.</p>
<p>Xv6 requires that the file system fit on one disk device and not change in size. As large databases and multimedia files drive storage requirements ever higher, operating systems are developing ways to eliminate the “one disk per file system” bottleneck. The basic approach is to combine many disks into a single logical disk. Hardware solutions such as RAID are still the most popular, but the current trend is moving toward implementing as much of this logic in software as possible. These software implementations typically allow rich functionality like growing or shrinking the logical device by adding or removing disks on the fly. Of course, a storage layer that can grow or shrink on the fly requires a file system that can do the same: the fixed-size array of inode blocks used by xv6 would not work well in such environments. Separating disk management from the file system may be the cleanest design, but the complex interface between the two has led some systems, like Sun’s ZFS, to combine them.</p>
<p>Xv6’s file system lacks many other features of modern file systems; for example, it lacks support for snapshots and incremental backup.</p>
<p>Modern Unix systems allow many kinds of resources to be accessed with the same system calls as on-disk storage: named pipes, network connections, remotely-accessed network file systems, and monitoring and control interfaces such as <code>/proc</code>. Instead of xv6’s <code>if</code> statements in <code>fileread</code> and <code>filewrite</code>, these systems typically give each open file a table of function pointers, one per operation, and call the function pointer to invoke that inode’s implementation of the call. Network file systems and user-level file systems provide functions that turn those calls into network RPCs and wait for the response before returning.</p>
<h2 id="exercises-6">Exercises</h2>
<ol>
<li><p>Why panic in <code>balloc</code> ? Can xv6 recover?</p></li>
<li><p>Why panic in <code>ialloc</code> ? Can xv6 recover?</p></li>
<li><p>Why doesn’t <code>filealloc</code> panic when it runs out of files? Why is this more common and therefore worth handling?</p></li>
<li><p>Suppose the file corresponding to <code>ip</code> gets unlinked by another process between <code>sys_link</code> ’s calls to <code>iunlock(ip)</code> and <code>dirlink</code>. Will the link be created correctly? Why or why not?</p></li>
<li><p><code>create</code> makes four function calls (one to <code>ialloc</code> and three to <code>dirlink</code>) that it requires to succeed. If any doesn’t, <code>create</code> calls <code>panic</code>. Why is this acceptable? Why can’t any of those four calls fail?</p></li>
<li><p><code>sys_chdir</code> calls <code>iunlock(ip)</code> before <code>iput(cp-&gt;cwd)</code>, which might try to lock <code>cp-&gt;cwd</code>, yet postponing <code>iunlock(ip)</code> until after the <code>iput</code> would not cause deadlocks. Why not?</p></li>
<li><p>Implement the <code>lseek</code> system call. Supporting <code>lseek</code> will also require that you modify <code>filewrite</code> to fill holes in the file with zero if <code>lseek</code> sets <code>off</code> beyond <code>f-&gt;ip-&gt;size.</code></p></li>
<li><p>Add <code>O_TRUNC</code> and <code>O_APPEND</code> to <code>open</code>, so that <code>&gt;</code> and <code>&gt;&gt;</code> operators work in the shell.</p></li>
<li><p>Modify the file system to support symbolic links.</p></li>
<li><p>Modify the file system to support named pipes.</p></li>
<li><p>Modify the file and VM system to support mmap.</p></li>
</ol>
<h1 id="CH:LOCK2">Concurrency revisited</h1>
<p>Simultaneously obtaining good parallel performance, correctness despite concurrency, and understandable code is a big challenge in kernel design. Straightforward use of locks is the best path to correctness, but is not always possible. This chapter highlights examples in which xv6 is forced to use locks in an involved way, and examples where xv6 uses lock-like techniques but not locks.</p>
<h2 id="locking-patterns">Locking patterns</h2>
<p>Cached items are often a challenge to lock. For example, the filesystem’s block cache <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/bio.c#L26"><span>(kernel/bio.c:26)</span></a> stores copies of up to <span><code>NBUF</code></span> disk blocks. It’s vital that a given disk block have at most one copy in the cache; otherwise, different processes might make conflicting changes to different copies of what ought to be the same block. Each cached block is stored in a <span><code>struct buf</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/buf.h#L1"><span>(kernel/buf.h:1)</span></a>. A <span><code>struct buf</code></span> has a lock field which helps ensure that only one process uses a given disk block at a time. However, that lock is not enough: what if a block is not present in the cache at all, and two processes want to use it at the same time? There is no <span><code>struct buf</code></span> (since the block isn’t yet cached), and thus there is nothing to lock. Xv6 deals with this situation by associating an additional lock (<span><code>bcache.lock</code></span>) with the set of identities of cached blocks. Code that needs to check if a block is cached (e.g., <span><code>bget</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/bio.c#L58"><span>(kernel/bio.c:58)</span></a>), or change the set of cached blocks, must hold <span><code>bcache.lock</code></span>; after that code has found the block and <span><code>struct buf</code></span> it needs, it can release <span><code> bcache.lock</code></span> and lock just the specific block. This is a common pattern: one lock for the set of items, plus one lock per item.</p>
<p>Ordinarily the same function that acquires a lock will release it. But a more precise way to view things is that a lock is acquired at the start of a sequence that must appear atomic, and released when that sequence ends. If the sequence starts and ends in different functions, or different threads, or on different CPUs, then the lock acquire and release must do the same. The function of the lock is to force other uses to wait, not to pin a piece of data to a particular agent. One example is the <span><code>acquire</code></span> in <span><code>yield</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L500"><span>(kernel/proc.c:500)</span></a>, which is released in the scheduler thread rather than in the acquiring process. Another example is the <span><code>acquiresleep</code></span> in <span><code>ilock</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L290"><span>(kernel/fs.c:290)</span></a>; this code often sleeps while reading the disk; it may wake up on a different CPU, which means the lock may be acquired and released on different CPUs.</p>
<p>Freeing an object that is protected by a lock embedded in the object is a delicate business, since owning the lock is not enough to guarantee that freeing would be correct. The problem case arises when some other thread is waiting in <span><code>acquire</code></span> to use the object; freeing the object implicitly frees the embedded lock, which will cause the waiting thread to malfunction. One solution is to track how many references to the object exist, so that it is only freed when the last reference disappears. See <span><code>pipeclose</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/pipe.c#L59"><span>(kernel/pipe.c:59)</span></a> for an example; <span><code>pi-&gt;readopen</code></span> and <span><code>pi-&gt;writeopen</code></span> track whether the pipe has file descriptors referring to it.</p>
<h2 id="lock-like-patterns">Lock-like patterns</h2>
<p>In many places xv6 uses a reference count or a flag as a kind of soft lock to indicate that an object is allocated and should not be freed or re-used. A process’s <span><code>p-&gt;state</code></span> acts in this way, as do the reference counts in <span><code>file</code></span>, <span><code>inode</code></span>, and <span><code>buf</code></span> structures. While in each case a lock protects the flag or reference count, it is the latter that prevents the object from being prematurely freed.</p>
<p>The file system uses <span><code>struct inode</code></span> reference counts as a kind of shared lock that can be held by multiple processes, in order to avoid deadlocks that would occur if the code used ordinary locks. For example, the loop in <span><code>namex</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/fs.c#L630"><span>(kernel/fs.c:630)</span></a> locks the directory named by each pathname component in turn. However, <span><code>namex</code></span> must release each lock at the end of the loop, since if it held multiple locks it could deadlock with itself if the pathname included a dot (e.g., <span><code>a/./b</code></span>). It might also deadlock with a concurrent lookup involving the directory and <span><code>..</code></span>. As Chapter <a href="#CH:FS" data-reference-type="ref" data-reference="CH:FS">7</a> explains, the solution is for the loop to carry the directory inode over to the next iteration with its reference count incremented, but not locked.</p>
<p>Some data items are protected by different mechanisms at different times, and may at times be protected from concurrent access implicitly by the structure of the xv6 code rather than by explicit locks. For example, when a physical page is free, it is protected by <code> kmem.lock</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/kalloc.c#L24"><span>(kernel/kalloc.c:24)</span></a>. If the page is then allocated as a pipe <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/pipe.c#L23"><span>(kernel/pipe.c:23)</span></a>, it is protected by a different lock (the embedded <code>pi-&gt;lock</code>). If the page is re-allocated for a new process’s user memory, it is not protected by a lock at all. Instead, the fact that the allocator won’t give that page to any other process (until it is freed) protects it from concurrent access. The ownership of a new process’s memory is complex: first the parent allocates and manipulates it in <span><code>fork</code></span>, then the child uses it, and (after the child exits) the parent again owns the memory and passes it to <span><code> kfree</code></span>. There are two lessons here: a data object may be protected from concurrency in different ways at different points in its lifetime, and the protection may take the form of implicit structure rather than explicit locks.</p>
<p>A final lock-like example is the need to disable interrupts around calls to <span><code>mycpu()</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L66"><span>(kernel/proc.c:66)</span></a>. Disabling interrupts causes the calling code to be atomic with respect to timer interrupts that could force a context switch, and thus move the process to a different CPU.</p>
<h2 id="no-locks-at-all">No locks at all</h2>
<p>There are a few places where xv6 shares mutable data with no locks at all. One is in the implementation of spinlocks, although one could view the RISC-V atomic instructions as relying on locks implemented in hardware. Another is the <span><code>started</code></span> variable in <span><code>main.c</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/main.c#L7"><span>(kernel/main.c:7)</span></a>, used to prevent other CPUs from running until CPU zero has finished initializing xv6; the <span><code>volatile</code></span> ensures that the compiler actually generates load and store instructions. A third are some uses of <span><code>p-&gt;parent</code></span> in <span><code>proc.c</code></span> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L383"><span>(kernel/proc.c:383)</span></a> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L291"><span>(kernel/proc.c:291)</span></a> where proper locking could deadlock, but it seems clear that no other process could be simultaneously modifying <span><code>p-&gt;parent</code></span>. A fourth example is <code>p-&gt;killed</code>, which is set while holding <code>p-&gt;lock</code> <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/proc.c#L596"><span>(kernel/proc.c:596)</span></a>, but checked without a holding lock <a href="https://github.com/mit-pdos/xv6-riscv/blob/riscv//kernel/trap.c#L56"><span>(kernel/trap.c:56)</span></a>.</p>
<p>Xv6 contains cases in which one CPU or thread writes some data, and another CPU or thread reads the data, but there is no specific lock dedicated to protecting that data. For example, in <span><code>fork</code></span>, the parent writes the child’s user memory pages, and the child (a different thread, perhaps on a different CPU) reads those pages; no lock explicitly protects those pages. This is not strictly a locking problem, since the child doesn’t start executing until after the parent has finished writing. It is a potential memory ordering problem (see Chapter <a href="#CH:LOCK" data-reference-type="ref" data-reference="CH:LOCK">5</a>), since without a memory barrier there’s no reason to expect one CPU to see another CPU’s writes. However, since the parent releases locks, and the child acquires locks as it starts up, the memory barriers in <span><code>acquire</code></span> and <span><code>release</code></span> ensure that the child’s CPU sees the parent’s writes.</p>
<h2 id="parallelism">Parallelism</h2>
<p>Locking is primarily about suppressing parallelism in the interests of correctness. Because performance is also important, kernel designers often have to think about how to use locks in a way that achieves both correctness and good parallelism. While xv6 is not systematically designed for high performance, it’s still worth considering which xv6 operations can execute in parallel, and which might conflict on locks.</p>
<p>Pipes in xv6 are an example of fairly good parallelism. Each pipe has its own lock, so that different processes can read and write different pipes in parallel on different CPUs. For a given pipe, however, the writer and reader must wait for each other to release the lock; they can’t read/write the same pipe at the same time. It is also the case that a read from an empty pipe (or a write to a full pipe) must block, but this is not due to the locking scheme.</p>
<p>Context switching is a more complex example. Two kernel threads, each executing on its own CPU, can call <span><code>yield</code></span>, <span><code>sched</code></span>, and <span><code> swtch</code></span> at the same time, and the calls will execute in parallel. The threads each hold a lock, but they are different locks, so they don’t have to wait for each other. Once in <span><code>scheduler</code></span>, however, the two CPUs may conflict on locks while searching the table of processes for one that is <span><code>RUNNABLE</code></span>. That is, xv6 is likely to get a performance benefit from multiple CPUs during context switch, but perhaps not as much as it could.</p>
<p>Another example is concurrent calls to <span><code>fork</code></span> from different processes on different CPUs. The calls may have to wait for each other for <span><code>pid_lock</code></span> and <span><code>kmem.lock</code></span>, and for per-process locks needed to search the process table for an <span><code>UNUSED</code></span> process. On the other hand, the two forking processes can copy user memory pages and format page-table pages fully in parallel.</p>
<p>The locking scheme in each of the above examples sacrifices parallel performance in certain cases. In each case it’s possible to obtain more parallelism using a more elaborate design. Whether it’s worthwhile depends on details: how often the relevant operations are invoked, how long the code spends with a contended lock held, how many CPUs might be running conflicting operations at the same time, whether other parts of the code are more restrictive bottlenecks. It can be difficult to guess whether a given locking scheme might cause performance problems, or whether a new design is significantly better, so measurement on realistic workloads is often required.</p>
<h2 id="exercises-7">Exercises</h2>
<ol>
<li><p>Modify xv6’s pipe implementation to allow a read and a write to the same pipe to proceed in parallel on different cores.</p></li>
<li><p>Modify xv6’s <code>scheduler()</code> to reduce lock contention when different cores are looking for runnable processes at the same time.</p></li>
<li><p>Eliminate some of the serialization in xv6’s <code>fork()</code>.</p></li>
</ol>
<h1 id="CH:SUM">Summary</h1>
<p>This text introduced the main ideas in operating systems by studying one operating system, xv6, line by line. Some code lines embody the essence of the main ideas (e.g., context switching, user/kernel boundary, locks, etc.) and each line is important; other code lines provide an illustration of how to implement a particular operating system idea and could easily be done in different ways (e.g., a better algorithm for scheduling, better on-disk data structures to represent files, better logging to allow for concurrent transactions, etc.). All the ideas were illustrated in the context of one particular, very successful system call interface, the Unix interface, but those ideas carry over to the design of other operating systems.</p>
</body>
</html>
